{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"colab":{"name":"day-4-fine-tuning-a-custom-model.ipynb","toc_visible":true},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11385452,"sourceType":"datasetVersion","datasetId":7129309}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Categorise finance transactions\n\nIn life, my financial transactions are often categorised incorrectly in my budgeting app. I decided to find a better solution.\n\nIn this example, I will first try to categorise with an existing Gemini model using a zero-shot prompt and evaluate its performance. Then I will tune a model with the data categorised by me and evaluate its performance.","metadata":{"id":"4KDIFPAL2EnL"}},{"cell_type":"code","source":"# Install required libraries\n!pip uninstall -qqy jupyterlab  # Remove unused conflicting packages\n!pip install -U -q \"google-genai==1.7.0\"","metadata":{"execution":{"iopub.status.busy":"2025-04-13T13:39:00.707248Z","iopub.execute_input":"2025-04-13T13:39:00.709201Z","iopub.status.idle":"2025-04-13T13:39:13.280984Z","shell.execute_reply.started":"2025-04-13T13:39:00.709137Z","shell.execute_reply":"2025-04-13T13:39:13.279141Z"},"id":"9wafTyEH1_xF","trusted":true},"outputs":[],"execution_count":187},{"cell_type":"code","source":"# Import necessary libraries\nfrom google import genai\nfrom google.genai import types\n\ngenai.__version__","metadata":{"id":"T0CBG9xL2PvT","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:39:13.285013Z","iopub.execute_input":"2025-04-13T13:39:13.285630Z","iopub.status.idle":"2025-04-13T13:39:13.294812Z","shell.execute_reply.started":"2025-04-13T13:39:13.285564Z","shell.execute_reply":"2025-04-13T13:39:13.293672Z"}},"outputs":[{"execution_count":188,"output_type":"execute_result","data":{"text/plain":"'1.7.0'"},"metadata":{}}],"execution_count":188},{"cell_type":"code","source":"# Set up the Google GenAI client\nfrom kaggle_secrets import UserSecretsClient\n\nGOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n\nclient = genai.Client(api_key=GOOGLE_API_KEY)","metadata":{"id":"VuJPY3GK2SLZ","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:39:13.296658Z","iopub.execute_input":"2025-04-13T13:39:13.297036Z","iopub.status.idle":"2025-04-13T13:39:13.476299Z","shell.execute_reply.started":"2025-04-13T13:39:13.296993Z","shell.execute_reply":"2025-04-13T13:39:13.475096Z"}},"outputs":[],"execution_count":189},{"cell_type":"markdown","source":"## Load the subcategory and category table\nIn this step, I load the subcategory and category table.","metadata":{}},{"cell_type":"markdown","source":"## Load the subcategory and category table\nIn this step, I load the subcategory, category table.","metadata":{"execution":{"iopub.status.busy":"2025-04-13T12:10:19.728661Z","iopub.execute_input":"2025-04-13T12:10:19.729115Z","iopub.status.idle":"2025-04-13T12:10:19.736220Z","shell.execute_reply.started":"2025-04-13T12:10:19.729076Z","shell.execute_reply":"2025-04-13T12:10:19.734777Z"}}},{"cell_type":"code","source":"import sqlite3\n\n# Connect to your database\ndb_conn = sqlite3.connect('/kaggle/working/transaction_categories.db')\ncursor = db_conn.cursor()\n\n# Create the tables\ncursor.execute('''\nCREATE TABLE IF NOT EXISTS categories (\n    category_id INTEGER PRIMARY KEY AUTOINCREMENT,\n    name VARCHAR(100) NOT NULL UNIQUE,\n    description TEXT,\n    display_order INT DEFAULT 100,\n    is_active BOOLEAN DEFAULT 1,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n)\n''')\n\ncursor.execute('''\nCREATE TABLE IF NOT EXISTS subcategories (\n    subcategory_id INTEGER PRIMARY KEY AUTOINCREMENT,\n    category_id INTEGER NOT NULL,\n    name VARCHAR(100) NOT NULL,\n    description TEXT,\n    display_order INT DEFAULT 100,\n    is_active BOOLEAN DEFAULT 1,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    FOREIGN KEY (category_id) REFERENCES categories(category_id)\n)\n''')\n\n\n# Insert main categories\ncategories = [\n    ('Food & Beverages', 'Expenses related to food and drinks', 10),\n    ('Shopping', 'Retail purchases and shopping expenses', 20),\n    ('Housing', 'Home-related expenses including rent and utilities', 30),\n    ('Transportation', 'Public and private transportation costs', 40),\n    ('Vehicle', 'Car and vehicle related expenses', 50),\n    ('Life & Entertainment', 'Leisure activities and entertainment', 60),\n    ('Communication, PC', 'Internet, phone and computer expenses', 70),\n    ('Financial expenses', 'Banking fees, loans, and financial costs', 80),\n    ('Investments', 'Investment-related transactions', 90),\n    ('Income', 'All sources of incoming money', 100),\n    ('TRANSFER', 'Money transfers between accounts', 110)\n]\n\ncursor.executemany('INSERT OR IGNORE INTO categories (name, description, display_order) VALUES (?, ?, ?)', categories)\n\n# Insert subcategories for Food & Beverages\nfood_subcategories = [\n    (1, 'Bar, cafe, drink, snacks', 10),\n    (1, 'Groceries', 20),\n    (1, 'Restaurant, fast-food', 30)\n]\ncursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', food_subcategories)\n\n# Insert subcategories for Shopping\nshopping_subcategories = [\n    (2, 'Clothes & Footwear', 10),\n    (2, 'Drug-store, chemist', 20),\n    (2, 'Electronics, accessories', 30),\n    (2, 'Gifts, joy', 40),\n    (2, 'Health and beauty', 50),\n    (2, 'Home, garden', 60),\n    (2, 'Jewels, accessories', 70),\n    (2, 'Kids', 80),\n    (2, 'Leisure time', 90),\n    (2, 'Pets, animals', 100),\n    (2, 'Stationery, tools', 110)\n]\ncursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', shopping_subcategories)\n\nhousing_subcategories=[\n    (3, 'Energy, utilities', 10),\n    (3, 'Maintenance, repairs', 20),\n    (3, 'Mortgage', 30),\n    (3, 'Property insurance', 40),\n    (3, 'Rent', 50),\n    (3, 'Services', 60)\n]\ncursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', housing_subcategories)\n\n\ntransportation_subcategories=[\n    (4, 'Business trips', 10),\n    (4, 'Long distance', 20),\n    (4, 'Public transport', 30),\n    (4, 'Taxi', 40)\n]\ncursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', transportation_subcategories)\n\nvehicle_subcategories=[\n    (5, 'Fuel', 10),\n    (5, 'Leasing', 20),\n    (5, 'Parking', 30),\n    (5, 'Rentals', 40),\n    (5, 'Vehicle insurance', 50),\n    (5, 'Vehicle maintenance', 60)\n]\ncursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', vehicle_subcategories)\n\nlife_subcategories=[\n    (6, 'Active sport, fitness', 10),\n    (6, 'Alcohol, tobacco', 20),\n    (6, 'Books, audio, subscriptions', 30),\n    (6, 'Charity, gifts', 40),\n    (6, 'Culture, sport events', 50),\n    (6, 'Education, development', 60),\n    (6, 'Health care, doctor', 70),\n    (6, 'Hobbies', 80),\n    (6, 'Holiday, trips, hotels', 90),\n    (6, 'Life events', 100),\n    (6, 'Lottery, gambling', 110),\n    (6, 'TV, Streaming', 120),\n    (6, 'Wellness, beauty', 130)\n]\ncursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', life_subcategories)\n\ncommunication_subcategories=[\n    (7, 'Internet', 10),\n    (7, 'Postal services', 20),\n    (7, 'Software, apps, games', 30),\n    (7, 'Telephony, mobile phone', 40)\n]\ncursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', communication_subcategories)\n\nfinancial_subcategories=[\n    (8, 'Advisory', 10),\n    (8, 'Charges, Fees', 20),\n    (8, 'Child Support', 30),\n    (8, 'Fines', 40),\n    (8, 'Insurances', 50),\n    (8, 'Loans, interests', 60),\n    (8, 'Taxes', 70)\n]\ncursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', financial_subcategories)\n\ninvestments_subcategories=[\n    (9, 'Collections', 10),\n    (9, 'Financial investments', 20),\n    (9, 'Realty', 30),\n    (9, 'Savings', 40),\n    (9, 'Vehicles, chattels', 50)\n]\ncursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', investments_subcategories)\n\nincome_subcategories=[\n    (10, 'Checks, coupons', 10),\n    (10, 'Child Support', 20),\n    (10, 'Dues & grants', 30),\n    (10, 'Gifts', 40),\n    (10, 'Interests, dividends', 50),\n    (10, 'Lending, renting', 60),\n    (10, 'Lottery earning', 70),\n    (10, 'Refunds (tax, purchase)', 80),\n    (10, 'Rental income', 90),\n    (10, 'Sale', 100),\n    (10, 'Wage, invoices', 110)\n]\ncursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', income_subcategories)\n\ntransfer_subcatgories=[\n    (11, 'TRANSFER', 10),   \n]\ncursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', transfer_subcatgories)\n\n# Commit the changes\ndb_conn.commit()\nprint(\"Database schema created successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:39:13.478707Z","iopub.execute_input":"2025-04-13T13:39:13.479154Z","iopub.status.idle":"2025-04-13T13:39:13.516876Z","shell.execute_reply.started":"2025-04-13T13:39:13.479116Z","shell.execute_reply":"2025-04-13T13:39:13.515666Z"}},"outputs":[{"name":"stdout","text":"Database schema created successfully!\n","output_type":"stream"}],"execution_count":190},{"cell_type":"markdown","source":"In this step, I create the mapping beween category and subcategory","metadata":{}},{"cell_type":"code","source":"def setup_database_and_get_hierarchy(output_path=\"/kaggle/working/category_mapping.csv\"):\n    \"\"\"\n    Initialize database, return category hierarchy, and output a simple mapping CSV.\n    \n    Args:\n        output_path: Path to save the mapping CSV\n        \n    Returns:\n        tuple: (db_connection, category_hierarchy_dict, subcategory_to_category_mapping)\n    \"\"\"\n    import sqlite3\n    import pandas as pd\n    \n    print(\"Setting up database and extracting category hierarchy...\")\n    \n    # Create database connection\n    db_conn = sqlite3.connect('/kaggle/working/transaction_categories.db')\n    cursor = db_conn.cursor()\n    \n    # Create tables and populate data if needed (your existing code)\n    # ... (Keep your existing table creation and population code)\n    \n    # Get complete hierarchy in one operation\n    cursor.execute(\"\"\"\n    SELECT \n        c.name as category, \n        s.name as subcategory\n    FROM categories c\n    JOIN subcategories s ON c.category_id = s.category_id\n    ORDER BY c.display_order, s.display_order\n    \"\"\")\n    \n    # Convert query results to DataFrame\n    results = cursor.fetchall()\n    mapping_df = pd.DataFrame(results, columns=['category', 'subcategory'])\n    \n    # Process results into usable format for return values\n    category_hierarchy = {}\n    subcat_to_cat_mapping = {}\n    \n    for category, subcategory in results:\n        # Build hierarchy dictionary\n        if category not in category_hierarchy:\n            category_hierarchy[category] = []\n        category_hierarchy[category].append(subcategory)\n        \n        # Build mapping dictionary\n        subcat_to_cat_mapping[subcategory] = category\n    \n    # Save to CSV file\n    mapping_df.to_csv(output_path, index=False)\n    \n    # Print summary\n    print(f\"\\nCategory-subcategory mapping saved to {output_path}\")\n    print(f\"Found {len(mapping_df['category'].unique())} categories and {len(mapping_df)} subcategories\")\n    print(\"\\nSample of mapping:\")\n    print(mapping_df.head(5))\n    \n    db_conn.commit()\n    \n    return db_conn, category_hierarchy, subcat_to_cat_mapping","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:39:13.522881Z","iopub.execute_input":"2025-04-13T13:39:13.523235Z","iopub.status.idle":"2025-04-13T13:39:13.532461Z","shell.execute_reply.started":"2025-04-13T13:39:13.523205Z","shell.execute_reply":"2025-04-13T13:39:13.531206Z"}},"outputs":[],"execution_count":191},{"cell_type":"markdown","source":"## Load the dataset\n\nI have uploaded transaction data categorised by me. Then I group it into training data and test data.","metadata":{"id":"peFm0w_0c1CO"}},{"cell_type":"code","source":"# Load and preprocess transaction data\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Load your data\nfile_path = \"/kaggle/input/training/categorized_transaction.csv\"\ndf = pd.read_csv(file_path)\n\n# Split into train and test sets (80/20 split)\ndf_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n\n# Display the subcategories (labels) in your dataset\nsubcategories = df['subcategory'].unique()\nprint(f\"Number of subcategories: {len(subcategories)}\")\nprint(\"Sample subcategories:\", subcategories[:10])  # Show first 10 subcategories\n\n# Quick look at note examples\nprint(\"\\nSample notes:\")\nfor i, note in enumerate(df['note'].head(3)):\n    print(f\"{i+1}. {note} → {df['subcategory'].iloc[i]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:39:13.533896Z","iopub.execute_input":"2025-04-13T13:39:13.534367Z","iopub.status.idle":"2025-04-13T13:39:13.603689Z","shell.execute_reply.started":"2025-04-13T13:39:13.534304Z","shell.execute_reply":"2025-04-13T13:39:13.602522Z"}},"outputs":[{"name":"stdout","text":"Number of subcategories: 64\nSample subcategories: ['Active sport, fitness' 'Advisory' 'Alcohol, tobacco'\n 'Bar, cafe, drink, snacks' 'Books, audio, subscriptions' 'Charges, Fees'\n 'Charity, gifts' 'Checks, coupons' 'Clothes & shoes'\n 'Culture, sport events']\n\nSample notes:\n1. AMAZON AUSYDNEY SOUTH CREDIT CARD PURCHASEAmazon Basics High-Density Round Foam Roller for Exercise and Recovery - 61cm, Blue Speckled → Active sport, fitness\n2. 02 DEC 20 - $98.00 LULULEMON ATHLETICA AUSTRAlbert Park [Eff Date: 30 NOV 20] → Active sport, fitness\n3. REBEL MELBOURNE CTRL MELBOURNE VI AUSTap and Pay xx3173Value Date_ 17/03/2018 → Active sport, fitness\n","output_type":"stream"}],"execution_count":192},{"cell_type":"markdown","source":"## Clean the data","metadata":{}},{"cell_type":"code","source":"import re\nimport pandas as pd\n\ndef clean_transaction_note(note):\n    \"\"\"\n    Clean transaction notes to remove common bank formatting, dates, card numbers, etc.\n    \"\"\"\n    # Handle None or empty strings\n    if note is None or pd.isna(note) or note == \"\":\n        return \"\"\n    \n    # Convert to string if needed\n    text = str(note)\n    \n    # Replace non-ASCII characters\n    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n    \n    # Extract main part of the transaction (before common transaction markers)\n    transaction_markers = r'\\s+(?:CREDIT CARD PURCHASE|EFTPOS|Value Date|tap and Pay|Card Purchase|CARD PURCHASE)'\n    parts = re.split(transaction_markers, text, flags=re.IGNORECASE)\n    main_text = parts[0] if parts else text\n    \n    # Clean amount figures and currency symbols\n    main_text = re.sub(r'(?:[$€£¥]|AUD|USD|EUR|GBP|NZD)\\s*[-+]?(?:\\d{1,3}(?:,\\d{3})*|\\d+)(?:\\.\\d{1,2})?', '', main_text)\n    main_text = re.sub(r'\\b[-+]?(?:\\d{1,3}(?:,\\d{3})*|\\d+)(?:\\.\\d{1,2})?\\b', '', main_text)\n    \n    # Remove card numbers (masked or full)\n    main_text = re.sub(r'(?:x{2,4}|X{2,4})\\d{4}|\\b\\d{4}[\\s-]?\\d{4}[\\s-]?\\d{4}[\\s-]?\\d{4}\\b', '', main_text)\n    \n    # Remove date patterns\n    date_pattern = r'(?:\\d{1,2}\\s+(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\\s+\\d{2,4}|' + \\\n                   r'(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\\s+\\d{1,2}(?:st|nd|rd|th)?\\s+\\d{2,4}|' + \\\n                   r'\\d{1,2}[-/.]\\d{1,2}[-/.]\\d{2,4}|' + \\\n                   r'\\d{4}[-/.]\\d{1,2}[-/.]\\d{1,2}|' + \\\n                   r'\\[Eff\\s+Date:.*?\\]|' + \\\n                   r'Value\\s+Date[_:]\\s*\\d{1,2}[-/.]\\d{1,2}[-/.]\\d{2,4})'\n    main_text = re.sub(date_pattern, '', main_text, flags=re.IGNORECASE)\n    \n    # Clean whitespace and punctuation\n    main_text = re.sub(r'\\s+', ' ', main_text)\n    main_text = re.sub(r'[\\s,.-]+$|^[\\s,.-]+', '', main_text)\n    main_text = re.sub(r'\\s+([,.])', r'\\1', main_text)\n    \n    return main_text.strip()[:500]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:39:13.605534Z","iopub.execute_input":"2025-04-13T13:39:13.606611Z","iopub.status.idle":"2025-04-13T13:39:13.618427Z","shell.execute_reply.started":"2025-04-13T13:39:13.606546Z","shell.execute_reply":"2025-04-13T13:39:13.617185Z"}},"outputs":[],"execution_count":193},{"cell_type":"code","source":"def add_category_column(df, db_conn):\n    \"\"\"\n    Add category column to DataFrame based on subcategory using database mapping.\n    \"\"\"\n    if 'category' in df.columns:\n        print(\"Category column already exists\")\n        return df\n    \n    try:\n        # Query the database for subcategory to category mapping\n        cursor = db_conn.cursor()\n        cursor.execute(\"\"\"\n        SELECT s.name as subcategory, c.name as category \n        FROM subcategories s\n        JOIN categories c ON s.category_id = c.category_id\n        \"\"\")\n        \n        # Create mapping dictionary\n        subcat_to_cat = {row[0]: row[1] for row in cursor.fetchall()}\n        \n        # Add category column\n        df_with_category = df.copy()\n        df_with_category['category'] = df['subcategory'].map(subcat_to_cat)\n        \n        # Check for unmapped subcategories\n        missing_count = df_with_category['category'].isna().sum()\n        if missing_count > 0:\n            print(f\"Warning: {missing_count} rows have unmapped subcategories\")\n            unmapped = df[df['subcategory'].map(lambda x: x not in subcat_to_cat)]['subcategory'].unique()\n            print(f\"Unmapped subcategories: {unmapped}\")\n            \n        # Fill missing with placeholder\n        df_with_category['category'] = df_with_category['category'].fillna(\"Unknown\")\n        \n        print(f\"Added categories to {len(df_with_category)} transactions\")\n        return df_with_category\n        \n    except Exception as e:\n        print(f\"Error getting category mapping: {e}\")\n        # Create placeholder category column if needed\n        df_copy = df.copy()\n        df_copy['category'] = \"Unknown\"\n        return df_copy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:39:13.619945Z","iopub.execute_input":"2025-04-13T13:39:13.620315Z","iopub.status.idle":"2025-04-13T13:39:13.637052Z","shell.execute_reply.started":"2025-04-13T13:39:13.620280Z","shell.execute_reply":"2025-04-13T13:39:13.635826Z"}},"outputs":[],"execution_count":194},{"cell_type":"code","source":"def sample_balanced_data(df, samples_per_subcategory):\n    \"\"\"\n    Create a balanced dataset by sampling evenly across subcategories.\n    If a subcategory has fewer than the requested samples, uses all available rows.\n    \"\"\"\n    # Group by subcategory and sample\n    sampled_df = (\n        df.groupby(\"subcategory\")[df.columns]\n        .apply(lambda x: x.sample(min(len(x), samples_per_subcategory)))\n        .reset_index(drop=True)\n    )\n    \n    # Convert subcategory to category type for efficiency\n    sampled_df[\"subcategory\"] = sampled_df[\"subcategory\"].astype(\"category\")\n    \n    return sampled_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:39:13.638358Z","iopub.execute_input":"2025-04-13T13:39:13.638703Z","iopub.status.idle":"2025-04-13T13:39:13.655009Z","shell.execute_reply.started":"2025-04-13T13:39:13.638669Z","shell.execute_reply":"2025-04-13T13:39:13.653826Z"}},"outputs":[],"execution_count":195},{"cell_type":"code","source":"def filter_unmapped_subcategories(df, db_conn):\n    \"\"\"\n    Filter out rows with subcategories that don't have a mapping in the database.\n    \n    Args:\n        df: DataFrame containing transaction data\n        db_conn: Database connection\n        \n    Returns:\n        DataFrame with only mapped subcategories\n    \"\"\"\n    # Get all valid subcategories from the database\n    cursor = db_conn.cursor()\n    cursor.execute(\"SELECT name FROM subcategories\")\n    valid_subcategories = [row[0] for row in cursor.fetchall()]\n    \n    # Filter the DataFrame to only include rows with valid subcategories\n    df_filtered = df[df['subcategory'].isin(valid_subcategories)].copy()\n    \n    # Report how many rows were filtered out\n    filtered_count = len(df) - len(df_filtered)\n    print(f\"Filtered out {filtered_count} rows with unmapped subcategories\")\n    print(f\"Unmapped subcategories: {df[~df['subcategory'].isin(valid_subcategories)]['subcategory'].unique()}\")\n    \n    return df_filtered","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:39:13.656550Z","iopub.execute_input":"2025-04-13T13:39:13.656942Z","iopub.status.idle":"2025-04-13T13:39:13.678140Z","shell.execute_reply.started":"2025-04-13T13:39:13.656897Z","shell.execute_reply":"2025-04-13T13:39:13.676830Z"}},"outputs":[],"execution_count":196},{"cell_type":"code","source":"def process_transaction_data(df, db_conn, train_samples=50, test_samples=10, sample_csv_path=None):\n    \"\"\"\n    Process transaction data through all steps: cleaning, categorizing, sampling, and filtering unmapped subcategories.\n    \n    Args:\n        df: DataFrame with transaction data\n        db_conn: Database connection for category mapping\n        train_samples: Number of samples per subcategory for training\n        test_samples: Number of samples per subcategory for testing\n        sample_csv_path: Path to save a sample CSV for review\n        \n    Returns:\n        Tuple of (train_df, test_df, df_with_categories)\n    \"\"\"\n    # Make a copy to avoid modifying the original\n    df_copy = df.copy()\n    \n    # Step 1: Clean transaction notes\n    print(\"Cleaning transaction notes...\")\n    df_copy['cleaned_note'] = df_copy['note'].apply(clean_transaction_note)\n    \n    # Step 2: Add category column\n    print(\"Adding category mapping...\")\n    df_with_categories = add_category_column(df_copy, db_conn)\n    \n    # Step 3: Filter out unmapped subcategories\n    print(\"Filtering out unmapped subcategories...\")\n    df_filtered = filter_unmapped_subcategories(df_with_categories, db_conn)\n    \n    # Step 4: Split into train and test data\n    train_df, test_df = train_test_split(df_filtered, test_size=0.2, random_state=42)\n    \n    # Step 5: Sample balanced datasets\n    print(f\"Creating balanced samples ({train_samples} per subcategory for training)...\")\n    train_sampled = sample_balanced_data(train_df, train_samples)\n    test_sampled = sample_balanced_data(test_df, test_samples)\n    \n    # Step 6: Save sample for review if requested\n    if sample_csv_path:\n        # Take a small sample from each subcategory for review\n        review_sample = sample_balanced_data(df_filtered, 2)\n        # Include original and cleaned notes for comparison\n        review_sample = review_sample[['note', 'cleaned_note', 'category', 'subcategory']]\n        review_sample.to_csv(sample_csv_path, index=False)\n        print(f\"Saved sample data to {sample_csv_path} for review\")\n    \n    # Print statistics\n    print(f\"Original data: {len(df)} transactions\")\n    print(f\"Filtered data: {len(df_filtered)} transactions\")\n    print(f\"Balanced training data: {len(train_sampled)} transactions ({train_sampled['subcategory'].nunique()} subcategories)\")\n    print(f\"Balanced test data: {len(test_sampled)} transactions ({test_sampled['subcategory'].nunique()} subcategories)\")\n    \n    return train_sampled, test_sampled, df_filtered","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:39:13.680148Z","iopub.execute_input":"2025-04-13T13:39:13.680500Z","iopub.status.idle":"2025-04-13T13:39:13.696046Z","shell.execute_reply.started":"2025-04-13T13:39:13.680466Z","shell.execute_reply":"2025-04-13T13:39:13.694815Z"}},"outputs":[],"execution_count":197},{"cell_type":"markdown","source":"## Sample the dataset\nNow sample the data. I will keep 50 rows for each subcategory for training.","metadata":{"id":"03lDs1O4ZQ0-"}},{"cell_type":"code","source":"# Connect to the database\nimport sqlite3\nimport pandas as pd\nfrom collections import Counter\n\ndb_conn = sqlite3.connect('/kaggle/working/transaction_categories.db')\n\n# Process the data\ndf_train_sampled, df_test_sampled, df_categorized = process_transaction_data(\n    df, \n    db_conn,\n    train_samples=50, \n    test_samples=10,\n    sample_csv_path=\"/kaggle/working/transaction_sample_review.csv\"\n)\n\n# Print distribution of categories and subcategories\ndef print_distribution_stats(df, dataset_name=\"Dataset\"):\n    print(f\"\\n{'-'*50}\")\n    print(f\"{dataset_name} Distribution Statistics:\")\n    print(f\"{'-'*50}\")\n    \n    # Category distribution\n    category_counts = df['category'].value_counts()\n    print(f\"\\nCategories ({len(category_counts)} unique):\")\n    print(f\"{'Category':<25} {'Count':<10} {'Percentage':<10}\")\n    print(f\"{'-'*45}\")\n    \n    for category, count in category_counts.items():\n        percentage = count / len(df) * 100\n        print(f\"{category[:25]:<25} {count:<10} {percentage:.1f}%\")\n    \n    # Subcategory distribution\n    subcategory_counts = df['subcategory'].value_counts()\n    print(f\"\\nSubcategories ({len(subcategory_counts)} unique):\")\n    print(f\"{'Subcategory':<30} {'Category':<20} {'Count':<10} {'Percentage':<10}\")\n    print(f\"{'-'*70}\")\n    \n    # Create a mapping from subcategory to category for lookup\n    subcat_to_cat = df.groupby('subcategory')['category'].first().to_dict()\n    \n    for subcategory, count in subcategory_counts.items():\n        percentage = count / len(df) * 100\n        category = subcat_to_cat.get(subcategory, \"Unknown\")\n        print(f\"{subcategory[:30]:<30} {category[:20]:<20} {count:<10} {percentage:.1f}%\")\n    \n    # Find subcategories with low counts (potential data issues)\n    low_count_threshold = 5  # Adjust as needed\n    low_count_subcats = subcategory_counts[subcategory_counts < low_count_threshold]\n    if len(low_count_subcats) > 0:\n        print(f\"\\nSubcategories with low counts (<{low_count_threshold}):\")\n        for subcat, count in low_count_subcats.items():\n            print(f\"  - {subcat}: {count} transactions\")\n\n# Display sample of training data\nprint(\"\\nSample of training data:\")\nprint(df_train_sampled[['cleaned_note', 'category', 'subcategory']].head())\n\n# Print distribution statistics for both datasets\nprint_distribution_stats(df_train_sampled, \"Training Data\")\nprint_distribution_stats(df_test_sampled, \"Test Data\")\n\n# Additional summary statistics\nprint(f\"\\n{'-'*50}\")\nprint(f\"Summary Statistics:\")\nprint(f\"{'-'*50}\")\nprint(f\"Total transactions in original data: {len(df)}\")\nprint(f\"Total transactions in training data: {len(df_train_sampled)}\")\nprint(f\"Total transactions in test data: {len(df_test_sampled)}\")\nprint(f\"Training data categories: {df_train_sampled['category'].nunique()}\")\nprint(f\"Test data categories: {df_test_sampled['category'].nunique()}\")\nprint(f\"Training data subcategories: {df_train_sampled['subcategory'].nunique()}\")\nprint(f\"Test data subcategories: {df_test_sampled['subcategory'].nunique()}\")\n\n# Check for any subcategories in test but not in training\ntrain_subcats = set(df_train_sampled['subcategory'].unique())\ntest_subcats = set(df_test_sampled['subcategory'].unique())\ntest_only_subcats = test_subcats - train_subcats\n\nif test_only_subcats:\n    print(f\"\\nWarning: {len(test_only_subcats)} subcategories in test data but not in training data:\")\n    for subcat in test_only_subcats:\n        print(f\"  - {subcat}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:39:13.697978Z","iopub.execute_input":"2025-04-13T13:39:13.698764Z","iopub.status.idle":"2025-04-13T13:39:14.271672Z","shell.execute_reply.started":"2025-04-13T13:39:13.698681Z","shell.execute_reply":"2025-04-13T13:39:14.270613Z"}},"outputs":[{"name":"stdout","text":"Cleaning transaction notes...\nAdding category mapping...\nWarning: 377 rows have unmapped subcategories\nUnmapped subcategories: ['Clothes & shoes' 'Financial expenses' 'Food & Drinks' 'Free time'\n 'Housing' 'Life & Entertainment' 'Loan, interests' 'Missing' 'Others'\n 'Phone, cell phone' 'Shopping' 'Transportation']\nAdded categories to 12388 transactions\nFiltering out unmapped subcategories...\nFiltered out 377 rows with unmapped subcategories\nUnmapped subcategories: ['Clothes & shoes' 'Financial expenses' 'Food & Drinks' 'Free time'\n 'Housing' 'Life & Entertainment' 'Loan, interests' 'Missing' 'Others'\n 'Phone, cell phone' 'Shopping' 'Transportation']\nCreating balanced samples (50 per subcategory for training)...\nSaved sample data to /kaggle/working/transaction_sample_review.csv for review\nOriginal data: 12388 transactions\nFiltered data: 12011 transactions\nBalanced training data: 1467 transactions (52 subcategories)\nBalanced test data: 311 transactions (45 subcategories)\n\nSample of training data:\n                                        cleaned_note              category  \\\n0  REBEL MELBOURNE CTRL MELBOURNE VI AUSTap and P...  Life & Entertainment   \n1         DEC - LULULEMON ATHLETICA AUSTRAlbert Park  Life & Entertainment   \n2  STATE TRUSTEES LIMIT MELBOURNE AUSCard Value D...    Financial expenses   \n3                              BWS BOX HILL BOX HILL  Life & Entertainment   \n4                              BWS HWATHORN HWATHORN  Life & Entertainment   \n\n             subcategory  \n0  Active sport, fitness  \n1  Active sport, fitness  \n2               Advisory  \n3       Alcohol, tobacco  \n4       Alcohol, tobacco  \n\n--------------------------------------------------\nTraining Data Distribution Statistics:\n--------------------------------------------------\n\nCategories (11 unique):\nCategory                  Count      Percentage\n---------------------------------------------\nLife & Entertainment      375        25.6%\nShopping                  261        17.8%\nHousing                   160        10.9%\nIncome                    157        10.7%\nFood & Beverages          150        10.2%\nCommunication, PC         102        7.0%\nTransportation            83         5.7%\nFinancial expenses        63         4.3%\nTransfer                  50         3.4%\nInvestments               47         3.2%\nVehicle                   19         1.3%\n\nSubcategories (52 unique):\nSubcategory                    Category             Count      Percentage\n----------------------------------------------------------------------\nInterests, dividends           Income               50         3.4%\nSoftware, apps, games          Communication, PC    50         3.4%\nHome, garden                   Shopping             50         3.4%\nHealth and beauty              Shopping             50         3.4%\nGroceries                      Food & Beverages     50         3.4%\nInternet                       Communication, PC    50         3.4%\nMaintenance, repairs           Housing              50         3.4%\nPublic transport               Transportation       50         3.4%\nRefunds (tax, purchase)        Income               50         3.4%\nEnergy, utilities              Housing              50         3.4%\nRestaurant, fast-food          Food & Beverages     50         3.4%\nDrug-store, chemist            Shopping             50         3.4%\nTRANSFER                       Transfer             50         3.4%\nTV, Streaming                  Life & Entertainment 50         3.4%\nCharity, gifts                 Life & Entertainment 50         3.4%\nCharges, Fees                  Financial expenses   50         3.4%\nBooks, audio, subscriptions    Life & Entertainment 50         3.4%\nBar, cafe, drink, snacks       Food & Beverages     50         3.4%\nWage, invoices                 Income               50         3.4%\nHobbies                        Life & Entertainment 50         3.4%\nFinancial investments          Investments          46         3.1%\nElectronics, accessories       Shopping             44         3.0%\nHoliday, trips, hotels         Life & Entertainment 43         2.9%\nWellness, beauty               Life & Entertainment 41         2.8%\nGifts, joy                     Shopping             37         2.5%\nRent                           Housing              36         2.5%\nEducation, development         Life & Entertainment 34         2.3%\nTaxi                           Transportation       32         2.2%\nStationery, tools              Shopping             26         1.8%\nCulture, sport events          Life & Entertainment 21         1.4%\nServices                       Housing              18         1.2%\nHealth care, doctor            Life & Entertainment 14         1.0%\nLife events                    Life & Entertainment 12         0.8%\nInsurances                     Financial expenses   11         0.7%\nRentals                        Vehicle              11         0.7%\nParking                        Vehicle              6          0.4%\nMortgage                       Housing              6          0.4%\nGifts                          Income               5          0.3%\nLottery, gambling              Life & Entertainment 4          0.3%\nAlcohol, tobacco               Life & Entertainment 4          0.3%\nJewels, accessories            Shopping             3          0.2%\nActive sport, fitness          Life & Entertainment 2          0.1%\nPostal services                Communication, PC    2          0.1%\nSavings                        Investments          1          0.1%\nFuel                           Vehicle              1          0.1%\nDues & grants                  Income               1          0.1%\nPets, animals                  Shopping             1          0.1%\nChecks, coupons                Income               1          0.1%\nLong distance                  Transportation       1          0.1%\nVehicle insurance              Vehicle              1          0.1%\nAdvisory                       Financial expenses   1          0.1%\nFines                          Financial expenses   1          0.1%\n\nSubcategories with low counts (<5):\n  - Lottery, gambling: 4 transactions\n  - Alcohol, tobacco: 4 transactions\n  - Jewels, accessories: 3 transactions\n  - Active sport, fitness: 2 transactions\n  - Postal services: 2 transactions\n  - Savings: 1 transactions\n  - Fuel: 1 transactions\n  - Dues & grants: 1 transactions\n  - Pets, animals: 1 transactions\n  - Checks, coupons: 1 transactions\n  - Long distance: 1 transactions\n  - Vehicle insurance: 1 transactions\n  - Advisory: 1 transactions\n  - Fines: 1 transactions\n\n--------------------------------------------------\nTest Data Distribution Statistics:\n--------------------------------------------------\n\nCategories (11 unique):\nCategory                  Count      Percentage\n---------------------------------------------\nLife & Entertainment      90         28.9%\nShopping                  55         17.7%\nIncome                    35         11.3%\nHousing                   33         10.6%\nFood & Beverages          30         9.6%\nCommunication, PC         21         6.8%\nTransportation            18         5.8%\nFinancial expenses        14         4.5%\nTransfer                  10         3.2%\nInvestments               3          1.0%\nVehicle                   2          0.6%\n\nSubcategories (45 unique):\nSubcategory                    Category             Count      Percentage\n----------------------------------------------------------------------\nWellness, beauty               Life & Entertainment 10         3.2%\nTRANSFER                       Transfer             10         3.2%\nRestaurant, fast-food          Food & Beverages     10         3.2%\nMaintenance, repairs           Housing              10         3.2%\nSoftware, apps, games          Communication, PC    10         3.2%\nInternet                       Communication, PC    10         3.2%\nInterests, dividends           Income               10         3.2%\nHome, garden                   Shopping             10         3.2%\nHoliday, trips, hotels         Life & Entertainment 10         3.2%\nHobbies                        Life & Entertainment 10         3.2%\nHealth and beauty              Shopping             10         3.2%\nPublic transport               Transportation       10         3.2%\nGroceries                      Food & Beverages     10         3.2%\nEnergy, utilities              Housing              10         3.2%\nDrug-store, chemist            Shopping             10         3.2%\nWage, invoices                 Income               10         3.2%\nBar, cafe, drink, snacks       Food & Beverages     10         3.2%\nBooks, audio, subscriptions    Life & Entertainment 10         3.2%\nCharges, Fees                  Financial expenses   10         3.2%\nCharity, gifts                 Life & Entertainment 10         3.2%\nRefunds (tax, purchase)        Income               10         3.2%\nTV, Streaming                  Life & Entertainment 10         3.2%\nEducation, development         Life & Entertainment 10         3.2%\nElectronics, accessories       Shopping             10         3.2%\nGifts, joy                     Shopping             9          2.9%\nRent                           Housing              9          2.9%\nTaxi                           Transportation       8          2.6%\nHealth care, doctor            Life & Entertainment 7          2.3%\nLife events                    Life & Entertainment 5          1.6%\nGifts                          Income               4          1.3%\nStationery, tools              Shopping             4          1.3%\nServices                       Housing              3          1.0%\nInsurances                     Financial expenses   3          1.0%\nLottery, gambling              Life & Entertainment 3          1.0%\nFinancial investments          Investments          3          1.0%\nCulture, sport events          Life & Entertainment 3          1.0%\nRentals                        Vehicle              2          0.6%\nPostal services                Communication, PC    1          0.3%\nPets, animals                  Shopping             1          0.3%\nMortgage                       Housing              1          0.3%\nJewels, accessories            Shopping             1          0.3%\nAdvisory                       Financial expenses   1          0.3%\nDues & grants                  Income               1          0.3%\nAlcohol, tobacco               Life & Entertainment 1          0.3%\nActive sport, fitness          Life & Entertainment 1          0.3%\n\nSubcategories with low counts (<5):\n  - Gifts: 4 transactions\n  - Stationery, tools: 4 transactions\n  - Services: 3 transactions\n  - Insurances: 3 transactions\n  - Lottery, gambling: 3 transactions\n  - Financial investments: 3 transactions\n  - Culture, sport events: 3 transactions\n  - Rentals: 2 transactions\n  - Postal services: 1 transactions\n  - Pets, animals: 1 transactions\n  - Mortgage: 1 transactions\n  - Jewels, accessories: 1 transactions\n  - Advisory: 1 transactions\n  - Dues & grants: 1 transactions\n  - Alcohol, tobacco: 1 transactions\n  - Active sport, fitness: 1 transactions\n\n--------------------------------------------------\nSummary Statistics:\n--------------------------------------------------\nTotal transactions in original data: 12388\nTotal transactions in training data: 1467\nTotal transactions in test data: 311\nTraining data categories: 11\nTest data categories: 11\nTraining data subcategories: 52\nTest data subcategories: 45\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_30/2278908751.py:40: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  subcat_to_cat = df.groupby('subcategory')['category'].first().to_dict()\n/tmp/ipykernel_30/2278908751.py:40: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  subcat_to_cat = df.groupby('subcategory')['category'].first().to_dict()\n","output_type":"stream"}],"execution_count":198},{"cell_type":"markdown","source":"## Instruct the zero-shot prompt\nI draft the prompt asking it to only use the subcategory from the loaded table.","metadata":{}},{"cell_type":"code","source":" import sqlite3\n\ndb_file = \"transaction_categories.db\"\ndb_conn = sqlite3.connect(db_file)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:39:14.273225Z","iopub.execute_input":"2025-04-13T13:39:14.273656Z","iopub.status.idle":"2025-04-13T13:39:14.280028Z","shell.execute_reply.started":"2025-04-13T13:39:14.273609Z","shell.execute_reply":"2025-04-13T13:39:14.278660Z"}},"outputs":[],"execution_count":199},{"cell_type":"code","source":"def predict_category_and_subcategory(transaction_note):\n    \"\"\"\n    Predicts the category and subcategory for a transaction using the zero-shot system instruction approach.\n    Ensures that the subcategory belongs to the selected category based on database mappings.\n    \n    Args:\n        transaction_note: The transaction text to classify\n        \n    Returns:\n        tuple: (predicted_category, predicted_subcategory)\n    \"\"\"\n    try:\n        system_instruct = \"\"\"\n        You are a financial transaction categorization assistant. You will analyze a transaction description and classify it into the appropriate category and subcategory.\n\n        Follow these steps exactly:\n        1. First, select the most appropriate CATEGORY from the available options\n        2. Then, select a SUBCATEGORY that belongs to the selected CATEGORY\n\n        Important: You must ensure the subcategory you select belongs to the category you chose. The database has specific parent-child relationships between categories and subcategories.\n\n        Your response must use this exact format:\n        CATEGORY: [selected category name]\n        SUBCATEGORY: [selected subcategory name]\n        \"\"\"\n        \n        # First, get all category-subcategory mappings from the database to provide context\n        cursor = db_conn.cursor()\n        cursor.execute(\"\"\"\n        SELECT c.name as category, s.name as subcategory \n        FROM categories c\n        JOIN subcategories s ON c.category_id = s.category_id\n        ORDER BY c.name, s.name\n        \"\"\")\n        mappings = cursor.fetchall()\n        \n        # Create context about the hierarchical structure\n        hierarchy_context = \"Category and subcategory hierarchy from the database:\\n\"\n        current_category = None\n        for category, subcategory in mappings:\n            if category != current_category:\n                hierarchy_context += f\"\\n{category}:\\n\"\n                current_category = category\n            hierarchy_context += f\"  - {subcategory}\\n\"\n        \n        # Make prediction with system instruction and hierarchy context\n        response = client.models.generate_content(\n            model=\"gemini-2.0-flash\",\n            config=types.GenerateContentConfig(\n                system_instruction=system_instruct,\n                temperature=0.2,  # Lower temperature for more consistent results\n            ),\n            contents=[\n                hierarchy_context,\n                f\"Transaction description: {transaction_note}\\n\\nPlease categorize this transaction:\"\n            ]\n        )\n        \n        text = response.text.strip()\n        \n        # Extract category and subcategory\n        try:\n            category_line = [line for line in text.split('\\n') if line.startswith(\"CATEGORY:\")][0]\n            subcategory_line = [line for line in text.split('\\n') if line.startswith(\"SUBCATEGORY:\")][0]\n            \n            category = category_line.replace(\"CATEGORY:\", \"\").strip()\n            subcategory = subcategory_line.replace(\"SUBCATEGORY:\", \"\").strip()\n            \n            # Verify that the subcategory belongs to the category using the database\n            cursor.execute(\"\"\"\n            SELECT COUNT(*) FROM subcategories s\n            JOIN categories c ON s.category_id = c.category_id\n            WHERE c.name = ? AND s.name = ?\n            \"\"\", (category, subcategory))\n            \n            count = cursor.fetchone()[0]\n            if count == 0:\n                # If the model returned an invalid mapping, find a valid subcategory for the category\n                cursor.execute(\"\"\"\n                SELECT s.name FROM subcategories s\n                JOIN categories c ON s.category_id = c.category_id\n                WHERE c.name = ?\n                LIMIT 1\n                \"\"\", (category,))\n                \n                result = cursor.fetchone()\n                if result:\n                    # Use a valid subcategory for this category\n                    subcategory = result[0]\n                    return category, subcategory\n                else:\n                    # If category is invalid too, return error\n                    return \"(invalid category)\", \"(invalid subcategory)\"\n            \n            return category, subcategory\n            \n        except (IndexError, KeyError) as e:\n            return \"(parsing error)\", \"(parsing error)\"\n            \n    except Exception as e:\n        return f\"(error)\", f\"(error: {str(e)})\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:39:14.285082Z","iopub.execute_input":"2025-04-13T13:39:14.285516Z","iopub.status.idle":"2025-04-13T13:39:14.298909Z","shell.execute_reply.started":"2025-04-13T13:39:14.285480Z","shell.execute_reply":"2025-04-13T13:39:14.297456Z"}},"outputs":[],"execution_count":200},{"cell_type":"markdown","source":"## Evaluate baseline performance\n\nNow I perform an evaluation on the available models to ensure I can measure how much the tuning helps.","metadata":{}},{"cell_type":"code","source":"import tqdm\nfrom tqdm.rich import tqdm as tqdmr\nimport warnings\n\n# Enable tqdm features on Pandas\ntqdmr.pandas()\n\n# Suppress the experimental warning\nwarnings.filterwarnings(\"ignore\", category=tqdm.TqdmExperimentalWarning)\n\n# Further sample the test data to be mindful of the free-tier quota\nTEST_SAMPLE_SIZE = 20\ndf_baseline_eval = df_test_sampled.sample(min(TEST_SAMPLE_SIZE, len(df_test_sampled)))\n\n# Ensure category column exists in test data\nif 'category' not in df_baseline_eval.columns:\n    # Add categories using the database mapping\n    cursor = db_conn.cursor()\n    cursor.execute(\"\"\"\n    SELECT s.name as subcategory, c.name as category \n    FROM subcategories s\n    JOIN categories c ON s.category_id = c.category_id\n    \"\"\")\n    subcat_to_cat = {row[0]: row[1] for row in cursor.fetchall()}\n    df_baseline_eval['category'] = df_baseline_eval['subcategory'].map(subcat_to_cat)\n\nprint(f\"Evaluating {len(df_baseline_eval)} transactions...\")\n\n# Make predictions using the sampled data with progress bar\n# This will return both category and subcategory\ndf_baseline_eval[['predicted_category', 'predicted_subcategory']] = df_baseline_eval['note'].progress_apply(\n    lambda x: pd.Series(predict_category_and_subcategory(x))\n)\n\n# Calculate the accuracy for both category and subcategory\ncategory_accuracy = (df_baseline_eval['category'] == df_baseline_eval['predicted_category']).mean()\nsubcategory_accuracy = (df_baseline_eval['subcategory'] == df_baseline_eval['predicted_subcategory']).mean()\ncombined_accuracy = ((df_baseline_eval['category'] == df_baseline_eval['predicted_category']) & \n                     (df_baseline_eval['subcategory'] == df_baseline_eval['predicted_subcategory'])).mean()\n\nprint(f\"Category accuracy: {category_accuracy:.2%}\")\nprint(f\"Subcategory accuracy: {subcategory_accuracy:.2%}\")\nprint(f\"Combined accuracy (both correct): {combined_accuracy:.2%}\")\n\n# Display some examples of predictions\nprint(\"\\nSample predictions:\")\nsample_results = df_baseline_eval[['note', 'category', 'subcategory', \n                                  'predicted_category', 'predicted_subcategory']].sample(min(5, len(df_baseline_eval)))\n\nfor idx, row in sample_results.iterrows():\n    print(f\"Transaction: {row['note'][:50]}...\")\n    print(f\"True category: {row['category']}\")\n    print(f\"Predicted category: {row['predicted_category']}\")\n    print(f\"Category correct: {row['category'] == row['predicted_category']}\")\n    print(f\"True subcategory: {row['subcategory']}\")\n    print(f\"Predicted subcategory: {row['predicted_subcategory']}\")\n    print(f\"Subcategory correct: {row['subcategory'] == row['predicted_subcategory']}\\n\")\n\n# Create a confusion matrix for categories\nprint(\"Category confusion matrix:\")\ncat_matrix = pd.crosstab(\n    df_baseline_eval['category'], \n    df_baseline_eval['predicted_category'],\n    rownames=['True'], \n    colnames=['Predicted']\n)\nprint(cat_matrix)\n\n# Create a confusion matrix for subcategories with errors\nprint(\"\\nMost common subcategory error patterns:\")\nerror_patterns = df_baseline_eval[df_baseline_eval['subcategory'] != df_baseline_eval['predicted_subcategory']]\nif len(error_patterns) > 0:\n    error_counts = error_patterns.groupby(['subcategory', 'predicted_subcategory']).size().reset_index(name='count')\n    error_counts = error_counts.sort_values('count', ascending=False)\n    print(error_counts.head(5))\nelse:\n    print(\"No subcategory errors found in the evaluation set!\")\n\n# Analysis of hierarchical errors\nprint(\"\\nError analysis by hierarchy:\")\nhierarchical_errors = df_baseline_eval[\n    (df_baseline_eval['category'] == df_baseline_eval['predicted_category']) & \n    (df_baseline_eval['subcategory'] != df_baseline_eval['predicted_subcategory'])\n]\nprint(f\"Correct category but wrong subcategory: {len(hierarchical_errors)} cases ({len(hierarchical_errors)/len(df_baseline_eval):.2%})\")\n\ncategory_errors = df_baseline_eval[df_baseline_eval['category'] != df_baseline_eval['predicted_category']]\nprint(f\"Wrong category: {len(category_errors)} cases ({len(category_errors)/len(df_baseline_eval):.2%})\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:39:14.300912Z","iopub.execute_input":"2025-04-13T13:39:14.301380Z","iopub.status.idle":"2025-04-13T13:39:24.089423Z","shell.execute_reply.started":"2025-04-13T13:39:14.301331Z","shell.execute_reply":"2025-04-13T13:39:24.088385Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output()","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da1ceb9ee3af4dc495e3a5c2fc2a2472"}},"metadata":{}},{"name":"stdout","text":"Evaluating 20 transactions...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Category accuracy: 65.00%\nSubcategory accuracy: 50.00%\nCombined accuracy (both correct): 50.00%\n\nSample predictions:\nTransaction: Interest for 01/08/2021 - 31/08/2021...\nTrue category: Income\nPredicted category: Financial expenses\nCategory correct: False\nTrue subcategory: Interests, dividends\nPredicted subcategory: Loans, interests\nSubcategory correct: False\n\nTransaction: Admin Fee (%)...\nTrue category: Financial expenses\nPredicted category: Financial expenses\nCategory correct: True\nTrue subcategory: Charges, Fees\nPredicted subcategory: Charges, Fees\nSubcategory correct: True\n\nTransaction: COLES EASTLAND 7881 RINGWOOD...\nTrue category: Food & Beverages\nPredicted category: Food & Beverages\nCategory correct: True\nTrue subcategory: Groceries\nPredicted subcategory: Groceries\nSubcategory correct: True\n\nTransaction: AMAZON AU SYDNEY SOUTH大脑药...\nTrue category: Shopping\nPredicted category: Shopping\nCategory correct: True\nTrue subcategory: Drug-store, chemist\nPredicted subcategory: Electronics, accessories\nSubcategory correct: False\n\nTransaction: Inward Debit Direct Entry QENERGY 347156835 034219...\nTrue category: Housing\nPredicted category: Housing\nCategory correct: True\nTrue subcategory: Energy, utilities\nPredicted subcategory: Energy, utilities\nSubcategory correct: True\n\nCategory confusion matrix:\nPredicted             Communication, PC  Financial expenses  Food & Beverages  \\\nTrue                                                                            \nFinancial expenses                    0                   2                 0   \nFood & Beverages                      0                   0                 2   \nHousing                               0                   0                 0   \nIncome                                0                   1                 0   \nLife & Entertainment                  1                   0                 1   \nShopping                              0                   0                 0   \nVehicle                               0                   0                 0   \n\nPredicted             Housing  Income  Shopping  Vehicle  \nTrue                                                      \nFinancial expenses          0       1         0        0  \nFood & Beverages            0       0         0        0  \nHousing                     3       0         0        0  \nIncome                      0       2         1        0  \nLife & Entertainment        0       0         1        1  \nShopping                    0       0         3        0  \nVehicle                     0       0         0        1  \n\nMost common subcategory error patterns:\n                  subcategory     predicted_subcategory  count\n51             Charity, gifts         Health and beauty      1\n16           Alcohol, tobacco  Bar, cafe, drink, snacks      1\n47              Charges, Fees            Wage, invoices      1\n188      Interests, dividends          Loans, interests      1\n91   Electronics, accessories         Health and beauty      1\n\nError analysis by hierarchy:\nCorrect category but wrong subcategory: 3 cases (15.00%)\nWrong category: 7 cases (35.00%)\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_30/3292486879.py:73: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  error_counts = error_patterns.groupby(['subcategory', 'predicted_subcategory']).size().reset_index(name='count')\n","output_type":"stream"}],"execution_count":201},{"cell_type":"markdown","source":"## Tune my model\nNow I train the model with training data to tune it for assessment and potential use.","metadata":{}},{"cell_type":"markdown","source":"from collections.abc import Iterable\nimport datetime\nimport time\nimport os\n\n# 1. Prepare your transaction data for fine-tuning\nprint(\"Preparing transaction data for fine-tuning...\")\n\n# Convert the DataFrame into the format expected by the API\ntraining_examples = []\nfor _, row in df_train_sampled.iterrows():\n    training_examples.append({\n        \"textInput\": str(row['note']),\n        \"output\": str(row['subcategory'])\n    })\n\nprint(f\"Created {len(training_examples)} training examples\")\nprint(f\"Sample example - Input: '{training_examples[0]['textInput'][:50]}...'\")\nprint(f\"Sample example - Output: '{training_examples[0]['output']}'\")\n\n# 2. Prepare the dataset in the required format\ntraining_data = {\"examples\": training_examples}\n\n# 3. Set up the fine-tuning job - find existing or create new\nmodel_id = None\n\ntry:\n    # Try to read previous model ID from file\n    try:\n        with open(\"tuned_model_id.txt\", \"r\") as f:\n            saved_model_id = f.read().strip()\n            if saved_model_id:\n                print(f\"Found previously saved model ID: {saved_model_id}\")\n                model_id = saved_model_id\n    except FileNotFoundError:\n        print(\"No previously saved model ID found.\")\n    \n    # If no saved ID, check for existing models\n    if not model_id:\n        queued_model = None\n        print(\"Checking for existing tuned models...\")\n        \n        # List models in reverse order (newest first)\n        for m in reversed(client.tunings.list()):\n            # Look for transaction classifier models with flexible matching\n            if (\"transaction\" in m.name.lower() or\n                m.name.startswith('tunedModels/personal-transaction-classifier-')):\n                \n                print(f\"Found potential model: {m.name} in state: {m.state.name}\")\n                \n                # If there is a completed model, use it\n                if m.state.name == 'JOB_STATE_SUCCEEDED':\n                    model_id = m.name\n                    print(f'Found existing completed model to reuse: {model_id}')\n                    break\n                elif m.state.name == 'JOB_STATE_RUNNING' and not queued_model:\n                    # If there's a model still running, remember it\n                    queued_model = m.name\n                    print(f'Found model still in progress: {queued_model}')\n        \n        # Use queued model if found and no completed model\n        if not model_id and queued_model:\n            model_id = queued_model\n            print(f'Using in-progress model: {model_id}')\n    \n    # Create new model if needed\n    if not model_id:\n        print(\"Starting new fine-tuning job...\")\n        tuning_op = client.tunings.tune(\n            base_model=\"models/gemini-1.5-flash-001-tuning\",\n            training_dataset=training_data,\n            config=types.CreateTuningJobConfig(\n                tuned_model_display_name=\"transaction-category-classifier\",  \n                batch_size=16,\n                epoch_count=3,\n            ),\n        )\n        \n        model_id = tuning_op.name\n        print(f\"Fine-tuning initiated. Model ID: {model_id}\")\n        print(f\"Current status: {tuning_op.state}\")\n        \n        # Poll for status updates (optional)\n        print(\"Initial training status:\")\n        print(f\"  - State: {tuning_op.state}\")\n        print(f\"  - Create time: {tuning_op.create_time}\")\n        if hasattr(tuning_op, 'progress') and tuning_op.progress:\n            print(f\"  - Progress: {tuning_op.progress}%\")\n    \n    # Save the model ID for later use\n    with open(\"tuned_model_id.txt\", \"w\") as f:\n        f.write(model_id)\n    \n    print(f\"\\nUsing model: {model_id}\")\n    print(\"This ID has been saved and will be used for predictions\")\n    \nexcept Exception as e:\n    print(f\"Error in fine-tuning process: {e}\")","metadata":{"execution":{"iopub.execute_input":"2025-04-10T05:48:56.388511Z","iopub.status.busy":"2025-04-10T05:48:56.387205Z","iopub.status.idle":"2025-04-10T05:48:58.145557Z","shell.execute_reply":"2025-04-10T05:48:58.144833Z","shell.execute_reply.started":"2025-04-10T05:48:56.388457Z"}}},{"cell_type":"markdown","source":"## Monitoring progress\nHere I monitor whether this model has been tuned and ready to use.","metadata":{}},{"cell_type":"code","source":"# 4. Monitor the fine-tuning progress\nstart_time = datetime.datetime.now(datetime.timezone.utc)\ntuned_model = client.tunings.get(name=model_id)\n\nwhile not tuned_model.has_ended:\n    print(f\"Current state: {tuned_model.state.name}\")\n    if hasattr(tuned_model, 'progress'):\n        print(f\"Progress: {tuned_model.progress}%\")\n    \n    time.sleep(60)  # Check every minute\n    tuned_model = client.tunings.get(name=model_id)\n\nprint(f\"Done! The model state is: {tuned_model.state.name}\")\n\nif not tuned_model.has_succeeded and tuned_model.error:\n    print(\"Error:\", tuned_model.error)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:39:24.090809Z","iopub.execute_input":"2025-04-13T13:39:24.091135Z","iopub.status.idle":"2025-04-13T13:39:24.130552Z","shell.execute_reply.started":"2025-04-13T13:39:24.091104Z","shell.execute_reply":"2025-04-13T13:39:24.129049Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[202], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 4. Monitor the fine-tuning progress\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m start_time \u001b[38;5;241m=\u001b[39m \u001b[43mdatetime\u001b[49m\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow(datetime\u001b[38;5;241m.\u001b[39mtimezone\u001b[38;5;241m.\u001b[39mutc)\n\u001b[1;32m      3\u001b[0m tuned_model \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mtunings\u001b[38;5;241m.\u001b[39mget(name\u001b[38;5;241m=\u001b[39mmodel_id)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tuned_model\u001b[38;5;241m.\u001b[39mhas_ended:\n","\u001b[0;31mNameError\u001b[0m: name 'datetime' is not defined"],"ename":"NameError","evalue":"name 'datetime' is not defined","output_type":"error"}],"execution_count":202},{"cell_type":"markdown","source":"## Evaluate Tuned Model\nHere I test and evaluate the performance of the tuned model.","metadata":{}},{"cell_type":"code","source":"# 5. Test the tuned model with a sample transaction\ndef categorize_transaction(transaction_note):\n    \"\"\"Use the fine-tuned model to categorize a transaction.\"\"\"\n    response = client.models.generate_content(\n        model=model_id,\n        contents=transaction_note,\n        config=types.GenerateContentConfig(\n            temperature=0.0,  # Use deterministic output for classification\n            max_output_tokens=10,  # Keep it short, we just need the category\n        )\n    )\n    \n    if response.candidates and response.candidates[0].content:\n        return response.candidates[0].content.parts[0].text.strip()\n    else:\n        return \"(error)\"\n\n# Test with a sample transaction\nsample_transaction = \"AMAZON PRIME MEMBERSHIP ANNUAL RENEWAL\"\npredicted_category = categorize_transaction(sample_transaction)\nprint(f\"Transaction: {sample_transaction}\")\nprint(f\"Predicted category: {predicted_category}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:39:24.131608Z","iopub.status.idle":"2025-04-13T13:39:24.131998Z","shell.execute_reply.started":"2025-04-13T13:39:24.131827Z","shell.execute_reply":"2025-04-13T13:39:24.131845Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 6. Evaluate the model on test data\nimport tqdm\nfrom tqdm.rich import tqdm as tqdmr\nimport warnings\n\n# Enable tqdm features on Pandas\ntqdmr.pandas()\n\n# Suppress the experimental warning\nwarnings.filterwarnings(\"ignore\", category=tqdm.TqdmExperimentalWarning)\n\n# Sample a subset of test data for evaluation\nTEST_SAMPLE_SIZE = 20\ndf_eval = df_test_sampled.sample(min(TEST_SAMPLE_SIZE, len(df_test_sampled)))\n\nprint(f\"Evaluating on {len(df_eval)} test transactions...\")\n\n# Make predictions with progress bar\ndf_eval['prediction'] = df_eval['note'].progress_apply(categorize_transaction)\n\n# Calculate accuracy\naccuracy = (df_eval['subcategory'] == df_eval['prediction']).mean()\nprint(f\"Model accuracy: {accuracy:.2%}\")\n\n# Display some examples\nprint(\"\\nSample predictions:\")\nfor idx, row in df_eval.sample(min(5, len(df_eval))).iterrows():\n    print(f\"Transaction: {row['note'][:50]}...\")\n    print(f\"True category: {row['subcategory']}\")\n    print(f\"Predicted: {row['prediction']}\")\n    print(f\"Correct: {row['subcategory'] == row['prediction']}\\n\")\n\n# Show error analysis\nerrors = df_eval[df_eval['subcategory'] != df_eval['prediction']]\nif len(errors) > 0:\n    print(f\"Found {len(errors)} misclassifications\")\n    print(\"Most common error patterns:\")\n    error_matrix = pd.crosstab(\n        errors['subcategory'], \n        errors['prediction'], \n        rownames=['True'], \n        colnames=['Predicted']\n    )\n    print(error_matrix)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:39:24.134087Z","iopub.status.idle":"2025-04-13T13:39:24.134457Z","shell.execute_reply.started":"2025-04-13T13:39:24.134289Z","shell.execute_reply":"2025-04-13T13:39:24.134307Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def list_tables() -> list[str]:\n    \"\"\"Retrieve the names of all tables in the database.\"\"\"\n    # Include print logging statements so you can see when functions are being called.\n    print(' - DB CALL: list_tables()')\n\n    cursor = db_conn.cursor()\n\n    # Fetch the table names.\n    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n\n    tables = cursor.fetchall()\n    return [t[0] for t in tables]\n\n\nlist_tables()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:39:24.135559Z","iopub.status.idle":"2025-04-13T13:39:24.135975Z","shell.execute_reply.started":"2025-04-13T13:39:24.135795Z","shell.execute_reply":"2025-04-13T13:39:24.135815Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def describe_table(table_name: str) -> list[tuple[str, str]]:\n    \"\"\"Look up the table schema.\n\n    Returns:\n      List of columns, where each entry is a tuple of (column, type).\n    \"\"\"\n    print(f' - DB CALL: describe_table({table_name})')\n\n    cursor = db_conn.cursor()\n\n    cursor.execute(f\"PRAGMA table_info({table_name});\")\n\n    schema = cursor.fetchall()\n    # [column index, column name, column type, ...]\n    return [(col[1], col[2]) for col in schema]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:39:24.138228Z","iopub.status.idle":"2025-04-13T13:39:24.138587Z","shell.execute_reply.started":"2025-04-13T13:39:24.138420Z","shell.execute_reply":"2025-04-13T13:39:24.138438Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def execute_query(sql: str) -> list[list[str]]:\n    \"\"\"Execute an SQL statement, returning the results.\"\"\"\n    print(f' - DB CALL: execute_query({sql})')\n\n    cursor = db_conn.cursor()\n\n    cursor.execute(sql)\n    return cursor.fetchall()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:39:24.140844Z","iopub.status.idle":"2025-04-13T13:39:24.141229Z","shell.execute_reply.started":"2025-04-13T13:39:24.141056Z","shell.execute_reply":"2025-04-13T13:39:24.141075Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# These are the database interaction tools defined earlier\ndb_tools = [list_tables, describe_table, execute_query]\n\n# System instruction for the AI to understand what it needs to do\ninstruction = \"\"\"You are a helpful chatbot that can interact with an SQL database for financial transactions. \nYou will first use list_tables to see what tables are present, describe_table to understand the\nschema, and execute_query to issue an SQL SELECT query to retrieve all category-subcategory combinations.\"\"\"\n\n# Create the Google Genai client\nclient = genai.Client(api_key=GOOGLE_API_KEY)\n\nfrom google.api_core import retry\nimport pandas as pd\n\n# Define retry logic for API rate limits\nis_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n\n# User query to get direct category-subcategory combinations\nuser_query = \"\"\"1. Find all unique combinations of category and subcategory from the database.\n2. Execute a SQL query that joins the categories and subcategories tables.\n3. Return the full category name and full subcategory name.\n4. Format your response as simple tabular data that can be saved as CSV.\"\"\"\n\n# Function to get category-subcategory combinations with retry logic\n@retry.Retry(predicate=is_retriable)\ndef get_category_subcategory_combinations():\n    response = client.models.generate_content(\n        model=\"gemini-2.0-flash\",\n        contents=user_query,\n        config=types.GenerateContentConfig(\n            system_instruction=instruction,\n            tools=db_tools,\n        ),\n    )\n    return response.text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:39:24.142985Z","iopub.status.idle":"2025-04-13T13:39:24.143358Z","shell.execute_reply.started":"2025-04-13T13:39:24.143188Z","shell.execute_reply":"2025-04-13T13:39:24.143207Z"}},"outputs":[],"execution_count":null}]}