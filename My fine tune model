{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"colab":{"name":"day-4-fine-tuning-a-custom-model.ipynb","toc_visible":true},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11456905,"sourceType":"datasetVersion","datasetId":7129309}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/robbietli/financial-transaction-categoriser?scriptVersionId=234594944\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"## Categorise finance transactions\n\nIn life, my financial transactions are often categorised incorrectly in my budgeting app. I decided to find a better solution.\n\nIn this example, I will first try to categorise with an existing Gemini model using a few-shot prompt and evaluate its performance. Then I will tune a model with the data categorised by me and evaluate its performance","metadata":{}},{"cell_type":"code","source":"# Install required libraries\n!pip uninstall -qqy jupyterlab  # Remove unused conflicting packages\n!pip install -U -q \"google-genai==1.7.0\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T06:19:56.184131Z","iopub.execute_input":"2025-04-18T06:19:56.185185Z","iopub.status.idle":"2025-04-18T06:20:08.379224Z","shell.execute_reply.started":"2025-04-18T06:19:56.185141Z","shell.execute_reply":"2025-04-18T06:20:08.377759Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Import necessary libraries\nfrom google import genai\nfrom google.genai import types\n\ngenai.__version__","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T06:20:08.382353Z","iopub.execute_input":"2025-04-18T06:20:08.382895Z","iopub.status.idle":"2025-04-18T06:20:08.392029Z","shell.execute_reply.started":"2025-04-18T06:20:08.382832Z","shell.execute_reply":"2025-04-18T06:20:08.390847Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\n\nGOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n\nclient = genai.Client(api_key=GOOGLE_API_KEY)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T06:20:08.393162Z","iopub.execute_input":"2025-04-18T06:20:08.393503Z","iopub.status.idle":"2025-04-18T06:20:08.718939Z","shell.execute_reply.started":"2025-04-18T06:20:08.393456Z","shell.execute_reply":"2025-04-18T06:20:08.717766Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Load the subcategory and category table\nIn this step, I load the subcategory, category table.","metadata":{}},{"cell_type":"code","source":"import sqlite3\n\n# Connect to your database\ndb_conn = sqlite3.connect('/kaggle/working/transaction_categories.db')\ncursor = db_conn.cursor()\n\n# Create the tables\ncursor.execute('''\nCREATE TABLE IF NOT EXISTS categories (\n    category_id INTEGER PRIMARY KEY AUTOINCREMENT,\n    name VARCHAR(100) NOT NULL UNIQUE,\n    description TEXT,\n    display_order INT DEFAULT 100,\n    is_active BOOLEAN DEFAULT 1,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n)\n''')\n\ncursor.execute('''\nCREATE TABLE IF NOT EXISTS subcategories (\n    subcategory_id INTEGER PRIMARY KEY AUTOINCREMENT,\n    category_id INTEGER NOT NULL,\n    name VARCHAR(100) NOT NULL,\n    description TEXT,\n    display_order INT DEFAULT 100,\n    is_active BOOLEAN DEFAULT 1,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    FOREIGN KEY (category_id) REFERENCES categories(category_id)\n)\n''')\n\n\n# Insert main categories\ncategories = [\n    ('Food & Beverages', 'Expenses related to food and drinks', 10),\n    ('Shopping', 'Retail purchases and shopping expenses', 20),\n    ('Housing', 'Home-related expenses including rent and utilities', 30),\n    ('Transportation', 'Public and private transportation costs', 40),\n    ('Vehicle', 'Car and vehicle related expenses', 50),\n    ('Life & Entertainment', 'Leisure activities and entertainment', 60),\n    ('Communication, PC', 'Internet, phone and computer expenses', 70),\n    ('Financial expenses', 'Banking fees, loans, and financial costs', 80),\n    ('Investments', 'Investment-related transactions', 90),\n    ('Income', 'All sources of incoming money', 100),\n    ('TRANSFER', 'Money transfers between accounts', 110)\n]\n\ncursor.executemany('INSERT OR IGNORE INTO categories (name, description, display_order) VALUES (?, ?, ?)', categories)\n\n# Insert subcategories for Food & Beverages\nfood_subcategories = [\n    (1, 'Bar, cafe, drink, snacks', 10),\n    (1, 'Groceries', 20),\n    (1, 'Restaurant, fast-food', 30)\n]\ncursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', food_subcategories)\n\n# Insert subcategories for Shopping\nshopping_subcategories = [\n    (2, 'Clothes & Footwear', 10),\n    (2, 'Drug-store, chemist', 20),\n    (2, 'Electronics, accessories', 30),\n    (2, 'Gifts, joy', 40),\n    (2, 'Health and beauty', 50),\n    (2, 'Home, garden', 60),\n    (2, 'Jewels, accessories', 70),\n    (2, 'Kids', 80),\n    (2, 'Leisure time', 90),\n    (2, 'Pets, animals', 100),\n    (2, 'Stationery, tools', 110)\n]\ncursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', shopping_subcategories)\n\nhousing_subcategories=[\n    (3, 'Energy, utilities', 10),\n    (3, 'Maintenance, repairs', 20),\n    (3, 'Mortgage', 30),\n    (3, 'Property insurance', 40),\n    (3, 'Rent', 50),\n    (3, 'Services', 60)\n]\ncursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', housing_subcategories)\n\n\ntransportation_subcategories=[\n    (4, 'Business trips', 10),\n    (4, 'Long distance', 20),\n    (4, 'Public transport', 30),\n    (4, 'Taxi', 40)\n]\ncursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', transportation_subcategories)\n\nvehicle_subcategories=[\n    (5, 'Fuel', 10),\n    (5, 'Leasing', 20),\n    (5, 'Parking', 30),\n    (5, 'Rentals', 40),\n    (5, 'Vehicle insurance', 50),\n    (5, 'Vehicle maintenance', 60)\n]\ncursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', vehicle_subcategories)\n\nlife_subcategories=[\n    (6, 'Active sport, fitness', 10),\n    (6, 'Alcohol, tobacco', 20),\n    (6, 'Books, audio, subscriptions', 30),\n    (6, 'Charity, gifts', 40),\n    (6, 'Culture, sport events', 50),\n    (6, 'Education, development', 60),\n    (6, 'Health care, doctor', 70),\n    (6, 'Hobbies', 80),\n    (6, 'Holiday, trips, hotels', 90),\n    (6, 'Life events', 100),\n    (6, 'Lottery, gambling', 110),\n    (6, 'TV, Streaming', 120),\n    (6, 'Wellness, beauty', 130)\n]\ncursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', life_subcategories)\n\ncommunication_subcategories=[\n    (7, 'Internet', 10),\n    (7, 'Postal services', 20),\n    (7, 'Software, apps, games', 30),\n    (7, 'Telephony, mobile phone', 40)\n]\ncursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', communication_subcategories)\n\nfinancial_subcategories=[\n    (8, 'Advisory', 10),\n    (8, 'Charges, Fees', 20),\n    (8, 'Child Support', 30),\n    (8, 'Fines', 40),\n    (8, 'Insurances', 50),\n    (8, 'Loans, interests', 60),\n    (8, 'Taxes', 70)\n]\ncursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', financial_subcategories)\n\ninvestments_subcategories=[\n    (9, 'Collections', 10),\n    (9, 'Financial investments', 20),\n    (9, 'Realty', 30),\n    (9, 'Savings', 40),\n    (9, 'Vehicles, chattels', 50)\n]\ncursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', investments_subcategories)\n\nincome_subcategories=[\n    (10, 'Checks, coupons', 10),\n    (10, 'Child Support', 20),\n    (10, 'Dues & grants', 30),\n    (10, 'Gifts', 40),\n    (10, 'Interests, dividends', 50),\n    (10, 'Lending, renting', 60),\n    (10, 'Lottery earning', 70),\n    (10, 'Refunds (tax, purchase)', 80),\n    (10, 'Rental income', 90),\n    (10, 'Sale', 100),\n    (10, 'Wage, invoices', 110)\n]\ncursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', income_subcategories)\n\ntransfer_subcatgories=[\n    (11, 'TRANSFER', 10),   \n]\ncursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', transfer_subcatgories)\n\n# Commit the changes\ndb_conn.commit()\nprint(\"Database schema created successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T06:20:08.723996Z","iopub.execute_input":"2025-04-18T06:20:08.72439Z","iopub.status.idle":"2025-04-18T06:20:08.760951Z","shell.execute_reply.started":"2025-04-18T06:20:08.724353Z","shell.execute_reply":"2025-04-18T06:20:08.760025Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"In this step, I create the mapping beween category and subcategory","metadata":{}},{"cell_type":"code","source":"def setup_database_and_get_hierarchy(output_path=\"/kaggle/working/category_mapping.csv\"):\n    \"\"\"\n    Initialize database, return category hierarchy, and output a simple mapping CSV.\n    \n    Args:\n        output_path: Path to save the mapping CSV\n        \n    Returns:\n        tuple: (db_connection, category_hierarchy_dict, subcategory_to_category_mapping)\n    \"\"\"\n    import sqlite3\n    import pandas as pd\n    \n    print(\"Setting up database and extracting category hierarchy...\")\n    \n    # Create database connection\n    db_conn = sqlite3.connect('/kaggle/working/transaction_categories.db')\n    cursor = db_conn.cursor()\n    \n    # Create tables and populate data if needed (your existing code)\n    # ... (Keep your existing table creation and population code)\n    \n    # Get complete hierarchy in one operation\n    cursor.execute(\"\"\"\n    SELECT \n        c.name as category, \n        s.name as subcategory\n    FROM categories c\n    JOIN subcategories s ON c.category_id = s.category_id\n    ORDER BY c.display_order, s.display_order\n    \"\"\")\n    \n    # Convert query results to DataFrame\n    results = cursor.fetchall()\n    mapping_df = pd.DataFrame(results, columns=['category', 'subcategory'])\n    \n    # Process results into usable format for return values\n    category_hierarchy = {}\n    subcat_to_cat_mapping = {}\n    \n    for category, subcategory in results:\n        # Build hierarchy dictionary\n        if category not in category_hierarchy:\n            category_hierarchy[category] = []\n        category_hierarchy[category].append(subcategory)\n        \n        # Build mapping dictionary\n        subcat_to_cat_mapping[subcategory] = category\n    \n    # Save to CSV file\n    mapping_df.to_csv(output_path, index=False)\n    \n    # Print summary\n    print(f\"\\nCategory-subcategory mapping saved to {output_path}\")\n    print(f\"Found {len(mapping_df['category'].unique())} categories and {len(mapping_df)} subcategories\")\n    print(\"\\nSample of mapping:\")\n    print(mapping_df.head(5))\n    \n    db_conn.commit()\n    \n    return db_conn, category_hierarchy, subcat_to_cat_mapping","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T06:20:08.762146Z","iopub.execute_input":"2025-04-18T06:20:08.762567Z","iopub.status.idle":"2025-04-18T06:20:08.775657Z","shell.execute_reply.started":"2025-04-18T06:20:08.762522Z","shell.execute_reply":"2025-04-18T06:20:08.774412Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Load the dataset\n\nI have uploaded transaction data categorised by me. Then I group it into training data and test data.","metadata":{"id":"peFm0w_0c1CO"}},{"cell_type":"code","source":"# Load and preprocess transaction data\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Load your data\nfile_path = \"/kaggle/input/training/categorized_transaction.csv\"\ndf = pd.read_csv(file_path)\n\n# Split into train and test sets (80/20 split)\ndf_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n\n# Display the subcategories (labels) in your dataset\nsubcategories = df['subcategory'].unique()\nprint(f\"Number of subcategories: {len(subcategories)}\")\nprint(\"Sample subcategories:\", subcategories[:10])  # Show first 10 subcategories\n\n# Quick look at note examples\nprint(\"\\nSample notes:\")\nfor i, note in enumerate(df['note'].head(3)):\n    print(f\"{i+1}. {note} → {df['subcategory'].iloc[i]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T06:20:08.77729Z","iopub.execute_input":"2025-04-18T06:20:08.777783Z","iopub.status.idle":"2025-04-18T06:20:08.836678Z","shell.execute_reply.started":"2025-04-18T06:20:08.77772Z","shell.execute_reply":"2025-04-18T06:20:08.835535Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Clean the data","metadata":{}},{"cell_type":"code","source":"import re\nimport pandas as pd\n\ndef clean_transaction_note(note):\n    \"\"\"\n    Clean transaction notes to remove common bank formatting, dates, card numbers, etc.\n    \"\"\"\n    # Handle None or empty strings\n    if note is None or pd.isna(note) or note == \"\":\n        return \"\"\n    \n    # Convert to string if needed\n    text = str(note)\n    \n    # Replace non-ASCII characters\n    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n    \n    # Extract main part of the transaction (before common transaction markers)\n    transaction_markers = r'\\s+(?:CREDIT CARD PURCHASE|EFTPOS|Value Date|tap and Pay|Card Purchase|CARD PURCHASE)'\n    parts = re.split(transaction_markers, text, flags=re.IGNORECASE)\n    main_text = parts[0] if parts else text\n    \n    # Clean amount figures and currency symbols\n    main_text = re.sub(r'(?:[$€£¥]|AUD|USD|EUR|GBP|NZD)\\s*[-+]?(?:\\d{1,3}(?:,\\d{3})*|\\d+)(?:\\.\\d{1,2})?', '', main_text)\n    main_text = re.sub(r'\\b[-+]?(?:\\d{1,3}(?:,\\d{3})*|\\d+)(?:\\.\\d{1,2})?\\b', '', main_text)\n    \n    # Remove card numbers (masked or full)\n    main_text = re.sub(r'(?:x{2,4}|X{2,4})\\d{4}|\\b\\d{4}[\\s-]?\\d{4}[\\s-]?\\d{4}[\\s-]?\\d{4}\\b', '', main_text)\n    \n    # Remove date patterns\n    date_pattern = r'(?:\\d{1,2}\\s+(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\\s+\\d{2,4}|' + \\\n                   r'(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\\s+\\d{1,2}(?:st|nd|rd|th)?\\s+\\d{2,4}|' + \\\n                   r'\\d{1,2}[-/.]\\d{1,2}[-/.]\\d{2,4}|' + \\\n                   r'\\d{4}[-/.]\\d{1,2}[-/.]\\d{1,2}|' + \\\n                   r'\\[Eff\\s+Date:.*?\\]|' + \\\n                   r'Value\\s+Date[_:]\\s*\\d{1,2}[-/.]\\d{1,2}[-/.]\\d{2,4})'\n    main_text = re.sub(date_pattern, '', main_text, flags=re.IGNORECASE)\n    \n    # Clean whitespace and punctuation\n    main_text = re.sub(r'\\s+', ' ', main_text)\n    main_text = re.sub(r'[\\s,.-]+$|^[\\s,.-]+', '', main_text)\n    main_text = re.sub(r'\\s+([,.])', r'\\1', main_text)\n    \n    return main_text.strip()[:500]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T06:20:08.838291Z","iopub.execute_input":"2025-04-18T06:20:08.838658Z","iopub.status.idle":"2025-04-18T06:20:08.847222Z","shell.execute_reply.started":"2025-04-18T06:20:08.838621Z","shell.execute_reply":"2025-04-18T06:20:08.846108Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def add_category_column(df, db_conn):\n    \"\"\"\n    Add category column to DataFrame based on subcategory using database mapping.\n    \"\"\"\n    if 'category' in df.columns:\n        print(\"Category column already exists\")\n        return df\n    \n    try:\n        # Query the database for subcategory to category mapping\n        cursor = db_conn.cursor()\n        cursor.execute(\"\"\"\n        SELECT s.name as subcategory, c.name as category \n        FROM subcategories s\n        JOIN categories c ON s.category_id = c.category_id\n        \"\"\")\n        \n        # Create mapping dictionary\n        subcat_to_cat = {row[0]: row[1] for row in cursor.fetchall()}\n        \n        # Add category column\n        df_with_category = df.copy()\n        df_with_category['category'] = df['subcategory'].map(subcat_to_cat)\n        \n        # Check for unmapped subcategories\n        missing_count = df_with_category['category'].isna().sum()\n        if missing_count > 0:\n            print(f\"Warning: {missing_count} rows have unmapped subcategories\")\n            unmapped = df[df['subcategory'].map(lambda x: x not in subcat_to_cat)]['subcategory'].unique()\n            print(f\"Unmapped subcategories: {unmapped}\")\n            \n        # Fill missing with placeholder\n        df_with_category['category'] = df_with_category['category'].fillna(\"Unknown\")\n        \n        print(f\"Added categories to {len(df_with_category)} transactions\")\n        return df_with_category\n        \n    except Exception as e:\n        print(f\"Error getting category mapping: {e}\")\n        # Create placeholder category column if needed\n        df_copy = df.copy()\n        df_copy['category'] = \"Unknown\"\n        return df_copy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T06:20:08.848963Z","iopub.execute_input":"2025-04-18T06:20:08.849515Z","iopub.status.idle":"2025-04-18T06:20:08.863603Z","shell.execute_reply.started":"2025-04-18T06:20:08.849479Z","shell.execute_reply":"2025-04-18T06:20:08.862156Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def sample_balanced_data(df, samples_per_label):\n    \"\"\"\n    Create a balanced dataset by sampling evenly across subcategories.\n    If a subcategory has fewer than the requested samples, uses all available rows.\n    \n    Args:\n        df: DataFrame containing transaction data\n        samples_per_label: Number of samples to select for each subcategory\n        \n    Returns:\n        DataFrame with balanced samples across subcategories\n    \"\"\"\n    # Group by subcategory and sample\n    sampled_df = (\n        df.groupby(\"subcategory\")[df.columns]\n        .apply(lambda x: x.sample(min(len(x), samples_per_label)))\n        .reset_index(drop=True)\n    )\n    \n    # Convert to category type for efficiency\n    sampled_df[\"subcategory\"] = sampled_df[\"subcategory\"].astype(\"category\")\n    \n    return sampled_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T06:20:08.865071Z","iopub.execute_input":"2025-04-18T06:20:08.865404Z","iopub.status.idle":"2025-04-18T06:20:08.8802Z","shell.execute_reply.started":"2025-04-18T06:20:08.865372Z","shell.execute_reply":"2025-04-18T06:20:08.879059Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def sample_balanced_data_combined(df, samples_per_label):\n    \"\"\"\n    Create a balanced dataset by sampling evenly across combined labels.\n    If a label has fewer than the requested samples, uses all available rows.\n    \"\"\"\n    # Group by combined label and sample\n    sampled_df = (\n        df.groupby(\"combined_label\")[df.columns]\n        .apply(lambda x: x.sample(min(len(x), samples_per_label)))\n        .reset_index(drop=True)\n    )\n    \n    # Convert to category type for efficiency\n    sampled_df[\"combined_label\"] = sampled_df[\"combined_label\"].astype(\"category\")\n    \n    return sampled_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T06:20:08.883474Z","iopub.execute_input":"2025-04-18T06:20:08.883937Z","iopub.status.idle":"2025-04-18T06:20:08.897623Z","shell.execute_reply.started":"2025-04-18T06:20:08.883887Z","shell.execute_reply":"2025-04-18T06:20:08.896407Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def filter_unmapped_subcategories(df, db_conn):\n    \"\"\"\n    Filter out rows with subcategories that don't have a mapping in the database.\n    \n    Args:\n        df: DataFrame containing transaction data\n        db_conn: Database connection\n        \n    Returns:\n        DataFrame with only mapped subcategories\n    \"\"\"\n    # Get all valid subcategories from the database\n    cursor = db_conn.cursor()\n    cursor.execute(\"SELECT name FROM subcategories\")\n    valid_subcategories = [row[0] for row in cursor.fetchall()]\n    \n    # Filter the DataFrame to only include rows with valid subcategories\n    df_filtered = df[df['subcategory'].isin(valid_subcategories)].copy()\n    \n    # Report how many rows were filtered out\n    filtered_count = len(df) - len(df_filtered)\n    print(f\"Filtered out {filtered_count} rows with unmapped subcategories\")\n    print(f\"Unmapped subcategories: {df[~df['subcategory'].isin(valid_subcategories)]['subcategory'].unique()}\")\n    \n    return df_filtered","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T06:20:08.899133Z","iopub.execute_input":"2025-04-18T06:20:08.899492Z","iopub.status.idle":"2025-04-18T06:20:08.912336Z","shell.execute_reply.started":"2025-04-18T06:20:08.899458Z","shell.execute_reply":"2025-04-18T06:20:08.911188Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def process_transaction_data_combined(df, db_conn, train_samples=50, test_samples=10, sample_csv_path=None):\n    \"\"\"\n    Process transaction data with combined category/subcategory labels\n    \"\"\"\n    # Make a copy to avoid modifying the original\n    df_copy = df.copy()\n    \n    # Step 1: Clean transaction notes\n    print(\"Cleaning transaction notes...\")\n    df_copy['cleaned_note'] = df_copy['note'].apply(clean_transaction_note)\n    \n    # Step 2: Add category column if needed\n    if 'category' not in df_copy.columns:\n        print(\"Adding category mapping...\")\n        df_copy = add_category_column(df_copy, db_conn)\n    \n    # Step 3: Create combined category/subcategory label\n    print(\"Creating combined labels...\")\n    df_copy['combined_label'] = df_copy['category'] + \"/\" + df_copy['subcategory']\n    \n    # Step 4: Filter out unmapped subcategories\n    print(\"Filtering out unmapped subcategories...\")\n    df_filtered = filter_unmapped_subcategories(df_copy, db_conn)\n    \n    # Step 5: Split into train and test data\n    train_df, test_df = train_test_split(df_filtered, test_size=0.2, random_state=42)\n    \n    # Step 6: Sample balanced datasets by subcategory first\n    print(f\"Creating balanced samples ({train_samples} per subcategory for training)...\")\n    train_sampled = sample_balanced_data(train_df, train_samples)\n    test_sampled = sample_balanced_data(test_df, test_samples)\n    \n    # Step 7: Create balanced datasets by combined label (optional)\n    print(\"Creating balanced samples by combined label...\")\n    try:\n        train_combined = sample_balanced_data_combined(train_df, max(5, train_samples // 3))\n        test_combined = sample_balanced_data_combined(test_df, max(2, test_samples // 3))\n        \n        # Merge the combined sampled data back into the main samples\n        # Only keep new rows that aren't already in the subcategory-balanced set\n        train_existing_indices = set(train_sampled.index)\n        test_existing_indices = set(test_sampled.index)\n        \n        new_train_rows = train_combined[~train_combined.index.isin(train_existing_indices)]\n        new_test_rows = test_combined[~test_combined.index.isin(test_existing_indices)]\n        \n        train_sampled = pd.concat([train_sampled, new_train_rows])\n        test_sampled = pd.concat([test_sampled, new_test_rows])\n        \n        print(f\"Added {len(new_train_rows)} additional rows from combined label sampling\")\n    except Exception as e:\n        print(f\"Skipping combined label sampling due to error: {e}\")\n    \n    # Step 8: Save sample for review if requested\n    if sample_csv_path:\n        # Take a small sample for review\n        review_sample = train_sampled.sample(min(len(train_sampled), 20))\n        # Include original, cleaned notes and combined label\n        review_sample = review_sample[['note', 'cleaned_note', 'category', 'subcategory', 'combined_label']]\n        review_sample.to_csv(sample_csv_path, index=False)\n        print(f\"Saved sample data to {sample_csv_path} for review\")\n    \n    # Print statistics\n    combined_labels_count = df_filtered['combined_label'].nunique()\n    print(f\"Original data: {len(df)} transactions\")\n    print(f\"Filtered data: {len(df_filtered)} transactions\")\n    print(f\"Unique combined labels: {combined_labels_count}\")\n    print(f\"Balanced training data: {len(train_sampled)} transactions\")\n    print(f\"Balanced test data: {len(test_sampled)} transactions\")\n    \n    return train_sampled, test_sampled, df_filtered","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T06:20:08.914045Z","iopub.execute_input":"2025-04-18T06:20:08.9144Z","iopub.status.idle":"2025-04-18T06:20:08.93346Z","shell.execute_reply.started":"2025-04-18T06:20:08.914364Z","shell.execute_reply":"2025-04-18T06:20:08.932387Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Sample the dataset\nNow sample the data. I will keep 50 rows for each subcategory for training.","metadata":{"id":"03lDs1O4ZQ0-"}},{"cell_type":"code","source":"# Connect to the database\nimport sqlite3\nimport pandas as pd\nfrom collections import Counter\n\ndb_conn = sqlite3.connect('/kaggle/working/transaction_categories.db')\n\n# Process the data\ndf_train_sampled, df_test_sampled, df_categorized = process_transaction_data_combined(\n    df, \n    db_conn,\n    train_samples=50, \n    test_samples=10,\n    sample_csv_path=\"/kaggle/working/transaction_sample_review.csv\"\n)\n\n# Print distribution of categories and subcategories\ndef print_distribution_stats(df, dataset_name=\"Dataset\"):\n    print(f\"\\n{'-'*50}\")\n    print(f\"{dataset_name} Distribution Statistics:\")\n    print(f\"{'-'*50}\")\n    \n    # Category distribution\n    category_counts = df['category'].value_counts()\n    print(f\"\\nCategories ({len(category_counts)} unique):\")\n    print(f\"{'Category':<25} {'Count':<10} {'Percentage':<10}\")\n    print(f\"{'-'*45}\")\n    \n    for category, count in category_counts.items():\n        percentage = count / len(df) * 100\n        print(f\"{category[:25]:<25} {count:<10} {percentage:.1f}%\")\n    \n    # Subcategory distribution\n    subcategory_counts = df['subcategory'].value_counts()\n    print(f\"\\nSubcategories ({len(subcategory_counts)} unique):\")\n    print(f\"{'Subcategory':<30} {'Category':<20} {'Count':<10} {'Percentage':<10}\")\n    print(f\"{'-'*70}\")\n    \n    # Create a mapping from subcategory to category for lookup\n    subcat_to_cat = df.groupby('subcategory')['category'].first().to_dict()\n    \n    for subcategory, count in subcategory_counts.items():\n        percentage = count / len(df) * 100\n        category = subcat_to_cat.get(subcategory, \"Unknown\")\n        print(f\"{subcategory[:30]:<30} {category[:20]:<20} {count:<10} {percentage:.1f}%\")\n    \n    # Find subcategories with low counts (potential data issues)\n    low_count_threshold = 5  # Adjust as needed\n    low_count_subcats = subcategory_counts[subcategory_counts < low_count_threshold]\n    if len(low_count_subcats) > 0:\n        print(f\"\\nSubcategories with low counts (<{low_count_threshold}):\")\n        for subcat, count in low_count_subcats.items():\n            print(f\"  - {subcat}: {count} transactions\")\n\n# Display sample of training data\nprint(\"\\nSample of training data:\")\nprint(df_train_sampled[['cleaned_note', 'category', 'subcategory']].head())\n\n# Print distribution statistics for both datasets\nprint_distribution_stats(df_train_sampled, \"Training Data\")\nprint_distribution_stats(df_test_sampled, \"Test Data\")\n\n# Additional summary statistics\nprint(f\"\\n{'-'*50}\")\nprint(f\"Summary Statistics:\")\nprint(f\"{'-'*50}\")\nprint(f\"Total transactions in original data: {len(df)}\")\nprint(f\"Total transactions in training data: {len(df_train_sampled)}\")\nprint(f\"Total transactions in test data: {len(df_test_sampled)}\")\nprint(f\"Training data categories: {df_train_sampled['category'].nunique()}\")\nprint(f\"Test data categories: {df_test_sampled['category'].nunique()}\")\nprint(f\"Training data subcategories: {df_train_sampled['subcategory'].nunique()}\")\nprint(f\"Test data subcategories: {df_test_sampled['subcategory'].nunique()}\")\n\n# Check for any subcategories in test but not in training\ntrain_subcats = set(df_train_sampled['subcategory'].unique())\ntest_subcats = set(df_test_sampled['subcategory'].unique())\ntest_only_subcats = test_subcats - train_subcats\n\nif test_only_subcats:\n    print(f\"\\nWarning: {len(test_only_subcats)} subcategories in test data but not in training data:\")\n    for subcat in test_only_subcats:\n        print(f\"  - {subcat}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T06:20:08.935064Z","iopub.execute_input":"2025-04-18T06:20:08.935516Z","iopub.status.idle":"2025-04-18T06:20:09.546687Z","shell.execute_reply.started":"2025-04-18T06:20:08.935471Z","shell.execute_reply":"2025-04-18T06:20:09.545432Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Instruct the few-shot prompt\nI draft the prompt asking it to only use the subcategory from the loaded table.","metadata":{}},{"cell_type":"code","source":" import sqlite3\n\ndb_file = \"transaction_categories.db\"\ndb_conn = sqlite3.connect(db_file)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T06:23:21.097986Z","iopub.execute_input":"2025-04-18T06:23:21.098531Z","iopub.status.idle":"2025-04-18T06:23:21.104724Z","shell.execute_reply.started":"2025-04-18T06:23:21.098476Z","shell.execute_reply":"2025-04-18T06:23:21.103381Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\nfrom google import genai\nfrom google.genai import types\n\ndef predict_category_and_subcategory_few_shot(transaction_note):\n    \"\"\"\n    Predicts the category and subcategory for a transaction using few-shot prompting.\n    Ensures that the subcategory belongs to the selected category based on database mappings.\n    \n    Args:\n        transaction_note: The transaction text to classify\n        \n    Returns:\n        tuple: (predicted_category, predicted_subcategory)\n    \"\"\"\n    try:\n        # Get the category-subcategory mapping\n        cursor = db_conn.cursor()\n        cursor.execute(\"\"\"\n        SELECT c.name as category, s.name as subcategory \n        FROM categories c\n        JOIN subcategories s ON c.category_id = s.category_id\n        ORDER BY c.name, s.name\n        \"\"\")\n        mappings = cursor.fetchall()\n        \n        # Create the hierarchy context\n        hierarchy_context = \"Category and subcategory hierarchy from the database:\\n\"\n        current_category = None\n        for category, subcategory in mappings:\n            if category != current_category:\n                hierarchy_context += f\"\\n{category}:\\n\"\n                current_category = category\n            hierarchy_context += f\"  - {subcategory}\\n\"\n        \n        # Create few-shot examples with clear category/subcategory relationships\n        examples = [\n            (\"AMAZON AUSYDNEY SOUTH Kindle Unlimited Subscription\", \"Life & Entertainment\", \"Books, audio, subscriptions\"),\n            (\"UBER EATS Melbourne AUS\", \"Food & Beverages\", \"Restaurant, fast-food\"),\n            (\"NETFLIX.COM SUBSCRIPTION\", \"Life & Entertainment\", \"TV, Streaming\"),\n            (\"COLES SUPERMARKET MELBOURNE\", \"Food & Beverages\", \"Groceries\"),\n            (\"TELSTRA MOBILE PAYMENT\", \"Communication, PC\", \"Telephony, mobile phone\")\n        ]\n            \n        # Build the few-shot examples part of the prompt\n        few_shot_examples = \"Here are some example transactions with their categories and subcategories:\\n\\n\"\n        for ex_note, ex_category, ex_subcategory in examples:\n            few_shot_examples += f\"Transaction: {ex_note}\\nCATEGORY: {ex_category}\\nSUBCATEGORY: {ex_subcategory}\\n\\n\"\n            \n        # System instruction with an emphasis on following the examples\n        system_instruct = \"\"\"\n        You are a financial transaction categorization assistant. You will analyze a transaction description and classify it into the appropriate category and subcategory.\n\n        Follow these steps exactly:\n        1. First, select the most appropriate CATEGORY from the available options\n        2. Then, select a SUBCATEGORY that belongs to the selected CATEGORY\n\n        Important: You must ensure the subcategory you select belongs to the category you chose. The database has specific parent-child relationships between categories and subcategories.\n\n        Study the provided examples carefully and follow the same pattern.\n\n        Your response must use this exact format:\n        CATEGORY: [selected category name]\n        SUBCATEGORY: [selected subcategory name]\n        \"\"\"\n        \n        # Make prediction with system instruction, hierarchy context, and few-shot examples\n        response = client.models.generate_content(\n            model=\"gemini-2.0-flash\",\n            config=types.GenerateContentConfig(\n                system_instruction=system_instruct,\n                temperature=0.2,  # Lower temperature for more consistent results\n            ),\n            contents=[\n                hierarchy_context,\n                few_shot_examples,\n                f\"Transaction description: {transaction_note}\\n\\nPlease categorize this transaction:\"\n            ]\n        )\n        \n        text = response.text.strip()\n        \n        # Extract category and subcategory\n        try:\n            category_line = [line for line in text.split('\\n') if line.startswith(\"CATEGORY:\")][0]\n            subcategory_line = [line for line in text.split('\\n') if line.startswith(\"SUBCATEGORY:\")][0]\n            \n            category = category_line.replace(\"CATEGORY:\", \"\").strip()\n            subcategory = subcategory_line.replace(\"SUBCATEGORY:\", \"\").strip()\n            \n            # Verify that the subcategory belongs to the category using the database\n            cursor.execute(\"\"\"\n            SELECT COUNT(*) FROM subcategories s\n            JOIN categories c ON s.category_id = c.category_id\n            WHERE c.name = ? AND s.name = ?\n            \"\"\", (category, subcategory))\n            \n            count = cursor.fetchone()[0]\n            if count == 0:\n                # If the model returned an invalid mapping, find a valid subcategory for the category\n                cursor.execute(\"\"\"\n                SELECT s.name FROM subcategories s\n                JOIN categories c ON s.category_id = c.category_id\n                WHERE c.name = ?\n                LIMIT 1\n                \"\"\", (category,))\n                \n                result = cursor.fetchone()\n                if result:\n                    # Use a valid subcategory for this category\n                    subcategory = result[0]\n                    return category, subcategory\n                else:\n                    # If category is invalid too, return error\n                    return \"(invalid category)\", \"(invalid subcategory)\"\n            \n            return category, subcategory\n            \n        except (IndexError, KeyError) as e:\n            return \"(parsing error)\", \"(parsing error)\"\n            \n    except Exception as e:\n        return f\"(error)\", f\"(error: {str(e)})\"\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T06:23:25.143875Z","iopub.execute_input":"2025-04-18T06:23:25.144273Z","iopub.status.idle":"2025-04-18T06:23:25.158189Z","shell.execute_reply.started":"2025-04-18T06:23:25.144209Z","shell.execute_reply":"2025-04-18T06:23:25.15695Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Evaluate baseline performance\n\nNow I perform an evaluation on the available models to ensure I can measure how much the tuning helps.","metadata":{}},{"cell_type":"code","source":"import tqdm\nfrom tqdm.rich import tqdm as tqdmr\nimport warnings\n\n# Enable tqdm features on Pandas\ntqdmr.pandas()\n\n# Suppress the experimental warning\nwarnings.filterwarnings(\"ignore\", category=tqdm.TqdmExperimentalWarning)\n\n# Further sample the test data to be mindful of the free-tier quota\nTEST_SAMPLE_SIZE = 20\ndf_baseline_eval = df_test_sampled.sample(min(TEST_SAMPLE_SIZE, len(df_test_sampled)))\n\n# Ensure category column exists in test data\nif 'category' not in df_baseline_eval.columns:\n    # Add categories using the database mapping\n    cursor = db_conn.cursor()\n    cursor.execute(\"\"\"\n    SELECT s.name as subcategory, c.name as category \n    FROM subcategories s\n    JOIN categories c ON s.category_id = c.category_id\n    \"\"\")\n    subcat_to_cat = {row[0]: row[1] for row in cursor.fetchall()}\n    df_baseline_eval['category'] = df_baseline_eval['subcategory'].map(subcat_to_cat)\n\nprint(f\"Evaluating {len(df_baseline_eval)} transactions...\")\n\n# Make predictions using the sampled data with progress bar\n# This will return both category and subcategory\ndf_baseline_eval[['predicted_category', 'predicted_subcategory']] = df_baseline_eval['note'].progress_apply(\n    lambda x: pd.Series(predict_category_and_subcategory_few_shot(x))\n)\n\n# Calculate the accuracy for both category and subcategory\ncategory_accuracy = (df_baseline_eval['category'] == df_baseline_eval['predicted_category']).mean()\nsubcategory_accuracy = (df_baseline_eval['subcategory'] == df_baseline_eval['predicted_subcategory']).mean()\ncombined_accuracy = ((df_baseline_eval['category'] == df_baseline_eval['predicted_category']) & \n                     (df_baseline_eval['subcategory'] == df_baseline_eval['predicted_subcategory'])).mean()\n\nprint(f\"Category accuracy: {category_accuracy:.2%}\")\nprint(f\"Subcategory accuracy: {subcategory_accuracy:.2%}\")\nprint(f\"Combined accuracy (both correct): {combined_accuracy:.2%}\")\n\n# Display some examples of predictions\nprint(\"\\nSample predictions:\")\nsample_results = df_baseline_eval[['note', 'category', 'subcategory', \n                                  'predicted_category', 'predicted_subcategory']].sample(min(5, len(df_baseline_eval)))\n\nfor idx, row in sample_results.iterrows():\n    print(f\"Transaction: {row['note'][:50]}...\")\n    print(f\"True category: {row['category']}\")\n    print(f\"Predicted category: {row['predicted_category']}\")\n    print(f\"Category correct: {row['category'] == row['predicted_category']}\")\n    print(f\"True subcategory: {row['subcategory']}\")\n    print(f\"Predicted subcategory: {row['predicted_subcategory']}\")\n    print(f\"Subcategory correct: {row['subcategory'] == row['predicted_subcategory']}\\n\")\n\n# Create a confusion matrix for categories\nprint(\"Category confusion matrix:\")\ncat_matrix = pd.crosstab(\n    df_baseline_eval['category'], \n    df_baseline_eval['predicted_category'],\n    rownames=['True'], \n    colnames=['Predicted']\n)\nprint(cat_matrix)\n\n# Create a confusion matrix for subcategories with errors\nprint(\"\\nMost common subcategory error patterns:\")\nerror_patterns = df_baseline_eval[df_baseline_eval['subcategory'] != df_baseline_eval['predicted_subcategory']]\nif len(error_patterns) > 0:\n    error_counts = error_patterns.groupby(['subcategory', 'predicted_subcategory']).size().reset_index(name='count')\n    error_counts = error_counts.sort_values('count', ascending=False)\n    print(error_counts.head(5))\nelse:\n    print(\"No subcategory errors found in the evaluation set!\")\n\n# Analysis of hierarchical errors\nprint(\"\\nError analysis by hierarchy:\")\nhierarchical_errors = df_baseline_eval[\n    (df_baseline_eval['category'] == df_baseline_eval['predicted_category']) & \n    (df_baseline_eval['subcategory'] != df_baseline_eval['predicted_subcategory'])\n]\nprint(f\"Correct category but wrong subcategory: {len(hierarchical_errors)} cases ({len(hierarchical_errors)/len(df_baseline_eval):.2%})\")\n\ncategory_errors = df_baseline_eval[df_baseline_eval['category'] != df_baseline_eval['predicted_category']]\nprint(f\"Wrong category: {len(category_errors)} cases ({len(category_errors)/len(df_baseline_eval):.2%})\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T06:25:37.018051Z","iopub.execute_input":"2025-04-18T06:25:37.018482Z","iopub.status.idle":"2025-04-18T06:25:51.207836Z","shell.execute_reply.started":"2025-04-18T06:25:37.018448Z","shell.execute_reply":"2025-04-18T06:25:51.206777Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Train my model\nHere I train one model to learn how to set the category and train 11 models, one for each subcategory.","metadata":{}},{"cell_type":"code","source":"import os\nimport json\nimport datetime\nimport time\nfrom google.api_core import retry\nfrom collections.abc import Iterable\nimport pandas as pd\nfrom tqdm.rich import tqdm as tqdmr\nimport warnings\nimport tqdm\n\n# Path for storing model IDs\nMODEL_IDS_FILE = \"/kaggle/input/training/combined_model_id.json\"\n\n# Suppress warnings\nwarnings.filterwarnings(\"ignore\", category=tqdm.TqdmExperimentalWarning)\n\ndef prepare_combined_training_data(df):\n    \"\"\"\n    Prepare training data for fine-tuning with combined labels.\n    \n    Args:\n        df: DataFrame with transaction data\n        \n    Returns:\n        Dictionary with examples in the format required by the API\n    \"\"\"\n    # Prepare examples\n    training_examples = []\n    for _, row in df.iterrows():\n        training_examples.append({\n            \"textInput\": str(row['note']),\n            \"output\": str(row['combined_label'])\n        })\n    \n    print(f\"Created {len(training_examples)} training examples with combined labels\")\n    \n    return {\"examples\": training_examples}\n\ndef get_model_ids():\n    \"\"\"Load model IDs from file or initialize empty structure\"\"\"\n    if os.path.exists(MODEL_IDS_FILE):\n        with open(MODEL_IDS_FILE, \"r\") as f:\n            return json.load(f)\n    return {\n        \"category_model\": None,\n        \"subcategory_models\": {}\n    }\n\ndef save_model_ids(model_ids):\n    \"\"\"Save model IDs to file\"\"\"\n    with open(MODEL_IDS_FILE, \"w\") as f:\n        json.dump(model_ids, f, indent=2)\n\ndef train_combined_model(df_train):\n    \"\"\"\n    Train a single model that predicts the combined \"Category/Subcategory\" label.\n    \n    Args:\n        df_train: DataFrame containing training data with 'combined_label' column\n        \n    Returns:\n        The model ID\n    \"\"\"\n    print(\"Starting combined model training process...\")\n    \n    # Ensure we have the combined label\n    if 'combined_label' not in df_train.columns:\n        if 'category' in df_train.columns and 'subcategory' in df_train.columns:\n            df_train['combined_label'] = df_train['category'] + \"/\" + df_train['subcategory']\n            print(\"Created combined labels from category and subcategory\")\n        else:\n            raise ValueError(\"Training data must include 'category' and 'subcategory' columns\")\n    \n    # Check if we already have a combined model\n    combined_model_id = None\n    model_file = \"/kaggle/input/training/combined_model_id.json\"\n    \n    try:\n        if os.path.exists(model_file):\n            with open(model_file, \"r\") as f:\n                saved_data = json.load(f)\n                combined_model_id = saved_data.get(\"combined_model\")\n                \n            if combined_model_id:\n                print(f\"Using existing combined model: {combined_model_id}\")\n                return combined_model_id\n    except Exception as e:\n        print(f\"Error loading existing model ID: {e}\")\n    \n    # Prepare training data\n    training_data = prepare_combined_training_data(df_train)\n    \n    # Don't create model if we don't have enough examples\n    if len(training_data[\"examples\"]) < 5:\n        print(\"Skipping tuning due to insufficient data\")\n        return None\n    \n    # Start tuning\n    print(\"Starting new combined classification fine-tuning job\")\n    \n    # Create display name (must be under 40 chars)\n    display_name = f\"txn-combined-{datetime.datetime.now().strftime('%m%d')}\"\n    \n    try:\n        tuning_op = client.tunings.tune(\n            base_model=\"models/gemini-1.5-flash-001-tuning\",\n            training_dataset=training_data,\n            config=types.CreateTuningJobConfig(\n                tuned_model_display_name=display_name,\n                batch_size=8,\n                epoch_count=5,\n            ),\n        )\n        \n        combined_model_id = tuning_op.name\n        print(f\"Fine-tuning initiated. Model ID: {combined_model_id}\")\n        print(f\"Current status: {tuning_op.state}\")\n        \n        # Save model ID\n        with open(model_file, \"w\") as f:\n            json.dump({\"combined_model\": combined_model_id}, f, indent=2)\n            \n    except Exception as e:\n        print(f\"Error starting tuning job: {e}\")\n        return None\n    \n    return combined_model_id\n\n# Get model status information\ndef get_model_status(model_id):\n    if not model_id:\n        return {\"state\": \"NOT_FOUND\"}\n    \n    try:\n        model = client.tunings.get(name=model_id)\n        return {\n            \"state\": model.state.name,\n            \"success\": model.has_succeeded,\n            \"ended\": model.has_ended,\n            \"error\": model.error if hasattr(model, \"error\") else None,\n            \"create_time\": model.create_time,\n            \"progress\": model.progress if hasattr(model, \"progress\") else None,\n            \"model_id\": model_id\n        }\n    except Exception as e:\n        return {\"state\": \"ERROR\", \"error\": str(e), \"model_id\": model_id}\n        return {\"state\": \"ERROR\", \"error\": str(e)}\n\n\n\ndef print_model_status(status_dict):\n    \"\"\"Pretty print model status information\"\"\"\n    state = status_dict.get(\"state\", \"UNKNOWN\")\n    create_time = status_dict.get(\"create_time\")\n    progress = status_dict.get(\"progress\")\n    model_id = status_dict.get(\"model_id\", \"Unknown\")\n    \n    print(f\"\\n=== COMBINED MODEL STATUS ===\")\n    print(f\"Model ID: {model_id}\")\n    \n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T06:37:46.806307Z","iopub.execute_input":"2025-04-18T06:37:46.806721Z","iopub.status.idle":"2025-04-18T06:37:46.825167Z","shell.execute_reply.started":"2025-04-18T06:37:46.806671Z","shell.execute_reply":"2025-04-18T06:37:46.824011Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_ids = train_combined_model(df_train_sampled)\nprint(f\"Models a check_model_statuses()re now being trained. IDs saved to {MODEL_IDS_FILE}\")","metadata":{"execution":{"iopub.status.busy":"2025-04-18T06:37:51.458107Z","iopub.execute_input":"2025-04-18T06:37:51.458522Z","iopub.status.idle":"2025-04-18T06:37:51.481159Z","shell.execute_reply.started":"2025-04-18T06:37:51.458485Z","shell.execute_reply":"2025-04-18T06:37:51.480173Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Monitoring progress\nHere I monitor whether this model has been tuned and ready to use.","metadata":{}},{"cell_type":"code","source":"get_model_status(\"tunedModels/txncombined0418-5yec1xoakuo4\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T06:37:54.740923Z","iopub.execute_input":"2025-04-18T06:37:54.741349Z","iopub.status.idle":"2025-04-18T06:37:56.041533Z","shell.execute_reply.started":"2025-04-18T06:37:54.741316Z","shell.execute_reply":"2025-04-18T06:37:56.040538Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Evaluate Tuned Model\nHere I test and evaluate the performance of tuned models.","metadata":{}},{"cell_type":"code","source":"def evaluate_combined_model(df_test, model_id=None, sample_size=20):\n    \"\"\"\n    Evaluate the combined category/subcategory model\n    \n    Args:\n        df_test: Test DataFrame\n        model_id: Optional model ID to use\n        sample_size: Number of samples to evaluate\n        \n    Returns:\n        DataFrame with evaluation results\n    \"\"\"\n    # Get the model ID if not provided\n    if not model_id:\n        try:\n            with open(\"/kaggle/working/combined_model_id.json\", \"r\") as f:\n                model_id = json.load(f).get(\"combined_model\")\n                \n            if not model_id:\n                print(\"No combined model ID found. Please train a model first.\")\n                return None\n        except:\n            print(\"No combined model ID file found. Please train a model first.\")\n            return None\n    \n    # Sample the test data\n    if len(df_test) > sample_size:\n        eval_df = df_test.sample(sample_size)\n    else:\n        eval_df = df_test.copy()\n    \n    # Ensure we have the combined label\n    if 'combined_label' not in eval_df.columns:\n        eval_df['combined_label'] = eval_df['category'] + \"/\" + eval_df['subcategory']\n    \n    # Get valid combinations from the database\n    cursor = db_conn.cursor()\n    cursor.execute(\"\"\"\n    SELECT c.name || '/' || s.name as combined\n    FROM categories c\n    JOIN subcategories s ON c.category_id = s.category_id\n    \"\"\")\n    valid_combinations = [row[0] for row in cursor.fetchall()]\n    \n    # Make predictions\n    results = []\n    print(f\"Evaluating combined model on {len(eval_df)} test transactions...\")\n    \n    for idx, row in tqdmr(eval_df.iterrows(), total=len(eval_df)):\n        transaction = row['note']\n        true_combined = row['combined_label']\n        true_category, true_subcategory = true_combined.split('/', 1)\n        \n        try:\n            # Get prediction from model\n            response = client.models.generate_content(\n                model=model_id,\n                contents=transaction,\n                config=types.GenerateContentConfig(\n                    temperature=0.0,\n                    max_output_tokens=20\n                )\n            )\n            \n            if response.candidates and response.candidates[0].content:\n                pred_combined = response.candidates[0].content.parts[0].text.strip()\n                \n                # Validate prediction against database\n                if pred_combined not in valid_combinations:\n                    # Try to find closest match\n                    matches = get_close_matches(pred_combined, valid_combinations, n=1, cutoff=0.6)\n                    if matches:\n                        print(f\"Corrected prediction: {pred_combined} → {matches[0]}\")\n                        pred_combined = matches[0]\n                    else:\n                        print(f\"Invalid prediction: {pred_combined} (no close match found)\")\n                        pred_combined = \"(invalid prediction)\"\n                \n                # Split prediction into category and subcategory\n                if \"/\" in pred_combined:\n                    pred_category, pred_subcategory = pred_combined.split('/', 1)\n                else:\n                    pred_category, pred_subcategory = pred_combined, \"(parsing error)\"\n            else:\n                pred_combined = \"(no response)\"\n                pred_category, pred_subcategory = \"(no response)\", \"(no response)\"\n                \n        except Exception as e:\n            error_message = str(e)\n            pred_combined = f\"(error: {error_message[:30]}...)\"\n            pred_category, pred_subcategory = \"(error)\", \"(error)\"\n        \n        # Store results\n        results.append({\n            'transaction': transaction,\n            'true_combined': true_combined,\n            'pred_combined': pred_combined,\n            'combined_correct': true_combined == pred_combined,\n            'true_category': true_category,\n            'pred_category': pred_category,\n            'category_correct': true_category == pred_category,\n            'true_subcategory': true_subcategory, \n            'pred_subcategory': pred_subcategory,\n            'subcategory_correct': true_subcategory == pred_subcategory\n        })\n    \n    # Convert to DataFrame\n    results_df = pd.DataFrame(results)\n    \n    # Calculate metrics\n    combined_accuracy = results_df['combined_correct'].mean()\n    category_accuracy = results_df['category_correct'].mean()\n    subcategory_accuracy = results_df['subcategory_correct'].mean()\n    \n    print(f\"Combined label accuracy: {combined_accuracy:.2%}\")\n    print(f\"Category accuracy: {category_accuracy:.2%}\")\n    print(f\"Subcategory accuracy: {subcategory_accuracy:.2%}\")\n    \n    # Show example predictions\n    print(\"\\nSample predictions:\")\n    for idx, row in results_df.sample(min(5, len(results_df))).iterrows():\n        print(f\"Transaction: {row['transaction'][:50]}...\")\n        print(f\"True: {row['true_combined']}\")\n        print(f\"Predicted: {row['pred_combined']}\")\n        print(f\"Correct: {row['combined_correct']}\\n\")\n    \n    # Display confusion matrix for categories (if enough data)\n    if len(results_df) >= 5:\n        print(\"\\nCategory confusion matrix:\")\n        try:\n            cat_matrix = pd.crosstab(\n                results_df['true_category'], \n                results_df['pred_category'],\n                rownames=['True'], \n                colnames=['Predicted']\n            )\n            print(cat_matrix)\n        except Exception as e:\n            print(f\"Unable to generate confusion matrix: {e}\")\n    \n    return results_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T06:41:28.157369Z","iopub.execute_input":"2025-04-18T06:41:28.1578Z","iopub.status.idle":"2025-04-18T06:41:28.190017Z","shell.execute_reply.started":"2025-04-18T06:41:28.157765Z","shell.execute_reply":"2025-04-18T06:41:28.188794Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def run_combined_model_workflow():\n    \"\"\"Run the complete workflow for the combined model approach\"\"\"\n    # Step 1: Process the data with combined labels\n    print(\"Processing transaction data...\")\n    df_train_sampled, df_test_sampled, df_categorized = process_transaction_data_combined(\n        df, \n        db_conn,\n        train_samples=50, \n        test_samples=10,\n        sample_csv_path=\"/kaggle/working/transaction_sample_review.csv\"\n    )\n    \n    # Step 2: Train the combined model\n    print(\"\\nTraining combined model...\")\n    combined_model_id = train_combined_model(df_train_sampled)\n    \n    # Step 3: Check model status\n    if combined_model_id:\n        print(f\"\\nCombined model ID: {combined_model_id}\")\n        try:\n            model = client.tunings.get(name=combined_model_id)\n            print(f\"Model state: {model.state.name}\")\n            \n            if model.has_succeeded:\n                # Step 4: Evaluate the model if it's ready\n                print(\"\\nEvaluating combined model...\")\n                results = evaluate_combined_model(df_test_sampled, combined_model_id, sample_size=20)\n                return results\n            else:\n                print(\"Model is not ready for evaluation yet. Check back later.\")\n                return None\n        except Exception as e:\n            print(f\"Error checking model status: {e}\")\n            return None\n    else:\n        print(\"No model was created. Please check the errors above.\")\n        return None\n\n# Run the workflow\nresults = run_combined_model_workflow()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T06:46:06.426293Z","iopub.execute_input":"2025-04-18T06:46:06.42669Z","iopub.status.idle":"2025-04-18T06:46:27.845953Z","shell.execute_reply.started":"2025-04-18T06:46:06.426657Z","shell.execute_reply":"2025-04-18T06:46:27.844774Z"}},"outputs":[],"execution_count":null}]}