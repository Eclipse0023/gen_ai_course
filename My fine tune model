{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"colab":{"name":"day-4-fine-tuning-a-custom-model.ipynb","toc_visible":true},"kaggle":{"accelerator":"none","dataSources":[{"datasetId":7099621,"sourceId":11349158,"sourceType":"datasetVersion"}],"dockerImageVersionId":30786,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Categorise finance transactions\n\nIn life, my financial transactions are often categorised incorrectly in my budgeting app. I decided to find a better solution.\n\nIn this example, I will first try to categorise with an existing Gemini model using a zero-shot prompt and evaluate its performance. Then I will tune a model with the data categorised by me and evaluate its performance.","metadata":{"id":"4KDIFPAL2EnL"}},{"cell_type":"code","source":"# Install required libraries\n!pip uninstall -qqy jupyterlab  # Remove unused conflicting packages\n!pip install -U -q \"google-genai==1.7.0\"","metadata":{"execution":{"iopub.execute_input":"2025-04-13T01:11:01.779506Z","iopub.status.busy":"2025-04-13T01:11:01.778477Z"},"id":"9wafTyEH1_xF","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Import necessary libraries\nfrom google import genai\nfrom google.genai import types\n\ngenai.__version__","metadata":{"id":"T0CBG9xL2PvT","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set up the Google GenAI client\nfrom kaggle_secrets import UserSecretsClient\n\nGOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n\nclient = genai.Client(api_key=GOOGLE_API_KEY)","metadata":{"id":"VuJPY3GK2SLZ","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Explore available models","metadata":{"id":"CqVA5QFO6n4z"}},{"cell_type":"code","source":"# List available models\nfor model in client.models.list():\n    print(model.name)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Use the dataset\n\nI have uploaded transaction data categorised by me. Then I group it into training data and test data.","metadata":{"id":"peFm0w_0c1CO"}},{"cell_type":"code","source":"# Load and preprocess transaction data\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Load your data\nfile_path = \"/kaggle/input/training/categorized_transaction.csv\"\ndf = pd.read_csv(file_path)\n\n# Split into train and test sets (80/20 split)\ndf_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n\n# Display the subcategories (labels) in your dataset\nsubcategories = df['subcategory'].unique()\nprint(f\"Number of subcategories: {len(subcategories)}\")\nprint(\"Sample subcategories:\", subcategories[:10])  # Show first 10 subcategories\n\n# Quick look at note examples\nprint(\"\\nSample notes:\")\nfor i, note in enumerate(df['note'].head(3)):\n    print(f\"{i+1}. {note} â†’ {df['subcategory'].iloc[i]}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Clean the data","metadata":{}},{"cell_type":"code","source":"import re\nimport pandas as pd\n\ndef preprocess_transaction_note(note):\n    \"\"\"\n    Clean and standardize a transaction note, removing dollar amounts and other noise.\n    \"\"\"\n    # Handle None or empty strings\n    if not note or pd.isna(note):\n        return \"\"\n    \n    # Convert to string if needed\n    text = str(note)\n    \n    # Extract meaningful parts (keeping merchant name and location)\n    # Split on common separators in transaction data\n    parts = re.split(r'\\s+(?:CREDIT CARD PURCHASE|EFTPOS|Value Date|tap and Pay|Card Purchase)', text, flags=re.IGNORECASE)\n    main_text = parts[0] if parts else text\n    \n    # Clean dollar amounts with various currency symbols\n    main_text = re.sub(r'(?:(?:\\$|AUD|USD|EUR|GBP)\\s*)?(?:-)?(?:\\d{1,3}(?:,\\d{3})*|\\d+)(?:\\.\\d{1,2})?', '', main_text)\n    \n    # Remove card numbers (masked or full)\n    main_text = re.sub(r'(?:x{2,4}|X{2,4})\\d{4}', '', main_text)\n    main_text = re.sub(r'\\b\\d{4}[\\s-]?\\d{4}[\\s-]?\\d{4}[\\s-]?\\d{4}\\b', '', main_text)\n    \n    # Remove dates in various formats\n    date_patterns = [\n        r'\\d{1,2}\\s+(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\\s+\\d{2,4}',  # 01 Jan 2023\n        r'(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\\s+\\d{1,2}\\s+\\d{2,4}',  # Jan 01 2023\n        r'\\d{1,2}/\\d{1,2}/\\d{2,4}',  # 01/01/2023\n        r'\\d{1,2}-\\d{1,2}-\\d{2,4}',  # 01-01-2023\n        r'\\d{1,2}\\s+[A-Za-z]{3}\\s+\\d{2}',  # 02 DEC 20\n    ]\n    for pattern in date_patterns:\n        main_text = re.sub(pattern, '', main_text, flags=re.IGNORECASE)\n    \n    # Remove reference numbers and transaction IDs\n    main_text = re.sub(r'Ref(?:erence)?:?\\s*[A-Za-z0-9]+', '', main_text, flags=re.IGNORECASE)\n    main_text = re.sub(r'#\\d+', '', main_text)\n    main_text = re.sub(r'\\[Eff\\s+Date:.*?\\]', '', main_text)\n    \n    # Remove email addresses (like in the newsgroup example)\n    main_text = re.sub(r'[\\w\\.-]+@[\\w\\.-]+', '', main_text)\n    \n    # Remove common transaction suffixes\n    suffixes = [\n        r'Electronic Transfer',\n        r'Direct Debit',\n        r'PURCHASE',\n        r'Purchase',\n        r'PAYMENT',\n        r'Payment',\n        r'MONTHLY',\n        r'RENEWAL',\n        r'Renewal',\n        r'SUBSCRIPTION',\n        r'Subscription'\n    ]\n    for suffix in suffixes:\n        main_text = re.sub(suffix, '', main_text, flags=re.IGNORECASE)\n    \n    # Clean up extra whitespace and standardize\n    main_text = re.sub(r'\\s+', ' ', main_text).strip()\n    \n    # Truncate long text (usually not an issue with transaction data but included for safety)\n    main_text = main_text[:500]\n    \n    return main_text","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def process_transactions_for_fine_tuning(df):\n    # Step 1: Clean transaction notes first\n    print(\"Cleaning transaction notes...\")\n    df['cleaned_note'] = df['note'].apply(preprocess_transaction_note)\n    \n    # Step 2: Then perform category/subcategory transformations\n    print(\"Transforming categories to codes...\")\n    df = transform_categories(df)\n    \n    return df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Sample the dataset\nNow sample the data. I will keep 50 rows for each subcategory for training.","metadata":{"id":"03lDs1O4ZQ0-"}},{"cell_type":"code","source":"def sample_data(df, num_samples):\n    \"\"\"\n    Sample rows from each subcategory, selecting num_samples from each.\n    If a subcategory has fewer than num_samples entries, takes all available rows.\n    \n    Args:\n        df: DataFrame containing transaction data\n        num_samples: Number of samples to take per subcategory\n        \n    Returns:\n        DataFrame with balanced samples across subcategories\n    \"\"\"\n    # Group by subcategory and sample\n    sampled_df = (\n        df.groupby(\"subcategory\")[df.columns]\n        .apply(lambda x: x.sample(min(len(x), num_samples)))\n        .reset_index(drop=True)\n    )\n    \n    # Convert subcategory to category type for efficiency\n    sampled_df[\"subcategory\"] = sampled_df[\"subcategory\"].astype(\"category\")\n    \n    return sampled_df\n\n# Sample training and test data\nTRAIN_NUM_SAMPLES = 50  # 50 samples per subcategory for training\nTEST_NUM_SAMPLES = 10   # 10 samples per subcategory for testing\n\n# Create balanced datasets\ndf_train_sampled = sample_data(df_train, TRAIN_NUM_SAMPLES)\ndf_test_sampled = sample_data(df_test, TEST_NUM_SAMPLES)\n\n# Print statistics about the sampled data\nprint(f\"Original training data: {len(df_train)} rows\")\nprint(f\"Sampled training data: {len(df_train_sampled)} rows\")\nprint(f\"Number of subcategories: {df_train_sampled['subcategory'].nunique()}\")\n\n# Show distribution of a few subcategories\nprint(\"\\nSample of subcategory counts in training data:\")\nprint(df_train_sampled['subcategory'].value_counts().head(5))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Load the subcategory and category table\nIn this step, I load the subcategory and category table.","metadata":{}},{"cell_type":"code","source":"import sqlite3\n\n# Connect to your database\ndb_conn = sqlite3.connect('/kaggle/working/transaction_categories.db')\ncursor = db_conn.cursor()\n\n# Create the tables\ncursor.execute('''\nCREATE TABLE IF NOT EXISTS categories (\n    category_id INTEGER PRIMARY KEY AUTOINCREMENT,\n    name VARCHAR(100) NOT NULL UNIQUE,\n    description TEXT,\n    display_order INT DEFAULT 100,\n    is_active BOOLEAN DEFAULT 1,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n)\n''')\n\ncursor.execute('''\nCREATE TABLE IF NOT EXISTS subcategories (\n    subcategory_id INTEGER PRIMARY KEY AUTOINCREMENT,\n    category_id INTEGER NOT NULL,\n    name VARCHAR(100) NOT NULL,\n    description TEXT,\n    display_order INT DEFAULT 100,\n    is_active BOOLEAN DEFAULT 1,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    FOREIGN KEY (category_id) REFERENCES categories(category_id)\n)\n''')\n\n\n# Insert main categories\ncategories = [\n    ('Food & Beverages', 'Expenses related to food and drinks', 10),\n    ('Shopping', 'Retail purchases and shopping expenses', 20),\n    ('Housing', 'Home-related expenses including rent and utilities', 30),\n    ('Transportation', 'Public and private transportation costs', 40),\n    ('Vehicle', 'Car and vehicle related expenses', 50),\n    ('Life & Entertainment', 'Leisure activities and entertainment', 60),\n    ('Communication, PC', 'Internet, phone and computer expenses', 70),\n    ('Financial expenses', 'Banking fees, loans, and financial costs', 80),\n    ('Investments', 'Investment-related transactions', 90),\n    ('Income', 'All sources of incoming money', 100),\n    ('Transfer', 'Money transfers between accounts', 110)\n]\n\ncursor.executemany('INSERT OR IGNORE INTO categories (name, description, display_order) VALUES (?, ?, ?)', categories)\n\n# Insert subcategories for Food & Beverages\nfood_subcategories = [\n    (1, 'Bar, cafe, drink, snacks', 10),\n    (1, 'Groceries', 20),\n    (1, 'Restaurant, fast-food', 30)\n]\ncursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', food_subcategories)\n\n# Insert subcategories for Shopping\nshopping_subcategories = [\n    (2, 'Clothes & Footwear', 10),\n    (2, 'Drug-store, chemist', 20),\n    (2, 'Electronics, accessories', 30),\n    (2, 'Gifts, joy', 40),\n    (2, 'Health and beauty', 50),\n    (2, 'Home, garden', 60),\n    (2, 'Jewels, accessories', 70),\n    (2, 'Kids', 80),\n    (2, 'Leisure time', 90),\n    (2, 'Pets, animals', 100),\n    (2, 'Stationery, tools', 110)\n]\ncursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', shopping_subcategories)\n\nhousing_subcategories=[\n    (3, 'Energy, utilities', 10),\n    (3, 'Maintenance, repairs', 20),\n    (3, 'Mortgage', 30),\n    (3, 'Property insurance', 40),\n    (3, 'Rent', 50),\n    (3, 'Services', 60)\n]\ncursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', housing_subcategories)\n\n\ntransportation_subcategories=[\n    (4, 'Business trips', 10),\n    (4, 'Long distance', 20),\n    (4, 'Public transport', 30),\n    (4, 'Taxi', 40)\n]\ncursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', transportation_subcategories)\n\nvehicle_subcategories=[\n    (5, 'Fuel', 10),\n    (5, 'Leasing', 20),\n    (5, 'Parking', 30),\n    (5, 'Rentals', 40),\n    (5, 'Vehicle insurance', 50),\n    (5, 'Vehicle maintenance', 60)\n]\ncursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', vehicle_subcategories)\n\nlife_subcategories=[\n    (6, 'Active sport, fitness', 10),\n    (6, 'Alcohol, tobacco', 20),\n    (6, 'Books, audio, subscriptions', 30),\n    (6, 'Charity, gifts', 40),\n    (6, 'Culture, sport events', 50),\n    (6, 'Education, development', 60),\n    (6, 'Health care, doctor', 70),\n    (6, 'Hobbies', 80),\n    (6, 'Holiday, trips, hotels', 90),\n    (6, 'Life events', 100),\n    (6, 'Lottery, gambling', 110),\n    (6, 'TV, Streaming', 120),\n    (6, 'Wellness, beauty', 130)\n]\ncursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', life_subcategories)\n\ncommunication_subcategories=[\n    (7, 'Internet', 10),\n    (7, 'Postal services', 20),\n    (7, 'Software, apps, games', 30),\n    (7, 'Telephony, mobile phone', 40)\n]\ncursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', communication_subcategories)\n\nfinancial_subcategories=[\n    (8, 'Advisory', 10),\n    (8, 'Charges, Fees', 20),\n    (8, 'Child Support', 30),\n    (8, 'Fines', 40),\n    (8, 'Insurances', 50),\n    (8, 'Loans, interests', 60),\n    (8, 'Taxes', 70)\n]\ncursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', financial_subcategories)\n\ninvestments_subcategories=[\n    (9, 'Collections', 10),\n    (9, 'Financial investments', 20),\n    (9, 'Realty', 30),\n    (9, 'Savings', 40),\n    (9, 'Vehicles, chattels', 50)\n]\ncursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', investments_subcategories)\n\nincome_subcategories=[\n    (10, 'Checks, coupons', 10),\n    (10, 'Child Support', 20),\n    (10, 'Dues & grants', 30),\n    (10, 'Gifts', 40),\n    (10, 'Interests, dividends', 50),\n    (10, 'Lending, renting', 60),\n    (10, 'Lottery, gambling', 70),\n    (10, 'Refunds (tax, purchase)', 80),\n    (10, 'Rental income', 90),\n    (10, 'Sale', 100),\n    (10, 'Wage, invoices', 110)\n]\ncursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', income_subcategories)\n\ntransfer_subcatgories=[\n    (11, 'Transfer', 10),   \n]\ncursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', transfer_subcatgories)\n\n# Commit the changes\ndb_conn.commit()\nprint(\"Database schema created successfully!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def list_tables() -> list[str]:\n    \"\"\"Retrieve the names of all tables in the database.\"\"\"\n    # Include print logging statements so you can see when functions are being called.\n    print(' - DB CALL: list_tables()')\n\n    cursor = db_conn.cursor()\n\n    # Fetch the table names.\n    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n\n    tables = cursor.fetchall()\n    return [t[0] for t in tables]\n\n\nlist_tables()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def describe_table(table_name: str) -> list[tuple[str, str]]:\n    \"\"\"Look up the table schema.\n\n    Returns:\n      List of columns, where each entry is a tuple of (column, type).\n    \"\"\"\n    print(f' - DB CALL: describe_table({table_name})')\n\n    cursor = db_conn.cursor()\n\n    cursor.execute(f\"PRAGMA table_info({table_name});\")\n\n    schema = cursor.fetchall()\n    # [column index, column name, column type, ...]\n    return [(col[1], col[2]) for col in schema]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def execute_query(sql: str) -> list[list[str]]:\n    \"\"\"Execute an SQL statement, returning the results.\"\"\"\n    print(f' - DB CALL: execute_query({sql})')\n\n    cursor = db_conn.cursor()\n\n    cursor.execute(sql)\n    return cursor.fetchall()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# These are the Python functions defined above.\ndb_tools = [list_tables, describe_table, execute_query]\n\ninstruction = \"\"\"You are a helpful chatbot that can interact with an SQL database for financial transactions. You will first use list_tables to see what tables are present, describe_table to understand the\nschema, and execute_query to issue an SQL SELECT query.\"\"\"\n\nclient = genai.Client(api_key=GOOGLE_API_KEY)\n\nfrom google.api_core import retry\n\n\nis_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n\ngenai.models.Models.generate_content = retry.Retry(\n    predicate=is_retriable)(genai.models.Models.generate_content)\n\n# Start with a message prompt for the model\n# Start with a message prompt for the model\nuser_query = \"\"\"1. You will find all unique combination of category.subcategory. \n2.Then you will pick one word or create short,one-word, acronyms for category, lower case only; \n3.You will pick max two words or create short, two-words max acronyms for subcategory, lower case only; \n4.Each acronym or word should be meaningful, distinctive, and ideally 3-6 characters.\n5.Avoid common words like \"and\", \"&\", \"of\", \"the\", etc.\n6.Make sure acronyms are unique within category and subcategory.\n7.Regenerate the combination using your generated result, seprate by \".\"\n\"\"\"\n                \n# Now use generate_content with required contents parameter\n# Add retry logic to your generate_content call\n@retry.Retry(predicate=is_retriable)\ndef get_category_mappings():\n    response = client.models.generate_content(\n        model=\"gemini-2.0-flash\",\n        contents=user_query,\n        config=types.GenerateContentConfig(\n            system_instruction=instruction,\n            tools=db_tools,\n        ),\n    )\n    return response.text\n\n# Get the category mappings with proper error handling\ntry:\n    category_response = get_category_mappings()\n    print(\"Successfully retrieved category mappings!\")\n    \n    # Save the raw response for reference\n    with open(\"category_mappings_raw.txt\", \"w\") as f:\n        f.write(category_response)\n        \nexcept Exception as e:\n    print(f\"Error retrieving category mappings: {e}\")\n    # Provide a fallback or exit gracefully","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\nimport pandas as pd\n\ndef parse_category_mappings(response_text):\n    \"\"\"\n    Parse the response text into a structured mapping DataFrame\n    with full names and their abbreviations\n    \"\"\"\n    # Create empty lists to store the parsed data\n    categories = []\n    subcategories = []\n    cat_abbrevs = []\n    subcat_abbrevs = []\n    \n    # Use regex to find patterns like \"Food & Beverages (food)\" or \"food.cafe\"\n    # Adjust this regex based on the actual format of your response\n    mapping_pattern = r'([A-Za-z &,]+)\\s*\\(([a-z0-9]{2,6})\\)'\n    combined_pattern = r'([a-z0-9]{2,6})\\.([a-z0-9]{2,6})'\n    \n    # First pass: extract category and subcategory full names with abbreviations\n    for line in response_text.split('\\n'):\n        if ':' in line:  # Likely a category with subcategories\n            parts = line.split(':')\n            if len(parts) >= 2:\n                cat_match = re.search(mapping_pattern, parts[0].strip())\n                if cat_match:\n                    cat_full = cat_match.group(1).strip()\n                    cat_abbr = cat_match.group(2).strip()\n                    \n                    # Process subcategories for this category\n                    subcat_section = parts[1].strip()\n                    for subcat_item in subcat_section.split(','):\n                        subcat_match = re.search(mapping_pattern, subcat_item.strip())\n                        if subcat_match:\n                            subcat_full = subcat_match.group(1).strip()\n                            subcat_abbr = subcat_match.group(2).strip()\n                            \n                            # Add to our lists\n                            categories.append(cat_full)\n                            subcategories.append(subcat_full)\n                            cat_abbrevs.append(cat_abbr)\n                            subcat_abbrevs.append(subcat_abbr)\n        \n        # Look for combined abbreviations like \"food.cafe\"\n        combined_matches = re.findall(combined_pattern, line)\n        for cat_abbr, subcat_abbr in combined_matches:\n            # Find the corresponding full names from earlier matches if possible\n            # This is a best-effort approach that might need manual verification\n            matches_cat = [i for i, abbr in enumerate(cat_abbrevs) if abbr == cat_abbr]\n            matches_subcat = [i for i, abbr in enumerate(subcat_abbrevs) if abbr == subcat_abbr]\n            \n            # If we find matches, use them; otherwise use placeholders\n            if matches_cat and matches_subcat:\n                cat_full = categories[matches_cat[0]]\n                subcat_full = subcategories[matches_subcat[0]]\n                \n                # Add to our lists if not already there\n                if cat_abbr not in cat_abbrevs or subcat_abbr not in subcat_abbrevs:\n                    categories.append(cat_full)\n                    subcategories.append(subcat_full)\n                    cat_abbrevs.append(cat_abbr)\n                    subcat_abbrevs.append(subcat_abbr)\n    \n    # Create a DataFrame from our parsed data\n    mapping_df = pd.DataFrame({\n        'category': categories,\n        'subcategory': subcategories,\n        'category_abbrev': cat_abbrevs,\n        'subcategory_abbrev': subcat_abbrevs,\n        'combined': [f\"{c}.{s}\" for c, s in zip(cat_abbrevs, subcat_abbrevs)]\n    })\n    \n    return mapping_df\n\n# Parse the response into a structured format\ntry:\n    category_mappings = parse_category_mappings(category_response)\n    print(f\"Parsed {len(category_mappings)} category/subcategory combinations\")\n    \n    # Save the structured mappings\n    category_mappings.to_csv(\"category_mappings.csv\", index=False)\n    print(\"Saved category mappings to CSV\")\n    \n    # Display a sample of the mappings\n    print(\"\\nSample mappings:\")\n    print(category_mappings.head())\n    \nexcept Exception as e:\n    print(f\"Error parsing category mappings: {e}\")\n    print(\"You may need to manually parse the response based on its actual format\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def transform_transaction_data(df, mapping_df):\n    \"\"\"\n    Transform the transaction data using the category mappings\n    \"\"\"\n    # Create a lookup dictionary for faster processing\n    mapping_dict = dict(zip(\n        zip(mapping_df['category'], mapping_df['subcategory']),\n        mapping_df['combined']\n    ))\n    \n    # Function to apply the mapping to each row\n    def get_combined_code(row):\n        key = (row['category'], row['subcategory'])\n        return mapping_dict.get(key, None)\n    \n    # Apply the mapping to create a new column\n    df_transformed = df.copy()\n    df_transformed['category_code'] = df.apply(get_combined_code, axis=1)\n    \n    return df_transformed\n\n# Apply the mappings to your transaction data\ntry:\n    # Check if your DataFrame has both category and subcategory columns\n    if 'category' not in df_train_sampled.columns:\n        # If you only have subcategory, we need to get the parent category\n        print(\"Retrieving parent categories for subcategories...\")\n        \n        # SQL query to get subcategory to category mapping\n        cursor = db_conn.cursor()\n        cursor.execute(\"\"\"\n        SELECT s.name as subcategory, c.name as category \n        FROM subcategories s\n        JOIN categories c ON s.category_id = c.category_id\n        \"\"\")\n        \n        subcat_to_cat = {row[0]: row[1] for row in cursor.fetchall()}\n        \n        # Add category column to DataFrame\n        df_train_sampled['category'] = df_train_sampled['subcategory'].map(subcat_to_cat)\n    \n    # Transform the data\n    df_transformed = transform_transaction_data(df_train_sampled, category_mappings)\n    print(f\"Transformed {len(df_transformed)} transactions with category codes\")\n    \n    # Save the transformed data\n    df_transformed.to_csv(\"transactions_transformed.csv\", index=False)\n    print(\"Saved transformed transactions to CSV\")\n    \n    # Display a sample\n    print(\"\\nSample transformed data:\")\n    print(df_transformed[['note', 'category', 'subcategory', 'category_code']].head())\n    \nexcept Exception as e:\n    print(f\"Error transforming transaction data: {e}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def prepare_fine_tuning_data(df_transformed):\n    \"\"\"\n    Prepare data for fine-tuning a model\n    \"\"\"\n    # Create training examples\n    training_examples = []\n    \n    for _, row in df_transformed.iterrows():\n        if pd.notna(row['category_code']):  # Skip rows without mapped codes\n            training_examples.append({\n                \"textInput\": str(row['note']),\n                \"output\": str(row['category_code'])  # Use the abbreviated code as output\n            })\n    \n    print(f\"Created {len(training_examples)} training examples\")\n    \n    # Prepare the dataset in the required format\n    training_data = {\"examples\": training_examples}\n    \n    return training_data\n\n# Prepare the fine-tuning data\ntuning_data = prepare_fine_tuning_data(df_transformed)\n\n# Save the fine-tuning data to a JSON file\nimport json\nwith open(\"fine_tuning_data.json\", \"w\") as f:\n    json.dump(tuning_data, f)\nprint(\"Saved fine-tuning data to JSON file\")\n\n# Display sample examples\nprint(\"\\nSample fine-tuning examples:\")\nfor example in tuning_data[\"examples\"][:3]:\n    print(f\"Input: '{example['textInput'][:50]}...'\")\n    print(f\"Output: '{example['output']}'\")\n    print()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Instruct the zero-shot prompt\nI draft the prompt asking it to only use the subcategory from the loaded table.","metadata":{}},{"cell_type":"code","source":" import sqlite3\n\ndb_file = \"transaction_categories.db\"\ndb_conn = sqlite3.connect(db_file)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from google.api_core import retry\n\n# Define a system instruction for classification with the subcategories list\nsystem_instruct = \"\"\"\nYou are a financial transaction categorization service. You will be provided with a transaction \ndescription (note) and must classify it into exactly one of the following subcategories:\n\n{}\n\nYour response must be ONLY the exact subcategory name from this list, with no additional text.\nDo not create new categories or modify existing ones.\nIf uncertain, choose the most likely subcategory from the list above.\n\"\"\"\n\n# Get the list of subcategories and format them for the prompt\nall_subcategories = sorted(df['subcategory'].unique())\nsubcategories_text = \"\\n\".join([f\"- {subcat}\" for subcat in all_subcategories])\n\n# Insert the subcategories into the system instruction\nsystem_instruct = system_instruct.format(subcategories_text)\n\n# Define a helper to retry when per-minute quota is reached\nis_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n\n@retry.Retry(predicate=is_retriable)\ndef predict_label(transaction_note: str) -> str:\n    \"\"\"Classify the provided transaction note into a subcategory from the predefined list.\"\"\"\n    response = client.models.generate_content(\n        model=\"gemini-2.0-flash\",\n        config=types.GenerateContentConfig(\n            system_instruction=system_instruct),\n        contents=transaction_note)\n    rc = response.candidates[0]\n    \n    # Any errors, filters, recitation, etc we can mark as a general error\n    if rc.finish_reason.name != \"STOP\":\n        return \"(error)\"\n    else:\n        # Clean up the response\n        prediction = response.text.strip()\n        \n        # Verify the prediction is from our list of subcategories\n        if prediction in all_subcategories:\n            return prediction\n        else:\n            # Find the closest matching subcategory if possible\n            for subcat in all_subcategories:\n                if subcat.lower() in prediction.lower():\n                    return subcat\n            return \"(invalid category)\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Evaluate baseline performance\n\nNow I perform an evaluation on the available models to ensure I can measure how much the tuning helps.","metadata":{}},{"cell_type":"code","source":"import tqdm\nfrom tqdm.rich import tqdm as tqdmr\nimport warnings\n\n# Enable tqdm features on Pandas\ntqdmr.pandas()\n\n# Suppress the experimental warning\nwarnings.filterwarnings(\"ignore\", category=tqdm.TqdmExperimentalWarning)\n\n# Further sample the test data to be mindful of the free-tier quota\n# Sample a small subset of test data (adjust number as needed)\nTEST_SAMPLE_SIZE = 20\ndf_baseline_eval = df_test_sampled.sample(min(TEST_SAMPLE_SIZE, len(df_test_sampled)))\n\nprint(f\"Evaluating {len(df_baseline_eval)} transactions...\")\n\n# Make predictions using the sampled data with progress bar\ndf_baseline_eval['prediction'] = df_baseline_eval['note'].progress_apply(predict_label)\n\n# Calculate the accuracy\naccuracy = (df_baseline_eval['subcategory'] == df_baseline_eval['prediction']).mean()\nprint(f\"Baseline accuracy: {accuracy:.2%}\")\n\n# Display some examples of predictions\nprint(\"\\nSample predictions:\")\nsample_results = df_baseline_eval[['note', 'subcategory', 'prediction']].sample(min(5, len(df_baseline_eval)))\nfor idx, row in sample_results.iterrows():\n    print(f\"Transaction: {row['note'][:50]}...\")\n    print(f\"True subcategory: {row['subcategory']}\")\n    print(f\"Predicted: {row['prediction']}\")\n    print(f\"Correct: {row['subcategory'] == row['prediction']}\\n\")\n\n# Create a confusion matrix to see where the model is making mistakes\nprint(\"Most common error patterns:\")\nerror_patterns = df_baseline_eval[df_baseline_eval['subcategory'] != df_baseline_eval['prediction']]\nif len(error_patterns) > 0:\n    error_counts = error_patterns.groupby(['subcategory', 'prediction']).size().reset_index(name='count')\n    error_counts = error_counts.sort_values('count', ascending=False)\n    print(error_counts.head(5))\nelse:\n    print(\"No errors found in the evaluation set!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Tune my model\nNow I train the model with training data to tune it for assessment and potential use.","metadata":{}},{"cell_type":"code","source":"from collections.abc import Iterable\nimport datetime\nimport time\nimport os\n\n# 1. Prepare your transaction data for fine-tuning\nprint(\"Preparing transaction data for fine-tuning...\")\n\n# Convert the DataFrame into the format expected by the API\ntraining_examples = []\nfor _, row in df_train_sampled.iterrows():\n    training_examples.append({\n        \"textInput\": str(row['note']),\n        \"output\": str(row['subcategory'])\n    })\n\nprint(f\"Created {len(training_examples)} training examples\")\nprint(f\"Sample example - Input: '{training_examples[0]['textInput'][:50]}...'\")\nprint(f\"Sample example - Output: '{training_examples[0]['output']}'\")\n\n# 2. Prepare the dataset in the required format\ntraining_data = {\"examples\": training_examples}\n\n# 3. Set up the fine-tuning job - find existing or create new\nmodel_id = None\n\ntry:\n    # Try to read previous model ID from file\n    try:\n        with open(\"tuned_model_id.txt\", \"r\") as f:\n            saved_model_id = f.read().strip()\n            if saved_model_id:\n                print(f\"Found previously saved model ID: {saved_model_id}\")\n                model_id = saved_model_id\n    except FileNotFoundError:\n        print(\"No previously saved model ID found.\")\n    \n    # If no saved ID, check for existing models\n    if not model_id:\n        queued_model = None\n        print(\"Checking for existing tuned models...\")\n        \n        # List models in reverse order (newest first)\n        for m in reversed(client.tunings.list()):\n            # Look for transaction classifier models with flexible matching\n            if (\"transaction\" in m.name.lower() or\n                m.name.startswith('tunedModels/personal-transaction-classifier-')):\n                \n                print(f\"Found potential model: {m.name} in state: {m.state.name}\")\n                \n                # If there is a completed model, use it\n                if m.state.name == 'JOB_STATE_SUCCEEDED':\n                    model_id = m.name\n                    print(f'Found existing completed model to reuse: {model_id}')\n                    break\n                elif m.state.name == 'JOB_STATE_RUNNING' and not queued_model:\n                    # If there's a model still running, remember it\n                    queued_model = m.name\n                    print(f'Found model still in progress: {queued_model}')\n        \n        # Use queued model if found and no completed model\n        if not model_id and queued_model:\n            model_id = queued_model\n            print(f'Using in-progress model: {model_id}')\n    \n    # Create new model if needed\n    if not model_id:\n        print(\"Starting new fine-tuning job...\")\n        tuning_op = client.tunings.tune(\n            base_model=\"models/gemini-1.5-flash-001-tuning\",\n            training_dataset=training_data,\n            config=types.CreateTuningJobConfig(\n                tuned_model_display_name=\"transaction-category-classifier\",  \n                batch_size=16,\n                epoch_count=3,\n            ),\n        )\n        \n        model_id = tuning_op.name\n        print(f\"Fine-tuning initiated. Model ID: {model_id}\")\n        print(f\"Current status: {tuning_op.state}\")\n        \n        # Poll for status updates (optional)\n        print(\"Initial training status:\")\n        print(f\"  - State: {tuning_op.state}\")\n        print(f\"  - Create time: {tuning_op.create_time}\")\n        if hasattr(tuning_op, 'progress') and tuning_op.progress:\n            print(f\"  - Progress: {tuning_op.progress}%\")\n    \n    # Save the model ID for later use\n    with open(\"tuned_model_id.txt\", \"w\") as f:\n        f.write(model_id)\n    \n    print(f\"\\nUsing model: {model_id}\")\n    print(\"This ID has been saved and will be used for predictions\")\n    \nexcept Exception as e:\n    print(f\"Error in fine-tuning process: {e}\")","metadata":{"execution":{"iopub.execute_input":"2025-04-10T05:48:56.388511Z","iopub.status.busy":"2025-04-10T05:48:56.387205Z","iopub.status.idle":"2025-04-10T05:48:58.145557Z","shell.execute_reply":"2025-04-10T05:48:58.144833Z","shell.execute_reply.started":"2025-04-10T05:48:56.388457Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Monitoring progress\nHere I monitor whether this model has been tuned and ready to use.","metadata":{}},{"cell_type":"code","source":"# 4. Monitor the fine-tuning progress\nstart_time = datetime.datetime.now(datetime.timezone.utc)\ntuned_model = client.tunings.get(name=model_id)\n\nwhile not tuned_model.has_ended:\n    print(f\"Current state: {tuned_model.state.name}\")\n    if hasattr(tuned_model, 'progress'):\n        print(f\"Progress: {tuned_model.progress}%\")\n    \n    time.sleep(60)  # Check every minute\n    tuned_model = client.tunings.get(name=model_id)\n\nprint(f\"Done! The model state is: {tuned_model.state.name}\")\n\nif not tuned_model.has_succeeded and tuned_model.error:\n    print(\"Error:\", tuned_model.error)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Evaluate Tuned Model\nHere I test and evaluate the performance of the tuned model.","metadata":{}},{"cell_type":"code","source":"# 5. Test the tuned model with a sample transaction\ndef categorize_transaction(transaction_note):\n    \"\"\"Use the fine-tuned model to categorize a transaction.\"\"\"\n    response = client.models.generate_content(\n        model=model_id,\n        contents=transaction_note,\n        config=types.GenerateContentConfig(\n            temperature=0.0,  # Use deterministic output for classification\n            max_output_tokens=10,  # Keep it short, we just need the category\n        )\n    )\n    \n    if response.candidates and response.candidates[0].content:\n        return response.candidates[0].content.parts[0].text.strip()\n    else:\n        return \"(error)\"\n\n# Test with a sample transaction\nsample_transaction = \"AMAZON PRIME MEMBERSHIP ANNUAL RENEWAL\"\npredicted_category = categorize_transaction(sample_transaction)\nprint(f\"Transaction: {sample_transaction}\")\nprint(f\"Predicted category: {predicted_category}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 6. Evaluate the model on test data\nimport tqdm\nfrom tqdm.rich import tqdm as tqdmr\nimport warnings\n\n# Enable tqdm features on Pandas\ntqdmr.pandas()\n\n# Suppress the experimental warning\nwarnings.filterwarnings(\"ignore\", category=tqdm.TqdmExperimentalWarning)\n\n# Sample a subset of test data for evaluation\nTEST_SAMPLE_SIZE = 20\ndf_eval = df_test_sampled.sample(min(TEST_SAMPLE_SIZE, len(df_test_sampled)))\n\nprint(f\"Evaluating on {len(df_eval)} test transactions...\")\n\n# Make predictions with progress bar\ndf_eval['prediction'] = df_eval['note'].progress_apply(categorize_transaction)\n\n# Calculate accuracy\naccuracy = (df_eval['subcategory'] == df_eval['prediction']).mean()\nprint(f\"Model accuracy: {accuracy:.2%}\")\n\n# Display some examples\nprint(\"\\nSample predictions:\")\nfor idx, row in df_eval.sample(min(5, len(df_eval))).iterrows():\n    print(f\"Transaction: {row['note'][:50]}...\")\n    print(f\"True category: {row['subcategory']}\")\n    print(f\"Predicted: {row['prediction']}\")\n    print(f\"Correct: {row['subcategory'] == row['prediction']}\\n\")\n\n# Show error analysis\nerrors = df_eval[df_eval['subcategory'] != df_eval['prediction']]\nif len(errors) > 0:\n    print(f\"Found {len(errors)} misclassifications\")\n    print(\"Most common error patterns:\")\n    error_matrix = pd.crosstab(\n        errors['subcategory'], \n        errors['prediction'], \n        rownames=['True'], \n        colnames=['Predicted']\n    )\n    print(error_matrix)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}