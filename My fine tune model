{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"colab":{"name":"day-4-fine-tuning-a-custom-model.ipynb","toc_visible":true},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Categorise finance transactions\n\nIn life, my financial transactions are often categorised incorrectly in my budgeting app. I decided to find a better solution.\n\nIn this example, I will first try to categorise with an existing Gemini model using a zero-shot prompt and evaluate its performance. Then I will tune a model with the data categorised by me and evaluate its performance.","metadata":{"id":"4KDIFPAL2EnL"}},{"cell_type":"code","source":"# Install required libraries\n!pip uninstall -qqy jupyterlab  # Remove unused conflicting packages\n!pip install -U -q \"google-genai==1.7.0\"","metadata":{"execution":{"iopub.status.busy":"2025-04-13T07:46:51.791162Z","iopub.execute_input":"2025-04-13T07:46:51.791556Z","iopub.status.idle":"2025-04-13T07:47:09.416070Z","shell.execute_reply.started":"2025-04-13T07:46:51.791511Z","shell.execute_reply":"2025-04-13T07:47:09.414668Z"},"id":"9wafTyEH1_xF","trusted":true},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Import necessary libraries\nfrom google import genai\nfrom google.genai import types\n\ngenai.__version__","metadata":{"id":"T0CBG9xL2PvT","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T07:47:09.418444Z","iopub.execute_input":"2025-04-13T07:47:09.418826Z","iopub.status.idle":"2025-04-13T07:47:10.703358Z","shell.execute_reply.started":"2025-04-13T07:47:09.418791Z","shell.execute_reply":"2025-04-13T07:47:10.702203Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"'1.7.0'"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"# Set up the Google GenAI client\nfrom kaggle_secrets import UserSecretsClient\n\nGOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n\nclient = genai.Client(api_key=GOOGLE_API_KEY)","metadata":{"id":"VuJPY3GK2SLZ","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T07:47:10.704764Z","iopub.execute_input":"2025-04-13T07:47:10.705347Z","iopub.status.idle":"2025-04-13T07:47:10.940607Z","shell.execute_reply.started":"2025-04-13T07:47:10.705301Z","shell.execute_reply":"2025-04-13T07:47:10.939582Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"### Explore available models","metadata":{"id":"CqVA5QFO6n4z"}},{"cell_type":"code","source":"# List available models\nfor model in client.models.list():\n    print(model.name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T07:47:10.941956Z","iopub.execute_input":"2025-04-13T07:47:10.942349Z","iopub.status.idle":"2025-04-13T07:47:11.172225Z","shell.execute_reply.started":"2025-04-13T07:47:10.942304Z","shell.execute_reply":"2025-04-13T07:47:11.171008Z"}},"outputs":[{"name":"stdout","text":"models/chat-bison-001\nmodels/text-bison-001\nmodels/embedding-gecko-001\nmodels/gemini-1.0-pro-vision-latest\nmodels/gemini-pro-vision\nmodels/gemini-1.5-pro-latest\nmodels/gemini-1.5-pro-001\nmodels/gemini-1.5-pro-002\nmodels/gemini-1.5-pro\nmodels/gemini-1.5-flash-latest\nmodels/gemini-1.5-flash-001\nmodels/gemini-1.5-flash-001-tuning\nmodels/gemini-1.5-flash\nmodels/gemini-1.5-flash-002\nmodels/gemini-1.5-flash-8b\nmodels/gemini-1.5-flash-8b-001\nmodels/gemini-1.5-flash-8b-latest\nmodels/gemini-1.5-flash-8b-exp-0827\nmodels/gemini-1.5-flash-8b-exp-0924\nmodels/gemini-2.5-pro-exp-03-25\nmodels/gemini-2.5-pro-preview-03-25\nmodels/gemini-2.0-flash-exp\nmodels/gemini-2.0-flash\nmodels/gemini-2.0-flash-001\nmodels/gemini-2.0-flash-exp-image-generation\nmodels/gemini-2.0-flash-lite-001\nmodels/gemini-2.0-flash-lite\nmodels/gemini-2.0-flash-lite-preview-02-05\nmodels/gemini-2.0-flash-lite-preview\nmodels/gemini-2.0-pro-exp\nmodels/gemini-2.0-pro-exp-02-05\nmodels/gemini-exp-1206\nmodels/gemini-2.0-flash-thinking-exp-01-21\nmodels/gemini-2.0-flash-thinking-exp\nmodels/gemini-2.0-flash-thinking-exp-1219\nmodels/learnlm-1.5-pro-experimental\nmodels/gemma-3-1b-it\nmodels/gemma-3-4b-it\nmodels/gemma-3-12b-it\nmodels/gemma-3-27b-it\nmodels/embedding-001\nmodels/text-embedding-004\nmodels/gemini-embedding-exp-03-07\nmodels/gemini-embedding-exp\nmodels/aqa\nmodels/imagen-3.0-generate-002\nmodels/veo-2.0-generate-001\nmodels/gemini-2.0-flash-live-001\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## Use the dataset\n\nI have uploaded transaction data categorised by me. Then I group it into training data and test data.","metadata":{"id":"peFm0w_0c1CO"}},{"cell_type":"code","source":"# Load and preprocess transaction data\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Load your data\nfile_path = \"/kaggle/input/training/categorized_transaction.csv\"\ndf = pd.read_csv(file_path)\n\n# Split into train and test sets (80/20 split)\ndf_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n\n# Display the subcategories (labels) in your dataset\nsubcategories = df['subcategory'].unique()\nprint(f\"Number of subcategories: {len(subcategories)}\")\nprint(\"Sample subcategories:\", subcategories[:10])  # Show first 10 subcategories\n\n# Quick look at note examples\nprint(\"\\nSample notes:\")\nfor i, note in enumerate(df['note'].head(3)):\n    print(f\"{i+1}. {note} → {df['subcategory'].iloc[i]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T07:47:11.175430Z","iopub.execute_input":"2025-04-13T07:47:11.175930Z","iopub.status.idle":"2025-04-13T07:47:12.246284Z","shell.execute_reply.started":"2025-04-13T07:47:11.175896Z","shell.execute_reply":"2025-04-13T07:47:12.245031Z"}},"outputs":[{"name":"stdout","text":"Number of subcategories: 65\nSample subcategories: ['Active sport, fitness' 'Advisory' 'Alcohol, tobacco'\n 'Bar, cafe, drink, snacks' 'Books, audio, subscriptions' 'Charges, Fees'\n 'Charity, gifts' 'Checks, coupons' 'Clothes & shoes'\n 'Culture, sport events']\n\nSample notes:\n1. AMAZON AUSYDNEY SOUTH CREDIT CARD PURCHASEAmazon Basics High-Density Round Foam Roller for Exercise and Recovery - 61cm, Blue Speckled → Active sport, fitness\n2. 02 DEC 20 - $98.00 LULULEMON ATHLETICA AUSTRAlbert Park [Eff Date: 30 NOV 20] → Active sport, fitness\n3. REBEL MELBOURNE CTRL MELBOURNE VI AUSTap and Pay xx3173Value Date_ 17/03/2018 → Active sport, fitness\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## Clean the data","metadata":{}},{"cell_type":"code","source":"import re\nimport pandas as pd\n\ndef preprocess_transaction_note(note):\n    \"\"\"\n    Clean and standardize a transaction note, removing dollar amounts and other noise.\n    \"\"\"\n    # Handle None or empty strings\n    if not note or pd.isna(note):\n        return \"\"\n    \n    # Convert to string if needed\n    text = str(note)\n    \n    # Extract meaningful parts (keeping merchant name and location)\n    # Split on common separators in transaction data\n    parts = re.split(r'\\s+(?:CREDIT CARD PURCHASE|EFTPOS|Value Date|tap and Pay|Card Purchase)', text, flags=re.IGNORECASE)\n    main_text = parts[0] if parts else text\n    \n    # Clean dollar amounts with various currency symbols\n    main_text = re.sub(r'(?:(?:\\$|AUD|USD|EUR|GBP)\\s*)?(?:-)?(?:\\d{1,3}(?:,\\d{3})*|\\d+)(?:\\.\\d{1,2})?', '', main_text)\n    \n    # Remove card numbers (masked or full)\n    main_text = re.sub(r'(?:x{2,4}|X{2,4})\\d{4}', '', main_text)\n    main_text = re.sub(r'\\b\\d{4}[\\s-]?\\d{4}[\\s-]?\\d{4}[\\s-]?\\d{4}\\b', '', main_text)\n    \n    # Remove dates in various formats\n    date_patterns = [\n        r'\\d{1,2}\\s+(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\\s+\\d{2,4}',  # 01 Jan 2023\n        r'(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\\s+\\d{1,2}\\s+\\d{2,4}',  # Jan 01 2023\n        r'\\d{1,2}/\\d{1,2}/\\d{2,4}',  # 01/01/2023\n        r'\\d{1,2}-\\d{1,2}-\\d{2,4}',  # 01-01-2023\n        r'\\d{1,2}\\s+[A-Za-z]{3}\\s+\\d{2}',  # 02 DEC 20\n    ]\n    for pattern in date_patterns:\n        main_text = re.sub(pattern, '', main_text, flags=re.IGNORECASE)\n    \n    # Remove reference numbers and transaction IDs\n    main_text = re.sub(r'Ref(?:erence)?:?\\s*[A-Za-z0-9]+', '', main_text, flags=re.IGNORECASE)\n    main_text = re.sub(r'#\\d+', '', main_text)\n    main_text = re.sub(r'\\[Eff\\s+Date:.*?\\]', '', main_text)\n    \n    # Remove email addresses (like in the newsgroup example)\n    main_text = re.sub(r'[\\w\\.-]+@[\\w\\.-]+', '', main_text)\n    \n    # Remove common transaction suffixes\n    suffixes = [\n        r'Electronic Transfer',\n        r'Direct Debit',\n        r'PURCHASE',\n        r'Purchase',\n        r'PAYMENT',\n        r'Payment',\n        r'MONTHLY',\n        r'RENEWAL',\n        r'Renewal',\n        r'SUBSCRIPTION',\n        r'Subscription'\n    ]\n    for suffix in suffixes:\n        main_text = re.sub(suffix, '', main_text, flags=re.IGNORECASE)\n    \n    # Clean up extra whitespace and standardize\n    main_text = re.sub(r'\\s+', ' ', main_text).strip()\n    \n    # Truncate long text (usually not an issue with transaction data but included for safety)\n    main_text = main_text[:500]\n    \n    return main_text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T07:47:12.247648Z","iopub.execute_input":"2025-04-13T07:47:12.248077Z","iopub.status.idle":"2025-04-13T07:47:12.258211Z","shell.execute_reply.started":"2025-04-13T07:47:12.248041Z","shell.execute_reply":"2025-04-13T07:47:12.257195Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def process_transactions_for_fine_tuning(df):\n    # Step 1: Clean transaction notes first\n    print(\"Cleaning transaction notes...\")\n    df['cleaned_note'] = df['note'].apply(preprocess_transaction_note)\n    \n    # Step 2: Then perform category/subcategory transformations\n    print(\"Transforming categories to codes...\")\n    df = transform_categories(df)\n    \n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T07:47:12.259458Z","iopub.execute_input":"2025-04-13T07:47:12.259831Z","iopub.status.idle":"2025-04-13T07:47:12.277539Z","shell.execute_reply.started":"2025-04-13T07:47:12.259762Z","shell.execute_reply":"2025-04-13T07:47:12.276379Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"## Sample the dataset\nNow sample the data. I will keep 50 rows for each subcategory for training.","metadata":{"id":"03lDs1O4ZQ0-"}},{"cell_type":"code","source":"def sample_data(df, num_samples):\n    \"\"\"\n    Sample rows from each subcategory, selecting num_samples from each.\n    If a subcategory has fewer than num_samples entries, takes all available rows.\n    \n    Args:\n        df: DataFrame containing transaction data\n        num_samples: Number of samples to take per subcategory\n        \n    Returns:\n        DataFrame with balanced samples across subcategories\n    \"\"\"\n    # Group by subcategory and sample\n    sampled_df = (\n        df.groupby(\"subcategory\")[df.columns]\n        .apply(lambda x: x.sample(min(len(x), num_samples)))\n        .reset_index(drop=True)\n    )\n    \n    # Convert subcategory to category type for efficiency\n    sampled_df[\"subcategory\"] = sampled_df[\"subcategory\"].astype(\"category\")\n    \n    return sampled_df\n\n# Sample training and test data\nTRAIN_NUM_SAMPLES = 50  # 50 samples per subcategory for training\nTEST_NUM_SAMPLES = 10   # 10 samples per subcategory for testing\n\n# Create balanced datasets\ndf_train_sampled = sample_data(df_train, TRAIN_NUM_SAMPLES)\ndf_test_sampled = sample_data(df_test, TEST_NUM_SAMPLES)\n\n# Print statistics about the sampled data\nprint(f\"Original training data: {len(df_train)} rows\")\nprint(f\"Sampled training data: {len(df_train_sampled)} rows\")\nprint(f\"Number of subcategories: {df_train_sampled['subcategory'].nunique()}\")\n\n# Show distribution of a few subcategories\nprint(\"\\nSample of subcategory counts in training data:\")\nprint(df_train_sampled['subcategory'].value_counts().head(5))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T07:47:12.278884Z","iopub.execute_input":"2025-04-13T07:47:12.279328Z","iopub.status.idle":"2025-04-13T07:47:12.333401Z","shell.execute_reply.started":"2025-04-13T07:47:12.279284Z","shell.execute_reply":"2025-04-13T07:47:12.332102Z"}},"outputs":[{"name":"stdout","text":"Original training data: 9936 rows\nSampled training data: 1654 rows\nNumber of subcategories: 63\n\nSample of subcategory counts in training data:\nsubcategory\nHobbies                  50\nInterests, dividends     50\nHome, garden             50\nTRANSFER                 50\nSoftware, apps, games    50\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"## Load the subcategory and category table\nIn this step, I load the subcategory and category table.","metadata":{}},{"cell_type":"code","source":"import sqlite3\n\n# Connect to your database\ndb_conn = sqlite3.connect('/kaggle/working/transaction_categories.db')\ncursor = db_conn.cursor()\n\n# Create the tables\ncursor.execute('''\nCREATE TABLE IF NOT EXISTS categories (\n    category_id INTEGER PRIMARY KEY AUTOINCREMENT,\n    name VARCHAR(100) NOT NULL UNIQUE,\n    description TEXT,\n    display_order INT DEFAULT 100,\n    is_active BOOLEAN DEFAULT 1,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n)\n''')\n\ncursor.execute('''\nCREATE TABLE IF NOT EXISTS subcategories (\n    subcategory_id INTEGER PRIMARY KEY AUTOINCREMENT,\n    category_id INTEGER NOT NULL,\n    name VARCHAR(100) NOT NULL,\n    description TEXT,\n    display_order INT DEFAULT 100,\n    is_active BOOLEAN DEFAULT 1,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    FOREIGN KEY (category_id) REFERENCES categories(category_id)\n)\n''')\n\n\n# Insert main categories\ncategories = [\n    ('Food & Beverages', 'Expenses related to food and drinks', 10),\n    ('Shopping', 'Retail purchases and shopping expenses', 20),\n    ('Housing', 'Home-related expenses including rent and utilities', 30),\n    ('Transportation', 'Public and private transportation costs', 40),\n    ('Vehicle', 'Car and vehicle related expenses', 50),\n    ('Life & Entertainment', 'Leisure activities and entertainment', 60),\n    ('Communication, PC', 'Internet, phone and computer expenses', 70),\n    ('Financial expenses', 'Banking fees, loans, and financial costs', 80),\n    ('Investments', 'Investment-related transactions', 90),\n    ('Income', 'All sources of incoming money', 100),\n    ('Transfer', 'Money transfers between accounts', 110)\n]\n\ncursor.executemany('INSERT OR IGNORE INTO categories (name, description, display_order) VALUES (?, ?, ?)', categories)\n\n# Insert subcategories for Food & Beverages\nfood_subcategories = [\n    (1, 'Bar, cafe, drink, snacks', 10),\n    (1, 'Groceries', 20),\n    (1, 'Restaurant, fast-food', 30)\n]\ncursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', food_subcategories)\n\n# Insert subcategories for Shopping\nshopping_subcategories = [\n    (2, 'Clothes & Footwear', 10),\n    (2, 'Drug-store, chemist', 20),\n    (2, 'Electronics, accessories', 30),\n    (2, 'Gifts, joy', 40),\n    (2, 'Health and beauty', 50),\n    (2, 'Home, garden', 60),\n    (2, 'Jewels, accessories', 70),\n    (2, 'Kids', 80),\n    (2, 'Leisure time', 90),\n    (2, 'Pets, animals', 100),\n    (2, 'Stationery, tools', 110)\n]\ncursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', shopping_subcategories)\n\nhousing_subcategories=[\n    (3, 'Energy, utilities', 10),\n    (3, 'Maintenance, repairs', 20),\n    (3, 'Mortgage', 30),\n    (3, 'Property insurance', 40),\n    (3, 'Rent', 50),\n    (3, 'Services', 60)\n]\ncursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', housing_subcategories)\n\n\ntransportation_subcategories=[\n    (4, 'Business trips', 10),\n    (4, 'Long distance', 20),\n    (4, 'Public transport', 30),\n    (4, 'Taxi', 40)\n]\ncursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', transportation_subcategories)\n\nvehicle_subcategories=[\n    (5, 'Fuel', 10),\n    (5, 'Leasing', 20),\n    (5, 'Parking', 30),\n    (5, 'Rentals', 40),\n    (5, 'Vehicle insurance', 50),\n    (5, 'Vehicle maintenance', 60)\n]\ncursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', vehicle_subcategories)\n\nlife_subcategories=[\n    (6, 'Active sport, fitness', 10),\n    (6, 'Alcohol, tobacco', 20),\n    (6, 'Books, audio, subscriptions', 30),\n    (6, 'Charity, gifts', 40),\n    (6, 'Culture, sport events', 50),\n    (6, 'Education, development', 60),\n    (6, 'Health care, doctor', 70),\n    (6, 'Hobbies', 80),\n    (6, 'Holiday, trips, hotels', 90),\n    (6, 'Life events', 100),\n    (6, 'Lottery, gambling', 110),\n    (6, 'TV, Streaming', 120),\n    (6, 'Wellness, beauty', 130)\n]\ncursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', life_subcategories)\n\ncommunication_subcategories=[\n    (7, 'Internet', 10),\n    (7, 'Postal services', 20),\n    (7, 'Software, apps, games', 30),\n    (7, 'Telephony, mobile phone', 40)\n]\ncursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', communication_subcategories)\n\nfinancial_subcategories=[\n    (8, 'Advisory', 10),\n    (8, 'Charges, Fees', 20),\n    (8, 'Child Support', 30),\n    (8, 'Fines', 40),\n    (8, 'Insurances', 50),\n    (8, 'Loans, interests', 60),\n    (8, 'Taxes', 70)\n]\ncursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', financial_subcategories)\n\ninvestments_subcategories=[\n    (9, 'Collections', 10),\n    (9, 'Financial investments', 20),\n    (9, 'Realty', 30),\n    (9, 'Savings', 40),\n    (9, 'Vehicles, chattels', 50)\n]\ncursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', investments_subcategories)\n\nincome_subcategories=[\n    (10, 'Checks, coupons', 10),\n    (10, 'Child Support', 20),\n    (10, 'Dues & grants', 30),\n    (10, 'Gifts', 40),\n    (10, 'Interests, dividends', 50),\n    (10, 'Lending, renting', 60),\n    (10, 'Lottery earning', 70),\n    (10, 'Refunds (tax, purchase)', 80),\n    (10, 'Rental income', 90),\n    (10, 'Sale', 100),\n    (10, 'Wage, invoices', 110)\n]\ncursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', income_subcategories)\n\ntransfer_subcatgories=[\n    (11, 'Transfer', 10),   \n]\ncursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', transfer_subcatgories)\n\n# Commit the changes\ndb_conn.commit()\nprint(\"Database schema created successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T07:47:12.334974Z","iopub.execute_input":"2025-04-13T07:47:12.335325Z","iopub.status.idle":"2025-04-13T07:47:12.385040Z","shell.execute_reply.started":"2025-04-13T07:47:12.335293Z","shell.execute_reply":"2025-04-13T07:47:12.383832Z"}},"outputs":[{"name":"stdout","text":"Database schema created successfully!\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"def list_tables() -> list[str]:\n    \"\"\"Retrieve the names of all tables in the database.\"\"\"\n    # Include print logging statements so you can see when functions are being called.\n    print(' - DB CALL: list_tables()')\n\n    cursor = db_conn.cursor()\n\n    # Fetch the table names.\n    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n\n    tables = cursor.fetchall()\n    return [t[0] for t in tables]\n\n\nlist_tables()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T07:47:12.386604Z","iopub.execute_input":"2025-04-13T07:47:12.387055Z","iopub.status.idle":"2025-04-13T07:47:12.397551Z","shell.execute_reply.started":"2025-04-13T07:47:12.387009Z","shell.execute_reply":"2025-04-13T07:47:12.396384Z"}},"outputs":[{"name":"stdout","text":" - DB CALL: list_tables()\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"['categories', 'sqlite_sequence', 'subcategories']"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"def describe_table(table_name: str) -> list[tuple[str, str]]:\n    \"\"\"Look up the table schema.\n\n    Returns:\n      List of columns, where each entry is a tuple of (column, type).\n    \"\"\"\n    print(f' - DB CALL: describe_table({table_name})')\n\n    cursor = db_conn.cursor()\n\n    cursor.execute(f\"PRAGMA table_info({table_name});\")\n\n    schema = cursor.fetchall()\n    # [column index, column name, column type, ...]\n    return [(col[1], col[2]) for col in schema]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T07:47:12.399097Z","iopub.execute_input":"2025-04-13T07:47:12.399523Z","iopub.status.idle":"2025-04-13T07:47:12.417932Z","shell.execute_reply.started":"2025-04-13T07:47:12.399478Z","shell.execute_reply":"2025-04-13T07:47:12.416686Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def execute_query(sql: str) -> list[list[str]]:\n    \"\"\"Execute an SQL statement, returning the results.\"\"\"\n    print(f' - DB CALL: execute_query({sql})')\n\n    cursor = db_conn.cursor()\n\n    cursor.execute(sql)\n    return cursor.fetchall()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T07:47:12.419315Z","iopub.execute_input":"2025-04-13T07:47:12.419734Z","iopub.status.idle":"2025-04-13T07:47:12.439322Z","shell.execute_reply.started":"2025-04-13T07:47:12.419689Z","shell.execute_reply":"2025-04-13T07:47:12.438179Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# These are the database interaction tools defined earlier\ndb_tools = [list_tables, describe_table, execute_query]\n\n# System instruction for the AI to understand what it needs to do\ninstruction = \"\"\"You are a helpful chatbot that can interact with an SQL database for financial transactions. \nYou will first use list_tables to see what tables are present, describe_table to understand the\nschema, and execute_query to issue an SQL SELECT query to retrieve all category-subcategory combinations.\"\"\"\n\n# Create the Google Genai client\nclient = genai.Client(api_key=GOOGLE_API_KEY)\n\nfrom google.api_core import retry\n\n# Define retry logic for API rate limits\nis_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n\n# User query to get category mappings suitable for training data\nuser_query = \"\"\"1. Find all unique combinations of category and subcategory from the database.\n2. Then show the combinations as category.subcategory\n3. Output your results ONLY as a CSV with these exact columns: category,subcategory,combination\n4. Do not include headers, just the data rows\n5. Do not include any explanatory text before or after the CSV data\n\"\"\"\n\n# Function to get category mappings with retry logic\n@retry.Retry(predicate=is_retriable)\ndef get_category_mappings_csv():\n    response = client.models.generate_content(\n        model=\"gemini-2.0-flash\",\n        contents=user_query,\n        config=types.GenerateContentConfig(\n            system_instruction=instruction,\n            tools=db_tools,\n        ),\n    )\n    return response.text\n\n# Get the category mappings with proper error handling\ntry:\n    print(\"Requesting AI to generate category-subcategory mappings in CSV format...\")\n    csv_response = get_category_mappings_csv()\n    \n    # Save the raw CSV response\n    csv_file_path = \"category_mappings.csv\"\n    with open(csv_file_path, \"w\") as f:\n        # Add the header row first\n        f.write(\"category,subcategory,combination\")\n        # Then add the data from the response\n        f.write(csv_response)\n    \n    print(f\"Successfully saved CSV mappings to {csv_file_path}\")\n    \n    # Load and display first few rows for verification\n    import pandas as pd\n    try:\n        df = pd.read_csv(csv_file_path)\n        print(\"\\nFirst 5 rows of the generated mappings:\")\n        print(df.head())\n    except Exception as e:\n        print(f\"Note: Could not display preview due to: {e}\")\n    \nexcept Exception as e:\n    print(f\"Error retrieving category mappings: {e}\")\n    # Provide a fallback or exit gracefully","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T08:23:14.038888Z","iopub.execute_input":"2025-04-13T08:23:14.039568Z","iopub.status.idle":"2025-04-13T08:23:20.115701Z","shell.execute_reply.started":"2025-04-13T08:23:14.039532Z","shell.execute_reply":"2025-04-13T08:23:20.114685Z"}},"outputs":[{"name":"stdout","text":"Requesting AI to generate category-subcategory mappings in CSV format...\n - DB CALL: list_tables()\n - DB CALL: describe_table(categories)\n - DB CALL: describe_table(subcategories)\n - DB CALL: execute_query(SELECT c.name AS category, s.name AS subcategory, c.name || '.' || s.name AS combination FROM categories c JOIN subcategories s ON c.category_id = s.category_id)\nSuccessfully saved CSV mappings to category_mappings.csv\n\nFirst 5 rows of the generated mappings:\n                                                                                                                            category  \\\nFood & Beverages Bar                 cafe                        drink                       snacks    Food & Beverages.Bar     cafe   \n                 Groceries          Food & Beverages.Groceries  NaN                         NaN        NaN                       NaN   \n                 Restaurant          fast-food                  Food & Beverages.Restaurant  fast-food NaN                       NaN   \nShopping         Clothes & Footwear Shopping.Clothes & Footwear NaN                         NaN        NaN                       NaN   \n                 Drug-store          chemist                    Shopping.Drug-store          chemist   NaN                       NaN   \n\n                                                                                                                            subcategory  \\\nFood & Beverages Bar                 cafe                        drink                       snacks    Food & Beverages.Bar       drink   \n                 Groceries          Food & Beverages.Groceries  NaN                         NaN        NaN                          NaN   \n                 Restaurant          fast-food                  Food & Beverages.Restaurant  fast-food NaN                          NaN   \nShopping         Clothes & Footwear Shopping.Clothes & Footwear NaN                         NaN        NaN                          NaN   \n                 Drug-store          chemist                    Shopping.Drug-store          chemist   NaN                          NaN   \n\n                                                                                                                            combination```csv  \nFood & Beverages Bar                 cafe                        drink                       snacks    Food & Beverages.Bar            snacks  \n                 Groceries          Food & Beverages.Groceries  NaN                         NaN        NaN                                NaN  \n                 Restaurant          fast-food                  Food & Beverages.Restaurant  fast-food NaN                                NaN  \nShopping         Clothes & Footwear Shopping.Clothes & Footwear NaN                         NaN        NaN                                NaN  \n                 Drug-store          chemist                    Shopping.Drug-store          chemist   NaN                                NaN  \n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"import re\nimport pandas as pd\n\ndef parse_category_mappings(response_text):\n    \"\"\"\n    Parse the response text into a structured mapping DataFrame\n    with full names and their abbreviations\n    \"\"\"\n    # Create empty lists to store the parsed data\n    categories = []\n    subcategories = []\n    cat_abbrevs = []\n    subcat_abbrevs = []\n    \n    # Use regex to find patterns like \"Food & Beverages (food)\" or \"food.cafe\"\n    # Adjust this regex based on the actual format of your response\n    mapping_pattern = r'([A-Za-z &,]+)\\s*\\(([a-z0-9]{2,6})\\)'\n    combined_pattern = r'([a-z0-9]{2,6})\\.([a-z0-9]{2,6})'\n    \n    # First pass: extract category and subcategory full names with abbreviations\n    for line in response_text.split('\\n'):\n        if ':' in line:  # Likely a category with subcategories\n            parts = line.split(':')\n            if len(parts) >= 2:\n                cat_match = re.search(mapping_pattern, parts[0].strip())\n                if cat_match:\n                    cat_full = cat_match.group(1).strip()\n                    cat_abbr = cat_match.group(2).strip()\n                    \n                    # Process subcategories for this category\n                    subcat_section = parts[1].strip()\n                    for subcat_item in subcat_section.split(','):\n                        subcat_match = re.search(mapping_pattern, subcat_item.strip())\n                        if subcat_match:\n                            subcat_full = subcat_match.group(1).strip()\n                            subcat_abbr = subcat_match.group(2).strip()\n                            \n                            # Add to our lists\n                            categories.append(cat_full)\n                            subcategories.append(subcat_full)\n                            cat_abbrevs.append(cat_abbr)\n                            subcat_abbrevs.append(subcat_abbr)\n        \n        # Look for combined abbreviations like \"food.cafe\"\n        combined_matches = re.findall(combined_pattern, line)\n        for cat_abbr, subcat_abbr in combined_matches:\n            # Find the corresponding full names from earlier matches if possible\n            # This is a best-effort approach that might need manual verification\n            matches_cat = [i for i, abbr in enumerate(cat_abbrevs) if abbr == cat_abbr]\n            matches_subcat = [i for i, abbr in enumerate(subcat_abbrevs) if abbr == subcat_abbr]\n            \n            # If we find matches, use them; otherwise use placeholders\n            if matches_cat and matches_subcat:\n                cat_full = categories[matches_cat[0]]\n                subcat_full = subcategories[matches_subcat[0]]\n                \n                # Add to our lists if not already there\n                if cat_abbr not in cat_abbrevs or subcat_abbr not in subcat_abbrevs:\n                    categories.append(cat_full)\n                    subcategories.append(subcat_full)\n                    cat_abbrevs.append(cat_abbr)\n                    subcat_abbrevs.append(subcat_abbr)\n    \n    # Create a DataFrame from our parsed data\n    mapping_df = pd.DataFrame({\n        'category': categories,\n        'subcategory': subcategories,\n        'category_abbrev': cat_abbrevs,\n        'subcategory_abbrev': subcat_abbrevs,\n        'combined': [f\"{c}.{s}\" for c, s in zip(cat_abbrevs, subcat_abbrevs)]\n    })\n    \n    return mapping_df\n\n# Parse the response into a structured format\ntry:\n    category_mappings = parse_category_mappings(category_response)\n    print(f\"Parsed {len(category_mappings)} category/subcategory combinations\")\n    \n    # Save the structured mappings\n    category_mappings.to_csv(\"category_mappings.csv\", index=False)\n    print(\"Saved category mappings to CSV\")\n    \n    # Display a sample of the mappings\n    print(\"\\nSample mappings:\")\n    print(category_mappings.head())\n    \nexcept Exception as e:\n    print(f\"Error parsing category mappings: {e}\")\n    print(\"You may need to manually parse the response based on its actual format\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T08:17:44.019951Z","iopub.execute_input":"2025-04-13T08:17:44.020331Z","iopub.status.idle":"2025-04-13T08:17:44.037748Z","shell.execute_reply.started":"2025-04-13T08:17:44.020298Z","shell.execute_reply":"2025-04-13T08:17:44.036561Z"}},"outputs":[{"name":"stdout","text":"Parsed 0 category/subcategory combinations\nSaved category mappings to CSV\n\nSample mappings:\nEmpty DataFrame\nColumns: [category, subcategory, category_abbrev, subcategory_abbrev, combined]\nIndex: []\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"def transform_transaction_data(df, mapping_df):\n    \"\"\"\n    Transform the transaction data using the category mappings\n    \"\"\"\n    # Create a lookup dictionary for faster processing\n    mapping_dict = dict(zip(\n        zip(mapping_df['category'], mapping_df['subcategory']),\n        mapping_df['combined']\n    ))\n    \n    # Function to apply the mapping to each row\n    def get_combined_code(row):\n        key = (row['category'], row['subcategory'])\n        return mapping_dict.get(key, None)\n    \n    # Apply the mapping to create a new column\n    df_transformed = df.copy()\n    df_transformed['category_code'] = df.apply(get_combined_code, axis=1)\n    \n    return df_transformed\n\n# Apply the mappings to your transaction data\ntry:\n    # Check if your DataFrame has both category and subcategory columns\n    if 'category' not in df_train_sampled.columns:\n        # If you only have subcategory, we need to get the parent category\n        print(\"Retrieving parent categories for subcategories...\")\n        \n        # SQL query to get subcategory to category mapping\n        cursor = db_conn.cursor()\n        cursor.execute(\"\"\"\n        SELECT s.name as subcategory, c.name as category \n        FROM subcategories s\n        JOIN categories c ON s.category_id = c.category_id\n        \"\"\")\n        \n        subcat_to_cat = {row[0]: row[1] for row in cursor.fetchall()}\n        \n        # Add category column to DataFrame\n        df_train_sampled['category'] = df_train_sampled['subcategory'].map(subcat_to_cat)\n    \n    # Transform the data\n    df_transformed = transform_transaction_data(df_train_sampled, category_mappings)\n    print(f\"Transformed {len(df_transformed)} transactions with category codes\")\n    \n    # Save the transformed data\n    df_transformed.to_csv(\"transactions_transformed.csv\", index=False)\n    print(\"Saved transformed transactions to CSV\")\n    \n    # Display a sample\n    print(\"\\nSample transformed data:\")\n    print(df_transformed[['note', 'category', 'subcategory', 'category_code']].head())\n    \nexcept Exception as e:\n    print(f\"Error transforming transaction data: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T07:47:22.638343Z","iopub.execute_input":"2025-04-13T07:47:22.638663Z","iopub.status.idle":"2025-04-13T07:47:22.681695Z","shell.execute_reply.started":"2025-04-13T07:47:22.638634Z","shell.execute_reply":"2025-04-13T07:47:22.680641Z"}},"outputs":[{"name":"stdout","text":"Retrieving parent categories for subcategories...\nTransformed 1654 transactions with category codes\nSaved transformed transactions to CSV\n\nSample transformed data:\n                                                note              category  \\\n0  02 DEC 20 - $98.00 LULULEMON ATHLETICA AUSTRAl...  Life & Entertainment   \n1  REBEL MELBOURNE CTRL MELBOURNE VI AUSTap and P...  Life & Entertainment   \n2  STATE TRUSTEES LIMIT MELBOURNE AUSCard xx2819V...    Financial expenses   \n3                         BWS 3071 HWATHORN HWATHORN  Life & Entertainment   \n4                         BWS 3895 BOX HILL BOX HILL  Life & Entertainment   \n\n             subcategory category_code  \n0  Active sport, fitness          None  \n1  Active sport, fitness          None  \n2               Advisory          None  \n3       Alcohol, tobacco          None  \n4       Alcohol, tobacco          None  \n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"def prepare_fine_tuning_data(df_transformed):\n    \"\"\"\n    Prepare data for fine-tuning a model\n    \"\"\"\n    # Create training examples\n    training_examples = []\n    \n    for _, row in df_transformed.iterrows():\n        if pd.notna(row['category_code']):  # Skip rows without mapped codes\n            training_examples.append({\n                \"textInput\": str(row['note']),\n                \"output\": str(row['category_code'])  # Use the abbreviated code as output\n            })\n    \n    print(f\"Created {len(training_examples)} training examples\")\n    \n    # Prepare the dataset in the required format\n    training_data = {\"examples\": training_examples}\n    \n    return training_data\n\n# Prepare the fine-tuning data\ntuning_data = prepare_fine_tuning_data(df_transformed)\n\n# Save the fine-tuning data to a JSON file\nimport json\nwith open(\"fine_tuning_data.json\", \"w\") as f:\n    json.dump(tuning_data, f)\nprint(\"Saved fine-tuning data to JSON file\")\n\n# Display sample examples\nprint(\"\\nSample fine-tuning examples:\")\nfor example in tuning_data[\"examples\"][:3]:\n    print(f\"Input: '{example['textInput'][:50]}...'\")\n    print(f\"Output: '{example['output']}'\")\n    print()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T07:47:22.683039Z","iopub.execute_input":"2025-04-13T07:47:22.683398Z","iopub.status.idle":"2025-04-13T07:47:22.777065Z","shell.execute_reply.started":"2025-04-13T07:47:22.683366Z","shell.execute_reply":"2025-04-13T07:47:22.775843Z"}},"outputs":[{"name":"stdout","text":"Created 0 training examples\nSaved fine-tuning data to JSON file\n\nSample fine-tuning examples:\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"## Instruct the zero-shot prompt\nI draft the prompt asking it to only use the subcategory from the loaded table.","metadata":{}},{"cell_type":"code","source":" import sqlite3\n\ndb_file = \"transaction_categories.db\"\ndb_conn = sqlite3.connect(db_file)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T07:47:22.778432Z","iopub.execute_input":"2025-04-13T07:47:22.779212Z","iopub.status.idle":"2025-04-13T07:47:22.783832Z","shell.execute_reply.started":"2025-04-13T07:47:22.779164Z","shell.execute_reply":"2025-04-13T07:47:22.782849Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"from google.api_core import retry\n\n# Define a system instruction for classification with the subcategories list\nsystem_instruct = \"\"\"\nYou are a financial transaction categorization service. You will be provided with a transaction \ndescription (note) and must classify it into exactly one of the following subcategories:\n\n{}\n\nYour response must be ONLY the exact subcategory name from this list, with no additional text.\nDo not create new categories or modify existing ones.\nIf uncertain, choose the most likely subcategory from the list above.\n\"\"\"\n\n# Get the list of subcategories and format them for the prompt\nall_subcategories = sorted(df['subcategory'].unique())\nsubcategories_text = \"\\n\".join([f\"- {subcat}\" for subcat in all_subcategories])\n\n# Insert the subcategories into the system instruction\nsystem_instruct = system_instruct.format(subcategories_text)\n\n# Define a helper to retry when per-minute quota is reached\nis_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n\n@retry.Retry(predicate=is_retriable)\ndef predict_label(transaction_note: str) -> str:\n    \"\"\"Classify the provided transaction note into a subcategory from the predefined list.\"\"\"\n    response = client.models.generate_content(\n        model=\"gemini-2.0-flash\",\n        config=types.GenerateContentConfig(\n            system_instruction=system_instruct),\n        contents=transaction_note)\n    rc = response.candidates[0]\n    \n    # Any errors, filters, recitation, etc we can mark as a general error\n    if rc.finish_reason.name != \"STOP\":\n        return \"(error)\"\n    else:\n        # Clean up the response\n        prediction = response.text.strip()\n        \n        # Verify the prediction is from our list of subcategories\n        if prediction in all_subcategories:\n            return prediction\n        else:\n            # Find the closest matching subcategory if possible\n            for subcat in all_subcategories:\n                if subcat.lower() in prediction.lower():\n                    return subcat\n            return \"(invalid category)\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T07:47:22.785220Z","iopub.execute_input":"2025-04-13T07:47:22.785508Z","iopub.status.idle":"2025-04-13T07:47:22.801107Z","shell.execute_reply.started":"2025-04-13T07:47:22.785480Z","shell.execute_reply":"2025-04-13T07:47:22.800078Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"## Evaluate baseline performance\n\nNow I perform an evaluation on the available models to ensure I can measure how much the tuning helps.","metadata":{}},{"cell_type":"code","source":"import tqdm\nfrom tqdm.rich import tqdm as tqdmr\nimport warnings\n\n# Enable tqdm features on Pandas\ntqdmr.pandas()\n\n# Suppress the experimental warning\nwarnings.filterwarnings(\"ignore\", category=tqdm.TqdmExperimentalWarning)\n\n# Further sample the test data to be mindful of the free-tier quota\n# Sample a small subset of test data (adjust number as needed)\nTEST_SAMPLE_SIZE = 20\ndf_baseline_eval = df_test_sampled.sample(min(TEST_SAMPLE_SIZE, len(df_test_sampled)))\n\nprint(f\"Evaluating {len(df_baseline_eval)} transactions...\")\n\n# Make predictions using the sampled data with progress bar\ndf_baseline_eval['prediction'] = df_baseline_eval['note'].progress_apply(predict_label)\n\n# Calculate the accuracy\naccuracy = (df_baseline_eval['subcategory'] == df_baseline_eval['prediction']).mean()\nprint(f\"Baseline accuracy: {accuracy:.2%}\")\n\n# Display some examples of predictions\nprint(\"\\nSample predictions:\")\nsample_results = df_baseline_eval[['note', 'subcategory', 'prediction']].sample(min(5, len(df_baseline_eval)))\nfor idx, row in sample_results.iterrows():\n    print(f\"Transaction: {row['note'][:50]}...\")\n    print(f\"True subcategory: {row['subcategory']}\")\n    print(f\"Predicted: {row['prediction']}\")\n    print(f\"Correct: {row['subcategory'] == row['prediction']}\\n\")\n\n# Create a confusion matrix to see where the model is making mistakes\nprint(\"Most common error patterns:\")\nerror_patterns = df_baseline_eval[df_baseline_eval['subcategory'] != df_baseline_eval['prediction']]\nif len(error_patterns) > 0:\n    error_counts = error_patterns.groupby(['subcategory', 'prediction']).size().reset_index(name='count')\n    error_counts = error_counts.sort_values('count', ascending=False)\n    print(error_counts.head(5))\nelse:\n    print(\"No errors found in the evaluation set!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T07:47:22.802444Z","iopub.execute_input":"2025-04-13T07:47:22.802949Z","iopub.status.idle":"2025-04-13T07:47:30.184811Z","shell.execute_reply.started":"2025-04-13T07:47:22.802906Z","shell.execute_reply":"2025-04-13T07:47:30.183830Z"}},"outputs":[{"name":"stdout","text":"Evaluating 20 transactions...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Output()","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4ddaf3b442943b18b00672f7ec08736"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Baseline accuracy: 45.00%\n\nSample predictions:\nTransaction: IKEA RICHMOND RICHMOND VI AUSTap and Pay xx3173Val...\nTrue subcategory: Maintenance, repairs\nPredicted: Home, garden\nCorrect: False\n\nTransaction: 土豆饼...\nTrue subcategory: Restaurant, fast-food\nPredicted: Restaurant, fast-food\nCorrect: True\n\nTransaction: IMAX THEATRE 24JAN20 ATMA896 07:45:01 5055 VISAAUD...\nTrue subcategory: Hobbies\nPredicted: Culture, sport events\nCorrect: False\n\nTransaction: JB hifi Giftcard...\nTrue subcategory: Gifts\nPredicted: Gifts\nCorrect: True\n\nTransaction: AMAZON MKTPLC AU SYDNEY SOUTH ...\nTrue subcategory: Electronics, accessories\nPredicted: Shopping\nCorrect: False\n\nMost common error patterns:\n                  subcategory            prediction  count\n118    Education, development              TRANSFER      1\n127  Electronics, accessories              Shopping      1\n102             Dues & grants     Energy, utilities      1\n335      Maintenance, repairs          Home, garden      1\n236                   Hobbies  Life & Entertainment      1\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_30/3877153812.py:38: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  error_counts = error_patterns.groupby(['subcategory', 'prediction']).size().reset_index(name='count')\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"## Tune my model\nNow I train the model with training data to tune it for assessment and potential use.","metadata":{}},{"cell_type":"markdown","source":"from collections.abc import Iterable\nimport datetime\nimport time\nimport os\n\n# 1. Prepare your transaction data for fine-tuning\nprint(\"Preparing transaction data for fine-tuning...\")\n\n# Convert the DataFrame into the format expected by the API\ntraining_examples = []\nfor _, row in df_train_sampled.iterrows():\n    training_examples.append({\n        \"textInput\": str(row['note']),\n        \"output\": str(row['subcategory'])\n    })\n\nprint(f\"Created {len(training_examples)} training examples\")\nprint(f\"Sample example - Input: '{training_examples[0]['textInput'][:50]}...'\")\nprint(f\"Sample example - Output: '{training_examples[0]['output']}'\")\n\n# 2. Prepare the dataset in the required format\ntraining_data = {\"examples\": training_examples}\n\n# 3. Set up the fine-tuning job - find existing or create new\nmodel_id = None\n\ntry:\n    # Try to read previous model ID from file\n    try:\n        with open(\"tuned_model_id.txt\", \"r\") as f:\n            saved_model_id = f.read().strip()\n            if saved_model_id:\n                print(f\"Found previously saved model ID: {saved_model_id}\")\n                model_id = saved_model_id\n    except FileNotFoundError:\n        print(\"No previously saved model ID found.\")\n    \n    # If no saved ID, check for existing models\n    if not model_id:\n        queued_model = None\n        print(\"Checking for existing tuned models...\")\n        \n        # List models in reverse order (newest first)\n        for m in reversed(client.tunings.list()):\n            # Look for transaction classifier models with flexible matching\n            if (\"transaction\" in m.name.lower() or\n                m.name.startswith('tunedModels/personal-transaction-classifier-')):\n                \n                print(f\"Found potential model: {m.name} in state: {m.state.name}\")\n                \n                # If there is a completed model, use it\n                if m.state.name == 'JOB_STATE_SUCCEEDED':\n                    model_id = m.name\n                    print(f'Found existing completed model to reuse: {model_id}')\n                    break\n                elif m.state.name == 'JOB_STATE_RUNNING' and not queued_model:\n                    # If there's a model still running, remember it\n                    queued_model = m.name\n                    print(f'Found model still in progress: {queued_model}')\n        \n        # Use queued model if found and no completed model\n        if not model_id and queued_model:\n            model_id = queued_model\n            print(f'Using in-progress model: {model_id}')\n    \n    # Create new model if needed\n    if not model_id:\n        print(\"Starting new fine-tuning job...\")\n        tuning_op = client.tunings.tune(\n            base_model=\"models/gemini-1.5-flash-001-tuning\",\n            training_dataset=training_data,\n            config=types.CreateTuningJobConfig(\n                tuned_model_display_name=\"transaction-category-classifier\",  \n                batch_size=16,\n                epoch_count=3,\n            ),\n        )\n        \n        model_id = tuning_op.name\n        print(f\"Fine-tuning initiated. Model ID: {model_id}\")\n        print(f\"Current status: {tuning_op.state}\")\n        \n        # Poll for status updates (optional)\n        print(\"Initial training status:\")\n        print(f\"  - State: {tuning_op.state}\")\n        print(f\"  - Create time: {tuning_op.create_time}\")\n        if hasattr(tuning_op, 'progress') and tuning_op.progress:\n            print(f\"  - Progress: {tuning_op.progress}%\")\n    \n    # Save the model ID for later use\n    with open(\"tuned_model_id.txt\", \"w\") as f:\n        f.write(model_id)\n    \n    print(f\"\\nUsing model: {model_id}\")\n    print(\"This ID has been saved and will be used for predictions\")\n    \nexcept Exception as e:\n    print(f\"Error in fine-tuning process: {e}\")","metadata":{"execution":{"iopub.execute_input":"2025-04-10T05:48:56.388511Z","iopub.status.busy":"2025-04-10T05:48:56.387205Z","iopub.status.idle":"2025-04-10T05:48:58.145557Z","shell.execute_reply":"2025-04-10T05:48:58.144833Z","shell.execute_reply.started":"2025-04-10T05:48:56.388457Z"}}},{"cell_type":"markdown","source":"## Monitoring progress\nHere I monitor whether this model has been tuned and ready to use.","metadata":{}},{"cell_type":"code","source":"# 4. Monitor the fine-tuning progress\nstart_time = datetime.datetime.now(datetime.timezone.utc)\ntuned_model = client.tunings.get(name=model_id)\n\nwhile not tuned_model.has_ended:\n    print(f\"Current state: {tuned_model.state.name}\")\n    if hasattr(tuned_model, 'progress'):\n        print(f\"Progress: {tuned_model.progress}%\")\n    \n    time.sleep(60)  # Check every minute\n    tuned_model = client.tunings.get(name=model_id)\n\nprint(f\"Done! The model state is: {tuned_model.state.name}\")\n\nif not tuned_model.has_succeeded and tuned_model.error:\n    print(\"Error:\", tuned_model.error)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T07:47:30.186350Z","iopub.execute_input":"2025-04-13T07:47:30.187137Z","iopub.status.idle":"2025-04-13T07:47:30.661765Z","shell.execute_reply.started":"2025-04-13T07:47:30.187088Z","shell.execute_reply":"2025-04-13T07:47:30.660318Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[20], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 4. Monitor the fine-tuning progress\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m start_time \u001b[38;5;241m=\u001b[39m \u001b[43mdatetime\u001b[49m\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow(datetime\u001b[38;5;241m.\u001b[39mtimezone\u001b[38;5;241m.\u001b[39mutc)\n\u001b[1;32m      3\u001b[0m tuned_model \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mtunings\u001b[38;5;241m.\u001b[39mget(name\u001b[38;5;241m=\u001b[39mmodel_id)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tuned_model\u001b[38;5;241m.\u001b[39mhas_ended:\n","\u001b[0;31mNameError\u001b[0m: name 'datetime' is not defined"],"ename":"NameError","evalue":"name 'datetime' is not defined","output_type":"error"}],"execution_count":20},{"cell_type":"markdown","source":"## Evaluate Tuned Model\nHere I test and evaluate the performance of the tuned model.","metadata":{}},{"cell_type":"code","source":"# 5. Test the tuned model with a sample transaction\ndef categorize_transaction(transaction_note):\n    \"\"\"Use the fine-tuned model to categorize a transaction.\"\"\"\n    response = client.models.generate_content(\n        model=model_id,\n        contents=transaction_note,\n        config=types.GenerateContentConfig(\n            temperature=0.0,  # Use deterministic output for classification\n            max_output_tokens=10,  # Keep it short, we just need the category\n        )\n    )\n    \n    if response.candidates and response.candidates[0].content:\n        return response.candidates[0].content.parts[0].text.strip()\n    else:\n        return \"(error)\"\n\n# Test with a sample transaction\nsample_transaction = \"AMAZON PRIME MEMBERSHIP ANNUAL RENEWAL\"\npredicted_category = categorize_transaction(sample_transaction)\nprint(f\"Transaction: {sample_transaction}\")\nprint(f\"Predicted category: {predicted_category}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T07:47:30.662702Z","iopub.status.idle":"2025-04-13T07:47:30.663058Z","shell.execute_reply.started":"2025-04-13T07:47:30.662895Z","shell.execute_reply":"2025-04-13T07:47:30.662914Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 6. Evaluate the model on test data\nimport tqdm\nfrom tqdm.rich import tqdm as tqdmr\nimport warnings\n\n# Enable tqdm features on Pandas\ntqdmr.pandas()\n\n# Suppress the experimental warning\nwarnings.filterwarnings(\"ignore\", category=tqdm.TqdmExperimentalWarning)\n\n# Sample a subset of test data for evaluation\nTEST_SAMPLE_SIZE = 20\ndf_eval = df_test_sampled.sample(min(TEST_SAMPLE_SIZE, len(df_test_sampled)))\n\nprint(f\"Evaluating on {len(df_eval)} test transactions...\")\n\n# Make predictions with progress bar\ndf_eval['prediction'] = df_eval['note'].progress_apply(categorize_transaction)\n\n# Calculate accuracy\naccuracy = (df_eval['subcategory'] == df_eval['prediction']).mean()\nprint(f\"Model accuracy: {accuracy:.2%}\")\n\n# Display some examples\nprint(\"\\nSample predictions:\")\nfor idx, row in df_eval.sample(min(5, len(df_eval))).iterrows():\n    print(f\"Transaction: {row['note'][:50]}...\")\n    print(f\"True category: {row['subcategory']}\")\n    print(f\"Predicted: {row['prediction']}\")\n    print(f\"Correct: {row['subcategory'] == row['prediction']}\\n\")\n\n# Show error analysis\nerrors = df_eval[df_eval['subcategory'] != df_eval['prediction']]\nif len(errors) > 0:\n    print(f\"Found {len(errors)} misclassifications\")\n    print(\"Most common error patterns:\")\n    error_matrix = pd.crosstab(\n        errors['subcategory'], \n        errors['prediction'], \n        rownames=['True'], \n        colnames=['Predicted']\n    )\n    print(error_matrix)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T07:47:30.664791Z","iopub.status.idle":"2025-04-13T07:47:30.665170Z","shell.execute_reply.started":"2025-04-13T07:47:30.665002Z","shell.execute_reply":"2025-04-13T07:47:30.665020Z"}},"outputs":[],"execution_count":null}]}