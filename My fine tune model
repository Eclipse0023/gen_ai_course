{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"colab":{"name":"day-4-fine-tuning-a-custom-model.ipynb","toc_visible":true},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Categorise finance transactions\n\nIn life, my financial transactions are often categorised incorrectly in my budgeting app. I decided to find a better solution.\n\nIn this example, I will first try to categorise with an existing Gemini model using a zero-shot prompt and evaluate its performance. Then I will tune a model with the data categorised by me and evaluate its performance.","metadata":{"id":"4KDIFPAL2EnL"}},{"cell_type":"code","source":"# Install required libraries\n!pip uninstall -qqy jupyterlab  # Remove unused conflicting packages\n!pip install -U -q \"google-genai==1.7.0\"","metadata":{"execution":{"iopub.status.busy":"2025-04-13T11:23:30.637376Z","iopub.execute_input":"2025-04-13T11:23:30.638195Z","iopub.status.idle":"2025-04-13T11:23:42.379303Z","shell.execute_reply.started":"2025-04-13T11:23:30.638158Z","shell.execute_reply":"2025-04-13T11:23:42.377932Z"},"id":"9wafTyEH1_xF","trusted":true},"outputs":[],"execution_count":25},{"cell_type":"code","source":"# Import necessary libraries\nfrom google import genai\nfrom google.genai import types\n\ngenai.__version__","metadata":{"id":"T0CBG9xL2PvT","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T11:23:42.381477Z","iopub.execute_input":"2025-04-13T11:23:42.381849Z","iopub.status.idle":"2025-04-13T11:23:42.389075Z","shell.execute_reply.started":"2025-04-13T11:23:42.381812Z","shell.execute_reply":"2025-04-13T11:23:42.388141Z"}},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"'1.7.0'"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"# Set up the Google GenAI client\nfrom kaggle_secrets import UserSecretsClient\n\nGOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n\nclient = genai.Client(api_key=GOOGLE_API_KEY)","metadata":{"id":"VuJPY3GK2SLZ","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T11:23:42.390478Z","iopub.execute_input":"2025-04-13T11:23:42.390809Z","iopub.status.idle":"2025-04-13T11:23:42.535593Z","shell.execute_reply.started":"2025-04-13T11:23:42.390778Z","shell.execute_reply":"2025-04-13T11:23:42.534668Z"}},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":"## Load the subcategory and category table\nIn this step, I load the subcategory and category table.","metadata":{}},{"cell_type":"code","source":"## Load the subcategory and category table\nIn this step, I load the subcategory and category table.","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import sqlite3\n\n# Connect to your database\ndb_conn = sqlite3.connect('/kaggle/working/transaction_categories.db')\ncursor = db_conn.cursor()\n\n# Create the tables\ncursor.execute('''\nCREATE TABLE IF NOT EXISTS categories (\n    category_id INTEGER PRIMARY KEY AUTOINCREMENT,\n    name VARCHAR(100) NOT NULL UNIQUE,\n    description TEXT,\n    display_order INT DEFAULT 100,\n    is_active BOOLEAN DEFAULT 1,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n)\n''')\n\ncursor.execute('''\nCREATE TABLE IF NOT EXISTS subcategories (\n    subcategory_id INTEGER PRIMARY KEY AUTOINCREMENT,\n    category_id INTEGER NOT NULL,\n    name VARCHAR(100) NOT NULL,\n    description TEXT,\n    display_order INT DEFAULT 100,\n    is_active BOOLEAN DEFAULT 1,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    FOREIGN KEY (category_id) REFERENCES categories(category_id)\n)\n''')\n\n\n# Insert main categories\ncategories = [\n    ('Food & Beverages', 'Expenses related to food and drinks', 10),\n    ('Shopping', 'Retail purchases and shopping expenses', 20),\n    ('Housing', 'Home-related expenses including rent and utilities', 30),\n    ('Transportation', 'Public and private transportation costs', 40),\n    ('Vehicle', 'Car and vehicle related expenses', 50),\n    ('Life & Entertainment', 'Leisure activities and entertainment', 60),\n    ('Communication, PC', 'Internet, phone and computer expenses', 70),\n    ('Financial expenses', 'Banking fees, loans, and financial costs', 80),\n    ('Investments', 'Investment-related transactions', 90),\n    ('Income', 'All sources of incoming money', 100),\n    ('Transfer', 'Money transfers between accounts', 110)\n]\n\ncursor.executemany('INSERT OR IGNORE INTO categories (name, description, display_order) VALUES (?, ?, ?)', categories)\n\n# Insert subcategories for Food & Beverages\nfood_subcategories = [\n    (1, 'Bar, cafe, drink, snacks', 10),\n    (1, 'Groceries', 20),\n    (1, 'Restaurant, fast-food', 30)\n]\ncursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', food_subcategories)\n\n# Insert subcategories for Shopping\nshopping_subcategories = [\n    (2, 'Clothes & Footwear', 10),\n    (2, 'Drug-store, chemist', 20),\n    (2, 'Electronics, accessories', 30),\n    (2, 'Gifts, joy', 40),\n    (2, 'Health and beauty', 50),\n    (2, 'Home, garden', 60),\n    (2, 'Jewels, accessories', 70),\n    (2, 'Kids', 80),\n    (2, 'Leisure time', 90),\n    (2, 'Pets, animals', 100),\n    (2, 'Stationery, tools', 110)\n]\ncursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', shopping_subcategories)\n\nhousing_subcategories=[\n    (3, 'Energy, utilities', 10),\n    (3, 'Maintenance, repairs', 20),\n    (3, 'Mortgage', 30),\n    (3, 'Property insurance', 40),\n    (3, 'Rent', 50),\n    (3, 'Services', 60)\n]\ncursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', housing_subcategories)\n\n\ntransportation_subcategories=[\n    (4, 'Business trips', 10),\n    (4, 'Long distance', 20),\n    (4, 'Public transport', 30),\n    (4, 'Taxi', 40)\n]\ncursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', transportation_subcategories)\n\nvehicle_subcategories=[\n    (5, 'Fuel', 10),\n    (5, 'Leasing', 20),\n    (5, 'Parking', 30),\n    (5, 'Rentals', 40),\n    (5, 'Vehicle insurance', 50),\n    (5, 'Vehicle maintenance', 60)\n]\ncursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', vehicle_subcategories)\n\nlife_subcategories=[\n    (6, 'Active sport, fitness', 10),\n    (6, 'Alcohol, tobacco', 20),\n    (6, 'Books, audio, subscriptions', 30),\n    (6, 'Charity, gifts', 40),\n    (6, 'Culture, sport events', 50),\n    (6, 'Education, development', 60),\n    (6, 'Health care, doctor', 70),\n    (6, 'Hobbies', 80),\n    (6, 'Holiday, trips, hotels', 90),\n    (6, 'Life events', 100),\n    (6, 'Lottery, gambling', 110),\n    (6, 'TV, Streaming', 120),\n    (6, 'Wellness, beauty', 130)\n]\ncursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', life_subcategories)\n\ncommunication_subcategories=[\n    (7, 'Internet', 10),\n    (7, 'Postal services', 20),\n    (7, 'Software, apps, games', 30),\n    (7, 'Telephony, mobile phone', 40)\n]\ncursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', communication_subcategories)\n\nfinancial_subcategories=[\n    (8, 'Advisory', 10),\n    (8, 'Charges, Fees', 20),\n    (8, 'Child Support', 30),\n    (8, 'Fines', 40),\n    (8, 'Insurances', 50),\n    (8, 'Loans, interests', 60),\n    (8, 'Taxes', 70)\n]\ncursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', financial_subcategories)\n\ninvestments_subcategories=[\n    (9, 'Collections', 10),\n    (9, 'Financial investments', 20),\n    (9, 'Realty', 30),\n    (9, 'Savings', 40),\n    (9, 'Vehicles, chattels', 50)\n]\ncursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', investments_subcategories)\n\nincome_subcategories=[\n    (10, 'Checks, coupons', 10),\n    (10, 'Child Support', 20),\n    (10, 'Dues & grants', 30),\n    (10, 'Gifts', 40),\n    (10, 'Interests, dividends', 50),\n    (10, 'Lending, renting', 60),\n    (10, 'Lottery earning', 70),\n    (10, 'Refunds (tax, purchase)', 80),\n    (10, 'Rental income', 90),\n    (10, 'Sale', 100),\n    (10, 'Wage, invoices', 110)\n]\ncursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', income_subcategories)\n\ntransfer_subcatgories=[\n    (11, 'Transfer', 10),   \n]\ncursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', transfer_subcatgories)\n\n# Commit the changes\ndb_conn.commit()\nprint(\"Database schema created successfully!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"In this step, I create the mapping beween category and subcategory","metadata":{}},{"cell_type":"code","source":"def setup_database_and_get_hierarchy(output_path=\"/kaggle/working/category_mapping.csv\"):\n    \"\"\"\n    Initialize database, return category hierarchy, and output a simple mapping CSV.\n    \n    Args:\n        output_path: Path to save the mapping CSV\n        \n    Returns:\n        tuple: (db_connection, category_hierarchy_dict, subcategory_to_category_mapping)\n    \"\"\"\n    import sqlite3\n    import pandas as pd\n    \n    print(\"Setting up database and extracting category hierarchy...\")\n    \n    # Create database connection\n    db_conn = sqlite3.connect('/kaggle/working/transaction_categories.db')\n    cursor = db_conn.cursor()\n    \n    # Create tables and populate data if needed (your existing code)\n    # ... (Keep your existing table creation and population code)\n    \n    # Get complete hierarchy in one operation\n    cursor.execute(\"\"\"\n    SELECT \n        c.name as category, \n        s.name as subcategory\n    FROM categories c\n    JOIN subcategories s ON c.category_id = s.category_id\n    ORDER BY c.display_order, s.display_order\n    \"\"\")\n    \n    # Convert query results to DataFrame\n    results = cursor.fetchall()\n    mapping_df = pd.DataFrame(results, columns=['category', 'subcategory'])\n    \n    # Process results into usable format for return values\n    category_hierarchy = {}\n    subcat_to_cat_mapping = {}\n    \n    for category, subcategory in results:\n        # Build hierarchy dictionary\n        if category not in category_hierarchy:\n            category_hierarchy[category] = []\n        category_hierarchy[category].append(subcategory)\n        \n        # Build mapping dictionary\n        subcat_to_cat_mapping[subcategory] = category\n    \n    # Save to CSV file\n    mapping_df.to_csv(output_path, index=False)\n    \n    # Print summary\n    print(f\"\\nCategory-subcategory mapping saved to {output_path}\")\n    print(f\"Found {len(mapping_df['category'].unique())} categories and {len(mapping_df)} subcategories\")\n    print(\"\\nSample of mapping:\")\n    print(mapping_df.head(5))\n    \n    db_conn.commit()\n    \n    return db_conn, category_hierarchy, subcat_to_cat_mapping","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Use the dataset\n\nI have uploaded transaction data categorised by me. Then I group it into training data and test data.","metadata":{"id":"peFm0w_0c1CO"}},{"cell_type":"code","source":"# Load and preprocess transaction data\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Load your data\nfile_path = \"/kaggle/input/training/categorized_transaction.csv\"\ndf = pd.read_csv(file_path)\n\n# Split into train and test sets (80/20 split)\ndf_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n\n# Display the subcategories (labels) in your dataset\nsubcategories = df['subcategory'].unique()\nprint(f\"Number of subcategories: {len(subcategories)}\")\nprint(\"Sample subcategories:\", subcategories[:10])  # Show first 10 subcategories\n\n# Quick look at note examples\nprint(\"\\nSample notes:\")\nfor i, note in enumerate(df['note'].head(3)):\n    print(f\"{i+1}. {note} → {df['subcategory'].iloc[i]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T11:23:42.774910Z","iopub.execute_input":"2025-04-13T11:23:42.775215Z","iopub.status.idle":"2025-04-13T11:23:42.813241Z","shell.execute_reply.started":"2025-04-13T11:23:42.775186Z","shell.execute_reply":"2025-04-13T11:23:42.812283Z"}},"outputs":[{"name":"stdout","text":"Number of subcategories: 65\nSample subcategories: ['Active sport, fitness' 'Advisory' 'Alcohol, tobacco'\n 'Bar, cafe, drink, snacks' 'Books, audio, subscriptions' 'Charges, Fees'\n 'Charity, gifts' 'Checks, coupons' 'Clothes & shoes'\n 'Culture, sport events']\n\nSample notes:\n1. AMAZON AUSYDNEY SOUTH CREDIT CARD PURCHASEAmazon Basics High-Density Round Foam Roller for Exercise and Recovery - 61cm, Blue Speckled → Active sport, fitness\n2. 02 DEC 20 - $98.00 LULULEMON ATHLETICA AUSTRAlbert Park [Eff Date: 30 NOV 20] → Active sport, fitness\n3. REBEL MELBOURNE CTRL MELBOURNE VI AUSTap and Pay xx3173Value Date_ 17/03/2018 → Active sport, fitness\n","output_type":"stream"}],"execution_count":29},{"cell_type":"markdown","source":"## Clean the data","metadata":{}},{"cell_type":"code","source":"import re\nimport pandas as pd\n\ndef preprocess_transaction_note(note):\n    \"\"\"\n    Optimized version that combines regex patterns and reduces redundant operations\n    \"\"\"\n    # Handle None or empty strings\n    if note is None or pd.isna(note) or note == \"\":\n        return \"\"\n    \n    # Convert to string if needed\n    text = str(note)\n    \n    # Replace non-ASCII characters\n    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n    \n    # Extract main part of the transaction (before common transaction markers)\n    transaction_markers = r'\\s+(?:CREDIT CARD PURCHASE|EFTPOS|Value Date|tap and Pay|Card Purchase|CARD PURCHASE)'\n    parts = re.split(transaction_markers, text, flags=re.IGNORECASE)\n    main_text = parts[0] if parts else text\n    \n    # Combined regex for cleaning amounts with various currency symbols\n    main_text = re.sub(r'(?:[$€£¥]|AUD|USD|EUR|GBP|NZD)\\s*[-+]?(?:\\d{1,3}(?:,\\d{3})*|\\d+)(?:\\.\\d{1,2})?', '', main_text)\n    main_text = re.sub(r'\\b[-+]?(?:\\d{1,3}(?:,\\d{3})*|\\d+)(?:\\.\\d{1,2})?\\b', '', main_text)\n    \n    # Remove card numbers (masked or full)\n    main_text = re.sub(r'(?:x{2,4}|X{2,4})\\d{4}|\\\\b\\d{4}[\\s-]?\\d{4}[\\s-]?\\d{4}[\\s-]?\\d{4}\\\\b', '', main_text)\n    \n    # Consolidated date pattern (combining multiple patterns)\n    date_pattern = r'(?:\\d{1,2}\\s+(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\\s+\\d{2,4}|' + \\\n                   r'(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\\s+\\d{1,2}(?:st|nd|rd|th)?\\s+\\d{2,4}|' + \\\n                   r'\\d{1,2}[-/.]\\d{1,2}[-/.]\\d{2,4}|' + \\\n                   r'\\d{4}[-/.]\\d{1,2}[-/.]\\d{1,2}|' + \\\n                   r'\\[Eff\\s+Date:.*?\\]|' + \\\n                   r'Value\\s+Date[_:]\\s*\\d{1,2}[-/.]\\d{1,2}[-/.]\\d{2,4})'\n    main_text = re.sub(date_pattern, '', main_text, flags=re.IGNORECASE)\n    \n    # Remove common transaction terminology in one pass\n    terms_to_remove = [\n        r'Electronic Transfer', r'Direct Debit', r'PURCHASE', r'Purchase', \n        r'PAYMENT', r'Payment', r'MONTHLY', r'RENEWAL', r'Renewal', \n        r'SUBSCRIPTION', r'Subscription', r'CREDIT CARD', r'Credit Card',\n        r'DEBIT CARD', r'Debit Card', r'ONLINE', r'Online', r'Value Date',\n        r'EFTPOS', r'ATM WITHDRAWAL', r'ATM Withdrawal', r'VISA[A-Z0-9]*',\n        r'MASTERCARD[A-Z0-9]*', r'AMEX[A-Z0-9]*', r'\\b(?:AUS|USA|CAN|GBR|NZL)\\b\\s*$',\n        r'\\b[A-Z0-9]{5,}\\b', r'Ref(?:erence)?(?:No)?:?\\s*[A-Z0-9]+',\n        r'(?:Transaction|ID|No|#)\\s*:?\\s*[A-Z0-9-]+', r'#\\d+', r'[\\w\\.-]+@[\\w\\.-]+'\n    ]\n    \n    for term in terms_to_remove:\n        main_text = re.sub(term, '', main_text, flags=re.IGNORECASE)\n    \n    # Clean whitespace and punctuation in one pass\n    main_text = re.sub(r'\\s+', ' ', main_text)\n    main_text = re.sub(r'[\\s,.-]+$|^[\\s,.-]+', '', main_text)\n    main_text = re.sub(r'\\s+([,.])', r'\\1', main_text)\n    \n    return main_text.strip()[:500]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T11:23:42.814539Z","iopub.execute_input":"2025-04-13T11:23:42.814847Z","iopub.status.idle":"2025-04-13T11:23:42.832410Z","shell.execute_reply.started":"2025-04-13T11:23:42.814818Z","shell.execute_reply":"2025-04-13T11:23:42.831339Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"def transform_categories(df):\n    \"\"\"\n    Transform category and subcategory fields with improved error handling\n    \"\"\"\n    # Deep copy to avoid modifying original\n    result_df = df.copy()\n    \n    if 'category' not in result_df.columns:\n        try:\n            # First try to get categories from database\n            import sqlite3\n            conn = sqlite3.connect('/kaggle/working/transaction_categories.db')\n            cursor = conn.cursor()\n            cursor.execute(\"\"\"\n            SELECT s.name as subcategory, c.name as category \n            FROM subcategories s\n            JOIN categories c ON s.category_id = c.category_id\n            \"\"\")\n            \n            subcat_to_cat = {row[0]: row[1] for row in cursor.fetchall()}\n            conn.close()\n            \n            # Add category column with mapping\n            result_df['category'] = result_df['subcategory'].map(subcat_to_cat)\n            \n            # Check for unmapped categories\n            missing_count = result_df['category'].isna().sum()\n            if missing_count > 0:\n                print(f\"Warning: {missing_count} rows have unmapped subcategories\")\n                # Show the unmapped subcategories\n                unmapped = result_df[result_df['category'].isna()]['subcategory'].unique()\n                print(f\"Unmapped subcategories: {unmapped}\")\n                # Fill missing with placeholder\n                result_df['category'] = result_df['category'].fillna(\"Unknown\")\n                \n            print(f\"Added categories to {len(result_df)} transactions\")\n            \n        except Exception as e:\n            print(f\"Error getting category mapping: {e}\")\n            # Create a placeholder category column\n            result_df['category'] = \"Unknown\"\n    \n    # Create a combined category code\n    result_df['category_code'] = result_df['category'] + \" - \" + result_df['subcategory']\n    \n    return result_df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def process_transactions_for_fine_tuning(df, output_filepath=None):\n    \"\"\"\n    Clean and transform transaction data for fine-tuning.\n    \n    Args:\n        df: DataFrame containing transaction data with 'note' and 'subcategory' columns\n        output_filepath: Optional path to save the processed CSV\n        \n    Returns:\n        Processed DataFrame with cleaned transaction notes and category mappings\n    \"\"\"\n    # Make a copy to avoid modifying the original\n    processed_df = df.copy()\n    \n    # Step 1: Clean transaction notes first - show progress\n    print(\"Cleaning transaction notes...\")\n    tqdm.pandas(desc=\"Processing transactions\")\n    processed_df['cleaned_note'] = processed_df['note'].progress_apply(preprocess_transaction_note)\n    \n    # Check for empty values after cleaning\n    empty_notes = processed_df['cleaned_note'].apply(lambda x: x is None or pd.isna(x) or x == \"\").sum()\n    if empty_notes > 0:\n        print(f\"Warning: {empty_notes} transactions have empty notes after cleaning\")\n        # Fill empty notes with a placeholder rather than leaving them empty\n        processed_df['cleaned_note'] = processed_df['cleaned_note'].replace('', 'Unknown transaction')\n    \n    # Step 2: Then perform category/subcategory transformations\n    print(\"Transforming categories...\")\n    \n    # Check for valid subcategories before transformation\n    # Get list of valid subcategories from database\n    try:\n        import sqlite3\n        conn = sqlite3.connect('/kaggle/working/transaction_categories.db')\n        cursor = conn.cursor()\n        cursor.execute(\"SELECT name FROM subcategories\")\n        valid_subcategories = set(row[0] for row in cursor.fetchall())\n        conn.close()\n        \n        # Count transactions before filtering\n        total_before = len(processed_df)\n        \n        # Filter out transactions with invalid subcategories\n        invalid_mask = ~processed_df['subcategory'].isin(valid_subcategories)\n        invalid_count = invalid_mask.sum()\n        \n        if invalid_count > 0:\n            print(f\"Removing {invalid_count} transactions with invalid subcategories\")\n            print(\"Invalid subcategories found:\")\n            print(processed_df.loc[invalid_mask, 'subcategory'].value_counts().head(10))\n            \n            # Remove transactions with invalid subcategories\n            processed_df = processed_df[~invalid_mask].reset_index(drop=True)\n            print(f\"Kept {len(processed_df)} of {total_before} transactions\")\n    except Exception as e:\n        print(f\"Warning: Could not validate subcategories: {e}\")\n    \n    # Continue with category transformation\n    processed_df = transform_categories(processed_df)\n    \n    # Step 3: Final validation to ensure no empty fields\n    for col in ['cleaned_note', 'category', 'subcategory', 'category_code']:\n        if col in processed_df.columns:\n            missing = processed_df[col].isna().sum()\n            if missing > 0:\n                print(f\"Warning: {missing} rows have missing values in '{col}' column\")\n                # Fill missing values appropriately based on column type\n                if col == 'cleaned_note':\n                    processed_df[col] = processed_df[col].fillna('Unknown transaction')\n                else:\n                    processed_df[col] = processed_df[col].fillna('Uncategorized')\n    \n    # Save processed data if requested\n    if output_filepath:\n        try:\n            processed_df.to_csv(output_filepath, index=False, encoding='utf-8')\n            print(f\"Successfully saved processed data to {output_filepath}\")\n        except Exception as e:\n            print(f\"Error saving CSV: {e}\")\n    \n    return processed_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T11:23:42.834042Z","iopub.execute_input":"2025-04-13T11:23:42.834441Z","iopub.status.idle":"2025-04-13T11:23:42.850815Z","shell.execute_reply.started":"2025-04-13T11:23:42.834409Z","shell.execute_reply":"2025-04-13T11:23:42.849852Z"}},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":"## Sample the dataset\nNow sample the data. I will keep 50 rows for each subcategory for training.","metadata":{"id":"03lDs1O4ZQ0-"}},{"cell_type":"code","source":"def sample_data(df, num_samples):\n    \"\"\"\n    Sample rows from each subcategory, selecting num_samples from each.\n    If a subcategory has fewer than num_samples entries, takes all available rows.\n    \n    Args:\n        df: DataFrame containing transaction data\n        num_samples: Number of samples to take per subcategory\n        \n    Returns:\n        DataFrame with balanced samples across subcategories\n    \"\"\"\n    # Group by subcategory and sample\n    sampled_df = (\n        df.groupby(\"subcategory\")[df.columns]\n        .apply(lambda x: x.sample(min(len(x), num_samples)))\n        .reset_index(drop=True)\n    )\n    \n    # Convert subcategory to category type for efficiency\n    sampled_df[\"subcategory\"] = sampled_df[\"subcategory\"].astype(\"category\")\n    \n    return sampled_df\n\n# Sample training and test data\nTRAIN_NUM_SAMPLES = 50  # 50 samples per subcategory for training\nTEST_NUM_SAMPLES = 10   # 10 samples per subcategory for testing\n\n# Create balanced datasets\ndf_train_sampled = sample_data(df_train, TRAIN_NUM_SAMPLES)\ndf_test_sampled = sample_data(df_test, TEST_NUM_SAMPLES)\n\n# Print statistics about the sampled data\nprint(f\"Original training data: {len(df_train)} rows\")\nprint(f\"Sampled training data: {len(df_train_sampled)} rows\")\nprint(f\"Number of subcategories: {df_train_sampled['subcategory'].nunique()}\")\n\n# Show distribution of a few subcategories\nprint(\"\\nSample of subcategory counts in training data:\")\nprint(df_train_sampled['subcategory'].value_counts().head(5))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T11:23:42.852165Z","iopub.execute_input":"2025-04-13T11:23:42.852575Z","iopub.status.idle":"2025-04-13T11:23:42.899709Z","shell.execute_reply.started":"2025-04-13T11:23:42.852530Z","shell.execute_reply":"2025-04-13T11:23:42.898613Z"}},"outputs":[{"name":"stdout","text":"Original training data: 9936 rows\nSampled training data: 1654 rows\nNumber of subcategories: 63\n\nSample of subcategory counts in training data:\nsubcategory\nHobbies                  50\nInterests, dividends     50\nHome, garden             50\nTRANSFER                 50\nSoftware, apps, games    50\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":32},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"try:\n    print(\"Retrieving category-subcategory combinations from database...\")\n    \n    # Direct database query approach - more reliable than AI-generated SQL\n    cursor = db_conn.cursor()\n    cursor.execute(\"\"\"\n    SELECT \n        c.name as category, \n        s.name as subcategory\n    FROM categories c\n    JOIN subcategories s ON c.category_id = s.category_id\n    ORDER BY c.name, s.name\n    \"\"\")\n    \n    # Get the combinations directly from database\n    combinations = cursor.fetchall()\n    \n    # Create DataFrame with the combinations\n    columns = [\"category\", \"subcategory\"]\n    combinations_df = pd.DataFrame(combinations, columns=columns)\n    \n    # Save as CSV\n    combinations_df.to_csv(\"category_subcategory_combinations.csv\", index=False)\n    \n    print(f\"Successfully retrieved and saved {len(combinations_df)} category-subcategory combinations\")\n    \n    # Display preview\n    print(\"\\nFirst 5 combinations:\")\n    print(combinations_df.head())\n    \n    # Adding categories to training data if needed\n    if 'category' not in df_train_sampled.columns:\n        print(\"\\nAdding category column to training data...\")\n        \n        # Create a lookup dictionary for mapping subcategories to categories\n        subcat_to_cat = dict(zip(combinations_df['subcategory'], combinations_df['category']))\n        \n        # Add category column to DataFrame\n        df_train_sampled['category'] = df_train_sampled['subcategory'].map(subcat_to_cat)\n        \n        print(f\"Added categories to {len(df_train_sampled)} training examples\")\n        \n        # Display sample\n        print(\"\\nSample of training data with categories:\")\n        print(df_train_sampled[['note', 'category', 'subcategory']].head())\n    \nexcept Exception as e:\n    print(f\"Error retrieving category-subcategory combinations: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T11:32:45.113436Z","iopub.execute_input":"2025-04-13T11:32:45.113833Z","iopub.status.idle":"2025-04-13T11:32:45.127181Z","shell.execute_reply.started":"2025-04-13T11:32:45.113798Z","shell.execute_reply":"2025-04-13T11:32:45.125979Z"}},"outputs":[{"name":"stdout","text":"Retrieving category-subcategory combinations from database...\nSuccessfully retrieved and saved 142 category-subcategory combinations\n\nFirst 5 combinations:\n            category            subcategory\n0  Communication, PC               Internet\n1  Communication, PC               Internet\n2  Communication, PC        Postal services\n3  Communication, PC        Postal services\n4  Communication, PC  Software, apps, games\n","output_type":"stream"}],"execution_count":50},{"cell_type":"code","source":"def add_category_column_to_training_data(df):\n    \"\"\"\n    Add the parent category column to the training data based on subcategory mappings\n    from the SQLite database.\n    \n    Args:\n        df: DataFrame containing transaction data with 'subcategory' column\n    \n    Returns:\n        DataFrame with added 'category' column\n    \"\"\"\n    if 'category' in df.columns:\n        print(\"Category column already exists in the DataFrame\")\n        return df\n    \n    try:\n        # Connect to the database\n        cursor = db_conn.cursor()\n        \n        # Get subcategory to category mapping\n        cursor.execute(\"\"\"\n        SELECT s.name as subcategory, c.name as category \n        FROM subcategories s\n        JOIN categories c ON s.category_id = c.category_id\n        \"\"\")\n        \n        # Create mapping dictionary\n        subcat_to_cat = {row[0]: row[1] for row in cursor.fetchall()}\n        \n        # Add category column\n        df_with_category = df.copy()\n        df_with_category['category'] = df['subcategory'].map(subcat_to_cat)\n        \n        # Check for any missing mappings\n        missing_count = df_with_category['category'].isna().sum()\n        if missing_count > 0:\n            print(f\"Warning: {missing_count} subcategories could not be mapped to a category\")\n            print(\"Unmapped subcategories:\")\n            unmapped = df[df['subcategory'].map(lambda x: x not in subcat_to_cat)]['subcategory'].unique()\n            print(unmapped)\n        \n        # Fill any missing values\n        df_with_category['category'] = df_with_category['category'].fillna(\"Uncategorized\")\n        \n        print(f\"Successfully added category column to {len(df_with_category)} rows\")\n        return df_with_category\n        \n    except Exception as e:\n        print(f\"Error adding category column: {e}\")\n        # Return original dataframe if there was an error\n        return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T11:23:43.024454Z","iopub.execute_input":"2025-04-13T11:23:43.024847Z","iopub.status.idle":"2025-04-13T11:23:43.034344Z","shell.execute_reply.started":"2025-04-13T11:23:43.024801Z","shell.execute_reply":"2025-04-13T11:23:43.033174Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"def prepare_fine_tuning_data(df):\n    \"\"\"\n    Prepare data for fine-tuning a model\n    \"\"\"\n    # Create training examples\n    training_examples = []\n    \n    for _, row in df.iterrows():\n        # Add this transaction to training examples\n        training_examples.append({\n            \"textInput\": str(row['note']),\n            \"output\": str(row['subcategory'])\n        })\n    \n    print(f\"Created {len(training_examples)} training examples\")\n    \n    # Prepare the dataset in the required format\n    training_data = {\"examples\": training_examples}\n    \n    return training_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T11:24:54.433815Z","iopub.execute_input":"2025-04-13T11:24:54.434689Z","iopub.status.idle":"2025-04-13T11:24:54.440120Z","shell.execute_reply.started":"2025-04-13T11:24:54.434650Z","shell.execute_reply":"2025-04-13T11:24:54.439160Z"}},"outputs":[],"execution_count":46},{"cell_type":"markdown","source":"## Instruct the zero-shot prompt\nI draft the prompt asking it to only use the subcategory from the loaded table.","metadata":{}},{"cell_type":"code","source":" import sqlite3\n\ndb_file = \"transaction_categories.db\"\ndb_conn = sqlite3.connect(db_file)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T11:23:43.051306Z","iopub.execute_input":"2025-04-13T11:23:43.051641Z","iopub.status.idle":"2025-04-13T11:23:43.065547Z","shell.execute_reply.started":"2025-04-13T11:23:43.051598Z","shell.execute_reply":"2025-04-13T11:23:43.064479Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"from google.api_core import retry\n\n# Define retry logic for API rate limits\nis_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n\n@retry.Retry(predicate=is_retriable)\ndef predict_transaction_category(transaction_note, model=\"gemini-2.0-flash\", use_system_instruction=True):\n    \"\"\"\n    Unified function to predict transaction category using either system instruction or fine-tuned model\n    \n    Args:\n        transaction_note: The transaction text to classify\n        model: Model to use (either base model or fine-tuned model ID)\n        use_system_instruction: Whether to use system instruction (for base model) or not (for fine-tuned model)\n    \n    Returns:\n        Dictionary with category and subcategory predictions\n    \"\"\"\n    try:\n        if use_system_instruction:\n            # Get the category hierarchy for system instruction\n            categories, category_hierarchy = get_categorization_hierarchy()\n            \n            # Format the instruction\n            formatted_hierarchy = \"\\n\".join([\n                f\"CATEGORY: {category_name}\\n\" + \n                \"\\n\".join([f\"  - {subcategory}\" for subcategory in subcategories]) + \"\\n\"\n                for category_name, subcategories in category_hierarchy.items()\n            ])\n            \n            system_instruct = f\"\"\"\n            You are a financial transaction categorization service. You will be provided with a transaction \n            description (note) and must classify it following a two-step process:\n\n            1. First, select the most appropriate CATEGORY from the list below.\n            2. Then, select the most specific SUBCATEGORY under that category.\n\n            Here is the complete categorization hierarchy:\n\n            {formatted_hierarchy}\n\n            Your response must be in this exact format:\n            CATEGORY: [selected category name]\n            SUBCATEGORY: [selected subcategory name]\n            \"\"\"\n            \n            # Make prediction with system instruction\n            response = client.models.generate_content(\n                model=model,\n                config=types.GenerateContentConfig(\n                    system_instruction=system_instruct,\n                    temperature=0.0\n                ),\n                contents=transaction_note\n            )\n        else:\n            # Use fine-tuned model without system instruction\n            response = client.models.generate_content(\n                model=model,\n                config=types.GenerateContentConfig(\n                    temperature=0.0,\n                    max_output_tokens=10\n                ),\n                contents=transaction_note\n            )\n        \n        # Process the response\n        if use_system_instruction:\n            text = response.text.strip()\n            \n            try:\n                # Extract category and subcategory\n                category_line = [line for line in text.split('\\n') if line.startswith(\"CATEGORY:\")][0]\n                subcategory_line = [line for line in text.split('\\n') if line.startswith(\"SUBCATEGORY:\")][0]\n                \n                category = category_line.replace(\"CATEGORY:\", \"\").strip()\n                subcategory = subcategory_line.replace(\"SUBCATEGORY:\", \"\").strip()\n                \n                return {\n                    \"category\": category,\n                    \"subcategory\": subcategory,\n                    \"error\": None\n                }\n            except (IndexError, KeyError) as e:\n                return {\n                    \"category\": \"(parsing error)\",\n                    \"subcategory\": \"(parsing error)\",\n                    \"error\": str(e)\n                }\n        else:\n            # Direct output from fine-tuned model\n            return {\n                \"category\": None,  # For fine-tuned model we only get subcategory\n                \"subcategory\": response.text.strip(),\n                \"error\": None\n            }\n            \n    except Exception as e:\n        return {\n            \"category\": \"(error)\",\n            \"subcategory\": \"(error)\",\n            \"error\": str(e)\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T11:26:56.002912Z","iopub.execute_input":"2025-04-13T11:26:56.003327Z","iopub.status.idle":"2025-04-13T11:26:56.020582Z","shell.execute_reply.started":"2025-04-13T11:26:56.003289Z","shell.execute_reply":"2025-04-13T11:26:56.019569Z"}},"outputs":[],"execution_count":47},{"cell_type":"markdown","source":"## Evaluate baseline performance\n\nNow I perform an evaluation on the available models to ensure I can measure how much the tuning helps.","metadata":{}},{"cell_type":"code","source":"import tqdm\nfrom tqdm.rich import tqdm as tqdmr\nimport warnings\n\n# Enable tqdm features on Pandas\ntqdmr.pandas()\n\n# Suppress the experimental warning\nwarnings.filterwarnings(\"ignore\", category=tqdm.TqdmExperimentalWarning)\n\n# Further sample the test data to be mindful of the free-tier quota\nTEST_SAMPLE_SIZE = 20\ndf_baseline_eval = df_test_sampled.sample(min(TEST_SAMPLE_SIZE, len(df_test_sampled)))\n\n# Ensure category column exists in test data\nif 'category' not in df_baseline_eval.columns:\n    # Add categories using the database mapping\n    cursor = db_conn.cursor()\n    cursor.execute(\"\"\"\n    SELECT s.name as subcategory, c.name as category \n    FROM subcategories s\n    JOIN categories c ON s.category_id = c.category_id\n    \"\"\")\n    subcat_to_cat = {row[0]: row[1] for row in cursor.fetchall()}\n    df_baseline_eval['category'] = df_baseline_eval['subcategory'].map(subcat_to_cat)\n\nprint(f\"Evaluating {len(df_baseline_eval)} transactions...\")\n\n# Make predictions using the sampled data with progress bar\n# This will return both category and subcategory\ndf_baseline_eval[['predicted_category', 'predicted_subcategory']] = df_baseline_eval['note'].progress_apply(\n    lambda x: pd.Series(predict_category_and_subcategory(x))\n)\n\n# Calculate the accuracy for both category and subcategory\ncategory_accuracy = (df_baseline_eval['category'] == df_baseline_eval['predicted_category']).mean()\nsubcategory_accuracy = (df_baseline_eval['subcategory'] == df_baseline_eval['predicted_subcategory']).mean()\ncombined_accuracy = ((df_baseline_eval['category'] == df_baseline_eval['predicted_category']) & \n                     (df_baseline_eval['subcategory'] == df_baseline_eval['predicted_subcategory'])).mean()\n\nprint(f\"Category accuracy: {category_accuracy:.2%}\")\nprint(f\"Subcategory accuracy: {subcategory_accuracy:.2%}\")\nprint(f\"Combined accuracy (both correct): {combined_accuracy:.2%}\")\n\n# Display some examples of predictions\nprint(\"\\nSample predictions:\")\nsample_results = df_baseline_eval[['note', 'category', 'subcategory', \n                                  'predicted_category', 'predicted_subcategory']].sample(min(5, len(df_baseline_eval)))\n\nfor idx, row in sample_results.iterrows():\n    print(f\"Transaction: {row['note'][:50]}...\")\n    print(f\"True category: {row['category']}\")\n    print(f\"Predicted category: {row['predicted_category']}\")\n    print(f\"Category correct: {row['category'] == row['predicted_category']}\")\n    print(f\"True subcategory: {row['subcategory']}\")\n    print(f\"Predicted subcategory: {row['predicted_subcategory']}\")\n    print(f\"Subcategory correct: {row['subcategory'] == row['predicted_subcategory']}\\n\")\n\n# Create a confusion matrix for categories\nprint(\"Category confusion matrix:\")\ncat_matrix = pd.crosstab(\n    df_baseline_eval['category'], \n    df_baseline_eval['predicted_category'],\n    rownames=['True'], \n    colnames=['Predicted']\n)\nprint(cat_matrix)\n\n# Create a confusion matrix for subcategories with errors\nprint(\"\\nMost common subcategory error patterns:\")\nerror_patterns = df_baseline_eval[df_baseline_eval['subcategory'] != df_baseline_eval['predicted_subcategory']]\nif len(error_patterns) > 0:\n    error_counts = error_patterns.groupby(['subcategory', 'predicted_subcategory']).size().reset_index(name='count')\n    error_counts = error_counts.sort_values('count', ascending=False)\n    print(error_counts.head(5))\nelse:\n    print(\"No subcategory errors found in the evaluation set!\")\n\n# Analysis of hierarchical errors\nprint(\"\\nError analysis by hierarchy:\")\nhierarchical_errors = df_baseline_eval[\n    (df_baseline_eval['category'] == df_baseline_eval['predicted_category']) & \n    (df_baseline_eval['subcategory'] != df_baseline_eval['predicted_subcategory'])\n]\nprint(f\"Correct category but wrong subcategory: {len(hierarchical_errors)} cases ({len(hierarchical_errors)/len(df_baseline_eval):.2%})\")\n\ncategory_errors = df_baseline_eval[df_baseline_eval['category'] != df_baseline_eval['predicted_category']]\nprint(f\"Wrong category: {len(category_errors)} cases ({len(category_errors)/len(df_baseline_eval):.2%})\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T11:29:45.771359Z","iopub.execute_input":"2025-04-13T11:29:45.771796Z","iopub.status.idle":"2025-04-13T11:29:54.159593Z","shell.execute_reply.started":"2025-04-13T11:29:45.771760Z","shell.execute_reply":"2025-04-13T11:29:54.158670Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output()","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a92f1e2da634b72914e0491d6d028bb"}},"metadata":{}},{"name":"stdout","text":"Evaluating 20 transactions...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Category accuracy: 60.00%\nSubcategory accuracy: 45.00%\nCombined accuracy (both correct): 45.00%\n\nSample predictions:\nTransaction: Direct Debit Aussie Broadband - 9XX1427cnXXX3459...\nTrue category: Communication, PC\nPredicted category: Communication, PC\nCategory correct: True\nTrue subcategory: Internet\nPredicted subcategory: Internet\nSubcategory correct: True\n\nTransaction: Purchase ALDIMOBILE CHATSWOOD...\nTrue category: nan\nPredicted category: Communication, PC\nCategory correct: False\nTrue subcategory: Phone, cell phone\nPredicted subcategory: Telephony, mobile phone\nSubcategory correct: False\n\nTransaction: GLOBIRD ENERGY RINGWOOD...\nTrue category: Housing\nPredicted category: Vehicle\nCategory correct: False\nTrue subcategory: Energy, utilities\nPredicted subcategory: Fuel\nSubcategory correct: False\n\nTransaction: AMAZON MARKETPLACE AU SYDNEY SOUTHSegbeauty Travel...\nTrue category: Shopping\nPredicted category: Shopping\nCategory correct: True\nTrue subcategory: Health and beauty\nPredicted subcategory: Health and beauty\nSubcategory correct: True\n\nTransaction: 日本ITO/艾特柔加厚...\nTrue category: nan\nPredicted category: (invalid category)\nCategory correct: False\nTrue subcategory: Shopping\nPredicted subcategory: (invalid subcategory)\nSubcategory correct: False\n\nCategory confusion matrix:\nPredicted             (invalid category)  Communication, PC  \\\nTrue                                                          \nCommunication, PC                      0                  1   \nFinancial expenses                     0                  0   \nFood & Beverages                       0                  0   \nHousing                                0                  0   \nIncome                                 0                  0   \nInvestments                            0                  0   \nLife & Entertainment                   1                  0   \nShopping                               0                  0   \n\nPredicted             Financial expenses  Food & Beverages  Housing  \\\nTrue                                                                  \nCommunication, PC                      0                 0        0   \nFinancial expenses                     2                 0        0   \nFood & Beverages                       0                 2        0   \nHousing                                0                 0        2   \nIncome                                 1                 0        0   \nInvestments                            0                 0        0   \nLife & Entertainment                   0                 0        0   \nShopping                               0                 1        0   \n\nPredicted             Investments  Shopping  Vehicle  \nTrue                                                  \nCommunication, PC               0         0        0  \nFinancial expenses              0         0        0  \nFood & Beverages                0         0        0  \nHousing                         0         0        1  \nIncome                          0         0        0  \nInvestments                     1         0        0  \nLife & Entertainment            0         1        0  \nShopping                        0         4        0  \n\nMost common subcategory error patterns:\n                  subcategory    predicted_subcategory  count\n24   Bar, cafe, drink, snacks                Groceries      2\n265         Phone, cell phone  Telephony, mobile phone      2\n325         Stationery, tools                Groceries      1\n194      Interests, dividends         Loans, interests      1\n308                  Shopping    (invalid subcategory)      1\n\nError analysis by hierarchy:\nCorrect category but wrong subcategory: 3 cases (15.00%)\nWrong category: 8 cases (40.00%)\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_30/3292486879.py:73: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  error_counts = error_patterns.groupby(['subcategory', 'predicted_subcategory']).size().reset_index(name='count')\n","output_type":"stream"}],"execution_count":48},{"cell_type":"markdown","source":"## Tune my model\nNow I train the model with training data to tune it for assessment and potential use.","metadata":{}},{"cell_type":"markdown","source":"from collections.abc import Iterable\nimport datetime\nimport time\nimport os\n\n# 1. Prepare your transaction data for fine-tuning\nprint(\"Preparing transaction data for fine-tuning...\")\n\n# Convert the DataFrame into the format expected by the API\ntraining_examples = []\nfor _, row in df_train_sampled.iterrows():\n    training_examples.append({\n        \"textInput\": str(row['note']),\n        \"output\": str(row['subcategory'])\n    })\n\nprint(f\"Created {len(training_examples)} training examples\")\nprint(f\"Sample example - Input: '{training_examples[0]['textInput'][:50]}...'\")\nprint(f\"Sample example - Output: '{training_examples[0]['output']}'\")\n\n# 2. Prepare the dataset in the required format\ntraining_data = {\"examples\": training_examples}\n\n# 3. Set up the fine-tuning job - find existing or create new\nmodel_id = None\n\ntry:\n    # Try to read previous model ID from file\n    try:\n        with open(\"tuned_model_id.txt\", \"r\") as f:\n            saved_model_id = f.read().strip()\n            if saved_model_id:\n                print(f\"Found previously saved model ID: {saved_model_id}\")\n                model_id = saved_model_id\n    except FileNotFoundError:\n        print(\"No previously saved model ID found.\")\n    \n    # If no saved ID, check for existing models\n    if not model_id:\n        queued_model = None\n        print(\"Checking for existing tuned models...\")\n        \n        # List models in reverse order (newest first)\n        for m in reversed(client.tunings.list()):\n            # Look for transaction classifier models with flexible matching\n            if (\"transaction\" in m.name.lower() or\n                m.name.startswith('tunedModels/personal-transaction-classifier-')):\n                \n                print(f\"Found potential model: {m.name} in state: {m.state.name}\")\n                \n                # If there is a completed model, use it\n                if m.state.name == 'JOB_STATE_SUCCEEDED':\n                    model_id = m.name\n                    print(f'Found existing completed model to reuse: {model_id}')\n                    break\n                elif m.state.name == 'JOB_STATE_RUNNING' and not queued_model:\n                    # If there's a model still running, remember it\n                    queued_model = m.name\n                    print(f'Found model still in progress: {queued_model}')\n        \n        # Use queued model if found and no completed model\n        if not model_id and queued_model:\n            model_id = queued_model\n            print(f'Using in-progress model: {model_id}')\n    \n    # Create new model if needed\n    if not model_id:\n        print(\"Starting new fine-tuning job...\")\n        tuning_op = client.tunings.tune(\n            base_model=\"models/gemini-1.5-flash-001-tuning\",\n            training_dataset=training_data,\n            config=types.CreateTuningJobConfig(\n                tuned_model_display_name=\"transaction-category-classifier\",  \n                batch_size=16,\n                epoch_count=3,\n            ),\n        )\n        \n        model_id = tuning_op.name\n        print(f\"Fine-tuning initiated. Model ID: {model_id}\")\n        print(f\"Current status: {tuning_op.state}\")\n        \n        # Poll for status updates (optional)\n        print(\"Initial training status:\")\n        print(f\"  - State: {tuning_op.state}\")\n        print(f\"  - Create time: {tuning_op.create_time}\")\n        if hasattr(tuning_op, 'progress') and tuning_op.progress:\n            print(f\"  - Progress: {tuning_op.progress}%\")\n    \n    # Save the model ID for later use\n    with open(\"tuned_model_id.txt\", \"w\") as f:\n        f.write(model_id)\n    \n    print(f\"\\nUsing model: {model_id}\")\n    print(\"This ID has been saved and will be used for predictions\")\n    \nexcept Exception as e:\n    print(f\"Error in fine-tuning process: {e}\")","metadata":{"execution":{"iopub.execute_input":"2025-04-10T05:48:56.388511Z","iopub.status.busy":"2025-04-10T05:48:56.387205Z","iopub.status.idle":"2025-04-10T05:48:58.145557Z","shell.execute_reply":"2025-04-10T05:48:58.144833Z","shell.execute_reply.started":"2025-04-10T05:48:56.388457Z"}}},{"cell_type":"markdown","source":"## Monitoring progress\nHere I monitor whether this model has been tuned and ready to use.","metadata":{}},{"cell_type":"code","source":"# 4. Monitor the fine-tuning progress\nstart_time = datetime.datetime.now(datetime.timezone.utc)\ntuned_model = client.tunings.get(name=model_id)\n\nwhile not tuned_model.has_ended:\n    print(f\"Current state: {tuned_model.state.name}\")\n    if hasattr(tuned_model, 'progress'):\n        print(f\"Progress: {tuned_model.progress}%\")\n    \n    time.sleep(60)  # Check every minute\n    tuned_model = client.tunings.get(name=model_id)\n\nprint(f\"Done! The model state is: {tuned_model.state.name}\")\n\nif not tuned_model.has_succeeded and tuned_model.error:\n    print(\"Error:\", tuned_model.error)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T11:23:50.458497Z","iopub.execute_input":"2025-04-13T11:23:50.458799Z","iopub.status.idle":"2025-04-13T11:23:50.493514Z","shell.execute_reply.started":"2025-04-13T11:23:50.458769Z","shell.execute_reply":"2025-04-13T11:23:50.492072Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[43], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 4. Monitor the fine-tuning progress\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m start_time \u001b[38;5;241m=\u001b[39m \u001b[43mdatetime\u001b[49m\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow(datetime\u001b[38;5;241m.\u001b[39mtimezone\u001b[38;5;241m.\u001b[39mutc)\n\u001b[1;32m      3\u001b[0m tuned_model \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mtunings\u001b[38;5;241m.\u001b[39mget(name\u001b[38;5;241m=\u001b[39mmodel_id)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tuned_model\u001b[38;5;241m.\u001b[39mhas_ended:\n","\u001b[0;31mNameError\u001b[0m: name 'datetime' is not defined"],"ename":"NameError","evalue":"name 'datetime' is not defined","output_type":"error"}],"execution_count":43},{"cell_type":"markdown","source":"## Evaluate Tuned Model\nHere I test and evaluate the performance of the tuned model.","metadata":{}},{"cell_type":"code","source":"# 5. Test the tuned model with a sample transaction\ndef categorize_transaction(transaction_note):\n    \"\"\"Use the fine-tuned model to categorize a transaction.\"\"\"\n    response = client.models.generate_content(\n        model=model_id,\n        contents=transaction_note,\n        config=types.GenerateContentConfig(\n            temperature=0.0,  # Use deterministic output for classification\n            max_output_tokens=10,  # Keep it short, we just need the category\n        )\n    )\n    \n    if response.candidates and response.candidates[0].content:\n        return response.candidates[0].content.parts[0].text.strip()\n    else:\n        return \"(error)\"\n\n# Test with a sample transaction\nsample_transaction = \"AMAZON PRIME MEMBERSHIP ANNUAL RENEWAL\"\npredicted_category = categorize_transaction(sample_transaction)\nprint(f\"Transaction: {sample_transaction}\")\nprint(f\"Predicted category: {predicted_category}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T11:23:50.494632Z","iopub.status.idle":"2025-04-13T11:23:50.495204Z","shell.execute_reply.started":"2025-04-13T11:23:50.494903Z","shell.execute_reply":"2025-04-13T11:23:50.494931Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 6. Evaluate the model on test data\nimport tqdm\nfrom tqdm.rich import tqdm as tqdmr\nimport warnings\n\n# Enable tqdm features on Pandas\ntqdmr.pandas()\n\n# Suppress the experimental warning\nwarnings.filterwarnings(\"ignore\", category=tqdm.TqdmExperimentalWarning)\n\n# Sample a subset of test data for evaluation\nTEST_SAMPLE_SIZE = 20\ndf_eval = df_test_sampled.sample(min(TEST_SAMPLE_SIZE, len(df_test_sampled)))\n\nprint(f\"Evaluating on {len(df_eval)} test transactions...\")\n\n# Make predictions with progress bar\ndf_eval['prediction'] = df_eval['note'].progress_apply(categorize_transaction)\n\n# Calculate accuracy\naccuracy = (df_eval['subcategory'] == df_eval['prediction']).mean()\nprint(f\"Model accuracy: {accuracy:.2%}\")\n\n# Display some examples\nprint(\"\\nSample predictions:\")\nfor idx, row in df_eval.sample(min(5, len(df_eval))).iterrows():\n    print(f\"Transaction: {row['note'][:50]}...\")\n    print(f\"True category: {row['subcategory']}\")\n    print(f\"Predicted: {row['prediction']}\")\n    print(f\"Correct: {row['subcategory'] == row['prediction']}\\n\")\n\n# Show error analysis\nerrors = df_eval[df_eval['subcategory'] != df_eval['prediction']]\nif len(errors) > 0:\n    print(f\"Found {len(errors)} misclassifications\")\n    print(\"Most common error patterns:\")\n    error_matrix = pd.crosstab(\n        errors['subcategory'], \n        errors['prediction'], \n        rownames=['True'], \n        colnames=['Predicted']\n    )\n    print(error_matrix)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T11:23:50.496828Z","iopub.status.idle":"2025-04-13T11:23:50.497250Z","shell.execute_reply.started":"2025-04-13T11:23:50.497039Z","shell.execute_reply":"2025-04-13T11:23:50.497057Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def list_tables() -> list[str]:\n    \"\"\"Retrieve the names of all tables in the database.\"\"\"\n    # Include print logging statements so you can see when functions are being called.\n    print(' - DB CALL: list_tables()')\n\n    cursor = db_conn.cursor()\n\n    # Fetch the table names.\n    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n\n    tables = cursor.fetchall()\n    return [t[0] for t in tables]\n\n\nlist_tables()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def describe_table(table_name: str) -> list[tuple[str, str]]:\n    \"\"\"Look up the table schema.\n\n    Returns:\n      List of columns, where each entry is a tuple of (column, type).\n    \"\"\"\n    print(f' - DB CALL: describe_table({table_name})')\n\n    cursor = db_conn.cursor()\n\n    cursor.execute(f\"PRAGMA table_info({table_name});\")\n\n    schema = cursor.fetchall()\n    # [column index, column name, column type, ...]\n    return [(col[1], col[2]) for col in schema]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def execute_query(sql: str) -> list[list[str]]:\n    \"\"\"Execute an SQL statement, returning the results.\"\"\"\n    print(f' - DB CALL: execute_query({sql})')\n\n    cursor = db_conn.cursor()\n\n    cursor.execute(sql)\n    return cursor.fetchall()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# These are the database interaction tools defined earlier\ndb_tools = [list_tables, describe_table, execute_query]\n\n# System instruction for the AI to understand what it needs to do\ninstruction = \"\"\"You are a helpful chatbot that can interact with an SQL database for financial transactions. \nYou will first use list_tables to see what tables are present, describe_table to understand the\nschema, and execute_query to issue an SQL SELECT query to retrieve all category-subcategory combinations.\"\"\"\n\n# Create the Google Genai client\nclient = genai.Client(api_key=GOOGLE_API_KEY)\n\nfrom google.api_core import retry\nimport pandas as pd\n\n# Define retry logic for API rate limits\nis_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n\n# User query to get direct category-subcategory combinations\nuser_query = \"\"\"1. Find all unique combinations of category and subcategory from the database.\n2. Execute a SQL query that joins the categories and subcategories tables.\n3. Return the full category name and full subcategory name.\n4. Format your response as simple tabular data that can be saved as CSV.\"\"\"\n\n# Function to get category-subcategory combinations with retry logic\n@retry.Retry(predicate=is_retriable)\ndef get_category_subcategory_combinations():\n    response = client.models.generate_content(\n        model=\"gemini-2.0-flash\",\n        contents=user_query,\n        config=types.GenerateContentConfig(\n            system_instruction=instruction,\n            tools=db_tools,\n        ),\n    )\n    return response.text","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}