{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90020b7c",
   "metadata": {
    "papermill": {
     "duration": 0.00832,
     "end_time": "2025-04-18T06:47:11.230451",
     "exception": false,
     "start_time": "2025-04-18T06:47:11.222131",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Categorise finance transactions\n",
    "\n",
    "In life, my financial transactions are often categorised incorrectly in my budgeting app. I decided to find a better solution.\n",
    "\n",
    "In this example, I will first try to categorise with an existing Gemini model using a few-shot prompt and evaluate its performance. Then I will tune a model with the data categorised by me and evaluate its performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7019b930",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T06:47:11.246586Z",
     "iopub.status.busy": "2025-04-18T06:47:11.245667Z",
     "iopub.status.idle": "2025-04-18T06:47:29.503411Z",
     "shell.execute_reply": "2025-04-18T06:47:29.501965Z"
    },
    "papermill": {
     "duration": 18.268662,
     "end_time": "2025-04-18T06:47:29.506150",
     "exception": false,
     "start_time": "2025-04-18T06:47:11.237488",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip uninstall -qqy jupyterlab  # Remove unused conflicting packages\n",
    "!pip install -U -q \"google-genai==1.7.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2138851",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T06:47:29.521846Z",
     "iopub.status.busy": "2025-04-18T06:47:29.521449Z",
     "iopub.status.idle": "2025-04-18T06:47:30.783031Z",
     "shell.execute_reply": "2025-04-18T06:47:30.782028Z"
    },
    "papermill": {
     "duration": 1.272002,
     "end_time": "2025-04-18T06:47:30.785241",
     "exception": false,
     "start_time": "2025-04-18T06:47:29.513239",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.7.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "genai.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffa75e10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T06:47:30.801898Z",
     "iopub.status.busy": "2025-04-18T06:47:30.800859Z",
     "iopub.status.idle": "2025-04-18T06:47:31.035090Z",
     "shell.execute_reply": "2025-04-18T06:47:31.034039Z"
    },
    "papermill": {
     "duration": 0.245011,
     "end_time": "2025-04-18T06:47:31.037460",
     "exception": false,
     "start_time": "2025-04-18T06:47:30.792449",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "GOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n",
    "\n",
    "client = genai.Client(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60cea97",
   "metadata": {
    "papermill": {
     "duration": 0.00727,
     "end_time": "2025-04-18T06:47:31.052836",
     "exception": false,
     "start_time": "2025-04-18T06:47:31.045566",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load the subcategory and category table\n",
    "In this step, I load the subcategory, category table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40d7b420",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T06:47:31.068726Z",
     "iopub.status.busy": "2025-04-18T06:47:31.068362Z",
     "iopub.status.idle": "2025-04-18T06:47:31.122759Z",
     "shell.execute_reply": "2025-04-18T06:47:31.121712Z"
    },
    "papermill": {
     "duration": 0.065082,
     "end_time": "2025-04-18T06:47:31.124900",
     "exception": false,
     "start_time": "2025-04-18T06:47:31.059818",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database schema created successfully!\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Connect to your database\n",
    "db_conn = sqlite3.connect('/kaggle/working/transaction_categories.db')\n",
    "cursor = db_conn.cursor()\n",
    "\n",
    "# Create the tables\n",
    "cursor.execute('''\n",
    "CREATE TABLE IF NOT EXISTS categories (\n",
    "    category_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    name VARCHAR(100) NOT NULL UNIQUE,\n",
    "    description TEXT,\n",
    "    display_order INT DEFAULT 100,\n",
    "    is_active BOOLEAN DEFAULT 1,\n",
    "    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    ")\n",
    "''')\n",
    "\n",
    "cursor.execute('''\n",
    "CREATE TABLE IF NOT EXISTS subcategories (\n",
    "    subcategory_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    category_id INTEGER NOT NULL,\n",
    "    name VARCHAR(100) NOT NULL,\n",
    "    description TEXT,\n",
    "    display_order INT DEFAULT 100,\n",
    "    is_active BOOLEAN DEFAULT 1,\n",
    "    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "    FOREIGN KEY (category_id) REFERENCES categories(category_id)\n",
    ")\n",
    "''')\n",
    "\n",
    "\n",
    "# Insert main categories\n",
    "categories = [\n",
    "    ('Food & Beverages', 'Expenses related to food and drinks', 10),\n",
    "    ('Shopping', 'Retail purchases and shopping expenses', 20),\n",
    "    ('Housing', 'Home-related expenses including rent and utilities', 30),\n",
    "    ('Transportation', 'Public and private transportation costs', 40),\n",
    "    ('Vehicle', 'Car and vehicle related expenses', 50),\n",
    "    ('Life & Entertainment', 'Leisure activities and entertainment', 60),\n",
    "    ('Communication, PC', 'Internet, phone and computer expenses', 70),\n",
    "    ('Financial expenses', 'Banking fees, loans, and financial costs', 80),\n",
    "    ('Investments', 'Investment-related transactions', 90),\n",
    "    ('Income', 'All sources of incoming money', 100),\n",
    "    ('TRANSFER', 'Money transfers between accounts', 110)\n",
    "]\n",
    "\n",
    "cursor.executemany('INSERT OR IGNORE INTO categories (name, description, display_order) VALUES (?, ?, ?)', categories)\n",
    "\n",
    "# Insert subcategories for Food & Beverages\n",
    "food_subcategories = [\n",
    "    (1, 'Bar, cafe, drink, snacks', 10),\n",
    "    (1, 'Groceries', 20),\n",
    "    (1, 'Restaurant, fast-food', 30)\n",
    "]\n",
    "cursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', food_subcategories)\n",
    "\n",
    "# Insert subcategories for Shopping\n",
    "shopping_subcategories = [\n",
    "    (2, 'Clothes & Footwear', 10),\n",
    "    (2, 'Drug-store, chemist', 20),\n",
    "    (2, 'Electronics, accessories', 30),\n",
    "    (2, 'Gifts, joy', 40),\n",
    "    (2, 'Health and beauty', 50),\n",
    "    (2, 'Home, garden', 60),\n",
    "    (2, 'Jewels, accessories', 70),\n",
    "    (2, 'Kids', 80),\n",
    "    (2, 'Leisure time', 90),\n",
    "    (2, 'Pets, animals', 100),\n",
    "    (2, 'Stationery, tools', 110)\n",
    "]\n",
    "cursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', shopping_subcategories)\n",
    "\n",
    "housing_subcategories=[\n",
    "    (3, 'Energy, utilities', 10),\n",
    "    (3, 'Maintenance, repairs', 20),\n",
    "    (3, 'Mortgage', 30),\n",
    "    (3, 'Property insurance', 40),\n",
    "    (3, 'Rent', 50),\n",
    "    (3, 'Services', 60)\n",
    "]\n",
    "cursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', housing_subcategories)\n",
    "\n",
    "\n",
    "transportation_subcategories=[\n",
    "    (4, 'Business trips', 10),\n",
    "    (4, 'Long distance', 20),\n",
    "    (4, 'Public transport', 30),\n",
    "    (4, 'Taxi', 40)\n",
    "]\n",
    "cursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', transportation_subcategories)\n",
    "\n",
    "vehicle_subcategories=[\n",
    "    (5, 'Fuel', 10),\n",
    "    (5, 'Leasing', 20),\n",
    "    (5, 'Parking', 30),\n",
    "    (5, 'Rentals', 40),\n",
    "    (5, 'Vehicle insurance', 50),\n",
    "    (5, 'Vehicle maintenance', 60)\n",
    "]\n",
    "cursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', vehicle_subcategories)\n",
    "\n",
    "life_subcategories=[\n",
    "    (6, 'Active sport, fitness', 10),\n",
    "    (6, 'Alcohol, tobacco', 20),\n",
    "    (6, 'Books, audio, subscriptions', 30),\n",
    "    (6, 'Charity, gifts', 40),\n",
    "    (6, 'Culture, sport events', 50),\n",
    "    (6, 'Education, development', 60),\n",
    "    (6, 'Health care, doctor', 70),\n",
    "    (6, 'Hobbies', 80),\n",
    "    (6, 'Holiday, trips, hotels', 90),\n",
    "    (6, 'Life events', 100),\n",
    "    (6, 'Lottery, gambling', 110),\n",
    "    (6, 'TV, Streaming', 120),\n",
    "    (6, 'Wellness, beauty', 130)\n",
    "]\n",
    "cursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', life_subcategories)\n",
    "\n",
    "communication_subcategories=[\n",
    "    (7, 'Internet', 10),\n",
    "    (7, 'Postal services', 20),\n",
    "    (7, 'Software, apps, games', 30),\n",
    "    (7, 'Telephony, mobile phone', 40)\n",
    "]\n",
    "cursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', communication_subcategories)\n",
    "\n",
    "financial_subcategories=[\n",
    "    (8, 'Advisory', 10),\n",
    "    (8, 'Charges, Fees', 20),\n",
    "    (8, 'Child Support', 30),\n",
    "    (8, 'Fines', 40),\n",
    "    (8, 'Insurances', 50),\n",
    "    (8, 'Loans, interests', 60),\n",
    "    (8, 'Taxes', 70)\n",
    "]\n",
    "cursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', financial_subcategories)\n",
    "\n",
    "investments_subcategories=[\n",
    "    (9, 'Collections', 10),\n",
    "    (9, 'Financial investments', 20),\n",
    "    (9, 'Realty', 30),\n",
    "    (9, 'Savings', 40),\n",
    "    (9, 'Vehicles, chattels', 50)\n",
    "]\n",
    "cursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', investments_subcategories)\n",
    "\n",
    "income_subcategories=[\n",
    "    (10, 'Checks, coupons', 10),\n",
    "    (10, 'Child Support', 20),\n",
    "    (10, 'Dues & grants', 30),\n",
    "    (10, 'Gifts', 40),\n",
    "    (10, 'Interests, dividends', 50),\n",
    "    (10, 'Lending, renting', 60),\n",
    "    (10, 'Lottery earning', 70),\n",
    "    (10, 'Refunds (tax, purchase)', 80),\n",
    "    (10, 'Rental income', 90),\n",
    "    (10, 'Sale', 100),\n",
    "    (10, 'Wage, invoices', 110)\n",
    "]\n",
    "cursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', income_subcategories)\n",
    "\n",
    "transfer_subcatgories=[\n",
    "    (11, 'TRANSFER', 10),   \n",
    "]\n",
    "cursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', transfer_subcatgories)\n",
    "\n",
    "# Commit the changes\n",
    "db_conn.commit()\n",
    "print(\"Database schema created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6233d68b",
   "metadata": {
    "papermill": {
     "duration": 0.00682,
     "end_time": "2025-04-18T06:47:31.138987",
     "exception": false,
     "start_time": "2025-04-18T06:47:31.132167",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In this step, I create the mapping beween category and subcategory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cd03988",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T06:47:31.155159Z",
     "iopub.status.busy": "2025-04-18T06:47:31.154266Z",
     "iopub.status.idle": "2025-04-18T06:47:31.162499Z",
     "shell.execute_reply": "2025-04-18T06:47:31.161555Z"
    },
    "papermill": {
     "duration": 0.018391,
     "end_time": "2025-04-18T06:47:31.164490",
     "exception": false,
     "start_time": "2025-04-18T06:47:31.146099",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def setup_database_and_get_hierarchy(output_path=\"/kaggle/working/category_mapping.csv\"):\n",
    "    \"\"\"\n",
    "    Initialize database, return category hierarchy, and output a simple mapping CSV.\n",
    "    \n",
    "    Args:\n",
    "        output_path: Path to save the mapping CSV\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (db_connection, category_hierarchy_dict, subcategory_to_category_mapping)\n",
    "    \"\"\"\n",
    "    import sqlite3\n",
    "    import pandas as pd\n",
    "    \n",
    "    print(\"Setting up database and extracting category hierarchy...\")\n",
    "    \n",
    "    # Create database connection\n",
    "    db_conn = sqlite3.connect('/kaggle/working/transaction_categories.db')\n",
    "    cursor = db_conn.cursor()\n",
    "    \n",
    "    # Create tables and populate data if needed (your existing code)\n",
    "    # ... (Keep your existing table creation and population code)\n",
    "    \n",
    "    # Get complete hierarchy in one operation\n",
    "    cursor.execute(\"\"\"\n",
    "    SELECT \n",
    "        c.name as category, \n",
    "        s.name as subcategory\n",
    "    FROM categories c\n",
    "    JOIN subcategories s ON c.category_id = s.category_id\n",
    "    ORDER BY c.display_order, s.display_order\n",
    "    \"\"\")\n",
    "    \n",
    "    # Convert query results to DataFrame\n",
    "    results = cursor.fetchall()\n",
    "    mapping_df = pd.DataFrame(results, columns=['category', 'subcategory'])\n",
    "    \n",
    "    # Process results into usable format for return values\n",
    "    category_hierarchy = {}\n",
    "    subcat_to_cat_mapping = {}\n",
    "    \n",
    "    for category, subcategory in results:\n",
    "        # Build hierarchy dictionary\n",
    "        if category not in category_hierarchy:\n",
    "            category_hierarchy[category] = []\n",
    "        category_hierarchy[category].append(subcategory)\n",
    "        \n",
    "        # Build mapping dictionary\n",
    "        subcat_to_cat_mapping[subcategory] = category\n",
    "    \n",
    "    # Save to CSV file\n",
    "    mapping_df.to_csv(output_path, index=False)\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\nCategory-subcategory mapping saved to {output_path}\")\n",
    "    print(f\"Found {len(mapping_df['category'].unique())} categories and {len(mapping_df)} subcategories\")\n",
    "    print(\"\\nSample of mapping:\")\n",
    "    print(mapping_df.head(5))\n",
    "    \n",
    "    db_conn.commit()\n",
    "    \n",
    "    return db_conn, category_hierarchy, subcat_to_cat_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2eb7759",
   "metadata": {
    "id": "peFm0w_0c1CO",
    "papermill": {
     "duration": 0.006806,
     "end_time": "2025-04-18T06:47:31.178330",
     "exception": false,
     "start_time": "2025-04-18T06:47:31.171524",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load the dataset\n",
    "\n",
    "I have uploaded transaction data categorised by me. Then I group it into training data and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ec82bf0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T06:47:31.194583Z",
     "iopub.status.busy": "2025-04-18T06:47:31.193715Z",
     "iopub.status.idle": "2025-04-18T06:47:33.463082Z",
     "shell.execute_reply": "2025-04-18T06:47:33.461842Z"
    },
    "papermill": {
     "duration": 2.279971,
     "end_time": "2025-04-18T06:47:33.465302",
     "exception": false,
     "start_time": "2025-04-18T06:47:31.185331",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of subcategories: 64\n",
      "Sample subcategories: ['Active sport, fitness' 'Advisory' 'Alcohol, tobacco'\n",
      " 'Bar, cafe, drink, snacks' 'Books, audio, subscriptions' 'Charges, Fees'\n",
      " 'Charity, gifts' 'Checks, coupons' 'Clothes & shoes'\n",
      " 'Culture, sport events']\n",
      "\n",
      "Sample notes:\n",
      "1. AMAZON AUSYDNEY SOUTH CREDIT CARD PURCHASEAmazon Basics High-Density Round Foam Roller for Exercise and Recovery - 61cm, Blue Speckled → Active sport, fitness\n",
      "2. 02 DEC 20 - $98.00 LULULEMON ATHLETICA AUSTRAlbert Park [Eff Date: 30 NOV 20] → Active sport, fitness\n",
      "3. REBEL MELBOURNE CTRL MELBOURNE VI AUSTap and Pay xx3173Value Date_ 17/03/2018 → Active sport, fitness\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess transaction data\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load your data\n",
    "file_path = \"/kaggle/input/training/categorized_transaction.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Split into train and test sets (80/20 split)\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Display the subcategories (labels) in your dataset\n",
    "subcategories = df['subcategory'].unique()\n",
    "print(f\"Number of subcategories: {len(subcategories)}\")\n",
    "print(\"Sample subcategories:\", subcategories[:10])  # Show first 10 subcategories\n",
    "\n",
    "# Quick look at note examples\n",
    "print(\"\\nSample notes:\")\n",
    "for i, note in enumerate(df['note'].head(3)):\n",
    "    print(f\"{i+1}. {note} → {df['subcategory'].iloc[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977e78d0",
   "metadata": {
    "papermill": {
     "duration": 0.006749,
     "end_time": "2025-04-18T06:47:33.479267",
     "exception": false,
     "start_time": "2025-04-18T06:47:33.472518",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "517b5901",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T06:47:33.496212Z",
     "iopub.status.busy": "2025-04-18T06:47:33.495550Z",
     "iopub.status.idle": "2025-04-18T06:47:33.504072Z",
     "shell.execute_reply": "2025-04-18T06:47:33.503099Z"
    },
    "papermill": {
     "duration": 0.019189,
     "end_time": "2025-04-18T06:47:33.506137",
     "exception": false,
     "start_time": "2025-04-18T06:47:33.486948",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def clean_transaction_note(note):\n",
    "    \"\"\"\n",
    "    Clean transaction notes to remove common bank formatting, dates, card numbers, etc.\n",
    "    \"\"\"\n",
    "    # Handle None or empty strings\n",
    "    if note is None or pd.isna(note) or note == \"\":\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to string if needed\n",
    "    text = str(note)\n",
    "    \n",
    "    # Replace non-ASCII characters\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "    \n",
    "    # Extract main part of the transaction (before common transaction markers)\n",
    "    transaction_markers = r'\\s+(?:CREDIT CARD PURCHASE|EFTPOS|Value Date|tap and Pay|Card Purchase|CARD PURCHASE)'\n",
    "    parts = re.split(transaction_markers, text, flags=re.IGNORECASE)\n",
    "    main_text = parts[0] if parts else text\n",
    "    \n",
    "    # Clean amount figures and currency symbols\n",
    "    main_text = re.sub(r'(?:[$€£¥]|AUD|USD|EUR|GBP|NZD)\\s*[-+]?(?:\\d{1,3}(?:,\\d{3})*|\\d+)(?:\\.\\d{1,2})?', '', main_text)\n",
    "    main_text = re.sub(r'\\b[-+]?(?:\\d{1,3}(?:,\\d{3})*|\\d+)(?:\\.\\d{1,2})?\\b', '', main_text)\n",
    "    \n",
    "    # Remove card numbers (masked or full)\n",
    "    main_text = re.sub(r'(?:x{2,4}|X{2,4})\\d{4}|\\b\\d{4}[\\s-]?\\d{4}[\\s-]?\\d{4}[\\s-]?\\d{4}\\b', '', main_text)\n",
    "    \n",
    "    # Remove date patterns\n",
    "    date_pattern = r'(?:\\d{1,2}\\s+(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\\s+\\d{2,4}|' + \\\n",
    "                   r'(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\\s+\\d{1,2}(?:st|nd|rd|th)?\\s+\\d{2,4}|' + \\\n",
    "                   r'\\d{1,2}[-/.]\\d{1,2}[-/.]\\d{2,4}|' + \\\n",
    "                   r'\\d{4}[-/.]\\d{1,2}[-/.]\\d{1,2}|' + \\\n",
    "                   r'\\[Eff\\s+Date:.*?\\]|' + \\\n",
    "                   r'Value\\s+Date[_:]\\s*\\d{1,2}[-/.]\\d{1,2}[-/.]\\d{2,4})'\n",
    "    main_text = re.sub(date_pattern, '', main_text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Clean whitespace and punctuation\n",
    "    main_text = re.sub(r'\\s+', ' ', main_text)\n",
    "    main_text = re.sub(r'[\\s,.-]+$|^[\\s,.-]+', '', main_text)\n",
    "    main_text = re.sub(r'\\s+([,.])', r'\\1', main_text)\n",
    "    \n",
    "    return main_text.strip()[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b06ab92e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T06:47:33.522205Z",
     "iopub.status.busy": "2025-04-18T06:47:33.521508Z",
     "iopub.status.idle": "2025-04-18T06:47:33.529772Z",
     "shell.execute_reply": "2025-04-18T06:47:33.528697Z"
    },
    "papermill": {
     "duration": 0.018464,
     "end_time": "2025-04-18T06:47:33.531772",
     "exception": false,
     "start_time": "2025-04-18T06:47:33.513308",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_category_column(df, db_conn):\n",
    "    \"\"\"\n",
    "    Add category column to DataFrame based on subcategory using database mapping.\n",
    "    \"\"\"\n",
    "    if 'category' in df.columns:\n",
    "        print(\"Category column already exists\")\n",
    "        return df\n",
    "    \n",
    "    try:\n",
    "        # Query the database for subcategory to category mapping\n",
    "        cursor = db_conn.cursor()\n",
    "        cursor.execute(\"\"\"\n",
    "        SELECT s.name as subcategory, c.name as category \n",
    "        FROM subcategories s\n",
    "        JOIN categories c ON s.category_id = c.category_id\n",
    "        \"\"\")\n",
    "        \n",
    "        # Create mapping dictionary\n",
    "        subcat_to_cat = {row[0]: row[1] for row in cursor.fetchall()}\n",
    "        \n",
    "        # Add category column\n",
    "        df_with_category = df.copy()\n",
    "        df_with_category['category'] = df['subcategory'].map(subcat_to_cat)\n",
    "        \n",
    "        # Check for unmapped subcategories\n",
    "        missing_count = df_with_category['category'].isna().sum()\n",
    "        if missing_count > 0:\n",
    "            print(f\"Warning: {missing_count} rows have unmapped subcategories\")\n",
    "            unmapped = df[df['subcategory'].map(lambda x: x not in subcat_to_cat)]['subcategory'].unique()\n",
    "            print(f\"Unmapped subcategories: {unmapped}\")\n",
    "            \n",
    "        # Fill missing with placeholder\n",
    "        df_with_category['category'] = df_with_category['category'].fillna(\"Unknown\")\n",
    "        \n",
    "        print(f\"Added categories to {len(df_with_category)} transactions\")\n",
    "        return df_with_category\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting category mapping: {e}\")\n",
    "        # Create placeholder category column if needed\n",
    "        df_copy = df.copy()\n",
    "        df_copy['category'] = \"Unknown\"\n",
    "        return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1a3c739",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T06:47:33.547666Z",
     "iopub.status.busy": "2025-04-18T06:47:33.547316Z",
     "iopub.status.idle": "2025-04-18T06:47:33.553152Z",
     "shell.execute_reply": "2025-04-18T06:47:33.552140Z"
    },
    "papermill": {
     "duration": 0.016168,
     "end_time": "2025-04-18T06:47:33.555105",
     "exception": false,
     "start_time": "2025-04-18T06:47:33.538937",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sample_balanced_data(df, samples_per_label):\n",
    "    \"\"\"\n",
    "    Create a balanced dataset by sampling evenly across subcategories.\n",
    "    If a subcategory has fewer than the requested samples, uses all available rows.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing transaction data\n",
    "        samples_per_label: Number of samples to select for each subcategory\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with balanced samples across subcategories\n",
    "    \"\"\"\n",
    "    # Group by subcategory and sample\n",
    "    sampled_df = (\n",
    "        df.groupby(\"subcategory\")[df.columns]\n",
    "        .apply(lambda x: x.sample(min(len(x), samples_per_label)))\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    \n",
    "    # Convert to category type for efficiency\n",
    "    sampled_df[\"subcategory\"] = sampled_df[\"subcategory\"].astype(\"category\")\n",
    "    \n",
    "    return sampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc2a424f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T06:47:33.571545Z",
     "iopub.status.busy": "2025-04-18T06:47:33.570769Z",
     "iopub.status.idle": "2025-04-18T06:47:33.576582Z",
     "shell.execute_reply": "2025-04-18T06:47:33.575648Z"
    },
    "papermill": {
     "duration": 0.016115,
     "end_time": "2025-04-18T06:47:33.578518",
     "exception": false,
     "start_time": "2025-04-18T06:47:33.562403",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sample_balanced_data_combined(df, samples_per_label):\n",
    "    \"\"\"\n",
    "    Create a balanced dataset by sampling evenly across combined labels.\n",
    "    If a label has fewer than the requested samples, uses all available rows.\n",
    "    \"\"\"\n",
    "    # Group by combined label and sample\n",
    "    sampled_df = (\n",
    "        df.groupby(\"combined_label\")[df.columns]\n",
    "        .apply(lambda x: x.sample(min(len(x), samples_per_label)))\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    \n",
    "    # Convert to category type for efficiency\n",
    "    sampled_df[\"combined_label\"] = sampled_df[\"combined_label\"].astype(\"category\")\n",
    "    \n",
    "    return sampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d95bdcd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T06:47:33.594731Z",
     "iopub.status.busy": "2025-04-18T06:47:33.594367Z",
     "iopub.status.idle": "2025-04-18T06:47:33.601069Z",
     "shell.execute_reply": "2025-04-18T06:47:33.599948Z"
    },
    "papermill": {
     "duration": 0.017416,
     "end_time": "2025-04-18T06:47:33.603147",
     "exception": false,
     "start_time": "2025-04-18T06:47:33.585731",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def filter_unmapped_subcategories(df, db_conn):\n",
    "    \"\"\"\n",
    "    Filter out rows with subcategories that don't have a mapping in the database.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing transaction data\n",
    "        db_conn: Database connection\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with only mapped subcategories\n",
    "    \"\"\"\n",
    "    # Get all valid subcategories from the database\n",
    "    cursor = db_conn.cursor()\n",
    "    cursor.execute(\"SELECT name FROM subcategories\")\n",
    "    valid_subcategories = [row[0] for row in cursor.fetchall()]\n",
    "    \n",
    "    # Filter the DataFrame to only include rows with valid subcategories\n",
    "    df_filtered = df[df['subcategory'].isin(valid_subcategories)].copy()\n",
    "    \n",
    "    # Report how many rows were filtered out\n",
    "    filtered_count = len(df) - len(df_filtered)\n",
    "    print(f\"Filtered out {filtered_count} rows with unmapped subcategories\")\n",
    "    print(f\"Unmapped subcategories: {df[~df['subcategory'].isin(valid_subcategories)]['subcategory'].unique()}\")\n",
    "    \n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a8b9fa1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T06:47:33.619775Z",
     "iopub.status.busy": "2025-04-18T06:47:33.619147Z",
     "iopub.status.idle": "2025-04-18T06:47:33.630353Z",
     "shell.execute_reply": "2025-04-18T06:47:33.629389Z"
    },
    "papermill": {
     "duration": 0.02184,
     "end_time": "2025-04-18T06:47:33.632298",
     "exception": false,
     "start_time": "2025-04-18T06:47:33.610458",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_transaction_data_combined(df, db_conn, train_samples=50, test_samples=10, sample_csv_path=None):\n",
    "    \"\"\"\n",
    "    Process transaction data with combined category/subcategory labels\n",
    "    \"\"\"\n",
    "    # Make a copy to avoid modifying the original\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Step 1: Clean transaction notes\n",
    "    print(\"Cleaning transaction notes...\")\n",
    "    df_copy['cleaned_note'] = df_copy['note'].apply(clean_transaction_note)\n",
    "    \n",
    "    # Step 2: Add category column if needed\n",
    "    if 'category' not in df_copy.columns:\n",
    "        print(\"Adding category mapping...\")\n",
    "        df_copy = add_category_column(df_copy, db_conn)\n",
    "    \n",
    "    # Step 3: Create combined category/subcategory label\n",
    "    print(\"Creating combined labels...\")\n",
    "    df_copy['combined_label'] = df_copy['category'] + \"/\" + df_copy['subcategory']\n",
    "    \n",
    "    # Step 4: Filter out unmapped subcategories\n",
    "    print(\"Filtering out unmapped subcategories...\")\n",
    "    df_filtered = filter_unmapped_subcategories(df_copy, db_conn)\n",
    "    \n",
    "    # Step 5: Split into train and test data\n",
    "    train_df, test_df = train_test_split(df_filtered, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Step 6: Sample balanced datasets by subcategory first\n",
    "    print(f\"Creating balanced samples ({train_samples} per subcategory for training)...\")\n",
    "    train_sampled = sample_balanced_data(train_df, train_samples)\n",
    "    test_sampled = sample_balanced_data(test_df, test_samples)\n",
    "    \n",
    "    # Step 7: Create balanced datasets by combined label (optional)\n",
    "    print(\"Creating balanced samples by combined label...\")\n",
    "    try:\n",
    "        train_combined = sample_balanced_data_combined(train_df, max(5, train_samples // 3))\n",
    "        test_combined = sample_balanced_data_combined(test_df, max(2, test_samples // 3))\n",
    "        \n",
    "        # Merge the combined sampled data back into the main samples\n",
    "        # Only keep new rows that aren't already in the subcategory-balanced set\n",
    "        train_existing_indices = set(train_sampled.index)\n",
    "        test_existing_indices = set(test_sampled.index)\n",
    "        \n",
    "        new_train_rows = train_combined[~train_combined.index.isin(train_existing_indices)]\n",
    "        new_test_rows = test_combined[~test_combined.index.isin(test_existing_indices)]\n",
    "        \n",
    "        train_sampled = pd.concat([train_sampled, new_train_rows])\n",
    "        test_sampled = pd.concat([test_sampled, new_test_rows])\n",
    "        \n",
    "        print(f\"Added {len(new_train_rows)} additional rows from combined label sampling\")\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping combined label sampling due to error: {e}\")\n",
    "    \n",
    "    # Step 8: Save sample for review if requested\n",
    "    if sample_csv_path:\n",
    "        # Take a small sample for review\n",
    "        review_sample = train_sampled.sample(min(len(train_sampled), 20))\n",
    "        # Include original, cleaned notes and combined label\n",
    "        review_sample = review_sample[['note', 'cleaned_note', 'category', 'subcategory', 'combined_label']]\n",
    "        review_sample.to_csv(sample_csv_path, index=False)\n",
    "        print(f\"Saved sample data to {sample_csv_path} for review\")\n",
    "    \n",
    "    # Print statistics\n",
    "    combined_labels_count = df_filtered['combined_label'].nunique()\n",
    "    print(f\"Original data: {len(df)} transactions\")\n",
    "    print(f\"Filtered data: {len(df_filtered)} transactions\")\n",
    "    print(f\"Unique combined labels: {combined_labels_count}\")\n",
    "    print(f\"Balanced training data: {len(train_sampled)} transactions\")\n",
    "    print(f\"Balanced test data: {len(test_sampled)} transactions\")\n",
    "    \n",
    "    return train_sampled, test_sampled, df_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe849bf",
   "metadata": {
    "id": "03lDs1O4ZQ0-",
    "papermill": {
     "duration": 0.006959,
     "end_time": "2025-04-18T06:47:33.646905",
     "exception": false,
     "start_time": "2025-04-18T06:47:33.639946",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Sample the dataset\n",
    "Now sample the data. I will keep 50 rows for each subcategory for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ae84d7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T06:47:33.663092Z",
     "iopub.status.busy": "2025-04-18T06:47:33.662724Z",
     "iopub.status.idle": "2025-04-18T06:47:34.283472Z",
     "shell.execute_reply": "2025-04-18T06:47:34.282505Z"
    },
    "papermill": {
     "duration": 0.63329,
     "end_time": "2025-04-18T06:47:34.287536",
     "exception": false,
     "start_time": "2025-04-18T06:47:33.654246",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning transaction notes...\n",
      "Adding category mapping...\n",
      "Warning: 377 rows have unmapped subcategories\n",
      "Unmapped subcategories: ['Clothes & shoes' 'Financial expenses' 'Food & Drinks' 'Free time'\n",
      " 'Housing' 'Life & Entertainment' 'Loan, interests' 'Missing' 'Others'\n",
      " 'Phone, cell phone' 'Shopping' 'Transportation']\n",
      "Added categories to 12388 transactions\n",
      "Creating combined labels...\n",
      "Filtering out unmapped subcategories...\n",
      "Filtered out 377 rows with unmapped subcategories\n",
      "Unmapped subcategories: ['Clothes & shoes' 'Financial expenses' 'Food & Drinks' 'Free time'\n",
      " 'Housing' 'Life & Entertainment' 'Loan, interests' 'Missing' 'Others'\n",
      " 'Phone, cell phone' 'Shopping' 'Transportation']\n",
      "Creating balanced samples (50 per subcategory for training)...\n",
      "Creating balanced samples by combined label...\n",
      "Added 0 additional rows from combined label sampling\n",
      "Saved sample data to /kaggle/working/transaction_sample_review.csv for review\n",
      "Original data: 12388 transactions\n",
      "Filtered data: 12011 transactions\n",
      "Unique combined labels: 52\n",
      "Balanced training data: 1467 transactions\n",
      "Balanced test data: 311 transactions\n",
      "\n",
      "Sample of training data:\n",
      "                                        cleaned_note              category  \\\n",
      "0         DEC - LULULEMON ATHLETICA AUSTRAlbert Park  Life & Entertainment   \n",
      "1  REBEL MELBOURNE CTRL MELBOURNE VI AUSTap and P...  Life & Entertainment   \n",
      "2  STATE TRUSTEES LIMIT MELBOURNE AUSCard Value D...    Financial expenses   \n",
      "3                              BWS BOX HILL BOX HILL  Life & Entertainment   \n",
      "4                       LIQUORLAND EASTLAND RINGWOOD  Life & Entertainment   \n",
      "\n",
      "             subcategory  \n",
      "0  Active sport, fitness  \n",
      "1  Active sport, fitness  \n",
      "2               Advisory  \n",
      "3       Alcohol, tobacco  \n",
      "4       Alcohol, tobacco  \n",
      "\n",
      "--------------------------------------------------\n",
      "Training Data Distribution Statistics:\n",
      "--------------------------------------------------\n",
      "\n",
      "Categories (11 unique):\n",
      "Category                  Count      Percentage\n",
      "---------------------------------------------\n",
      "Life & Entertainment      375        25.6%\n",
      "Shopping                  261        17.8%\n",
      "Housing                   160        10.9%\n",
      "Income                    157        10.7%\n",
      "Food & Beverages          150        10.2%\n",
      "Communication, PC         102        7.0%\n",
      "Transportation            83         5.7%\n",
      "Financial expenses        63         4.3%\n",
      "TRANSFER                  50         3.4%\n",
      "Investments               47         3.2%\n",
      "Vehicle                   19         1.3%\n",
      "\n",
      "Subcategories (52 unique):\n",
      "Subcategory                    Category             Count      Percentage\n",
      "----------------------------------------------------------------------\n",
      "Interests, dividends           Income               50         3.4%\n",
      "Software, apps, games          Communication, PC    50         3.4%\n",
      "Home, garden                   Shopping             50         3.4%\n",
      "Health and beauty              Shopping             50         3.4%\n",
      "Groceries                      Food & Beverages     50         3.4%\n",
      "Internet                       Communication, PC    50         3.4%\n",
      "Maintenance, repairs           Housing              50         3.4%\n",
      "Public transport               Transportation       50         3.4%\n",
      "Refunds (tax, purchase)        Income               50         3.4%\n",
      "Energy, utilities              Housing              50         3.4%\n",
      "Restaurant, fast-food          Food & Beverages     50         3.4%\n",
      "Drug-store, chemist            Shopping             50         3.4%\n",
      "TRANSFER                       TRANSFER             50         3.4%\n",
      "TV, Streaming                  Life & Entertainment 50         3.4%\n",
      "Charity, gifts                 Life & Entertainment 50         3.4%\n",
      "Charges, Fees                  Financial expenses   50         3.4%\n",
      "Books, audio, subscriptions    Life & Entertainment 50         3.4%\n",
      "Bar, cafe, drink, snacks       Food & Beverages     50         3.4%\n",
      "Wage, invoices                 Income               50         3.4%\n",
      "Hobbies                        Life & Entertainment 50         3.4%\n",
      "Financial investments          Investments          46         3.1%\n",
      "Electronics, accessories       Shopping             44         3.0%\n",
      "Holiday, trips, hotels         Life & Entertainment 43         2.9%\n",
      "Wellness, beauty               Life & Entertainment 41         2.8%\n",
      "Gifts, joy                     Shopping             37         2.5%\n",
      "Rent                           Housing              36         2.5%\n",
      "Education, development         Life & Entertainment 34         2.3%\n",
      "Taxi                           Transportation       32         2.2%\n",
      "Stationery, tools              Shopping             26         1.8%\n",
      "Culture, sport events          Life & Entertainment 21         1.4%\n",
      "Services                       Housing              18         1.2%\n",
      "Health care, doctor            Life & Entertainment 14         1.0%\n",
      "Life events                    Life & Entertainment 12         0.8%\n",
      "Insurances                     Financial expenses   11         0.7%\n",
      "Rentals                        Vehicle              11         0.7%\n",
      "Parking                        Vehicle              6          0.4%\n",
      "Mortgage                       Housing              6          0.4%\n",
      "Gifts                          Income               5          0.3%\n",
      "Lottery, gambling              Life & Entertainment 4          0.3%\n",
      "Alcohol, tobacco               Life & Entertainment 4          0.3%\n",
      "Jewels, accessories            Shopping             3          0.2%\n",
      "Active sport, fitness          Life & Entertainment 2          0.1%\n",
      "Postal services                Communication, PC    2          0.1%\n",
      "Savings                        Investments          1          0.1%\n",
      "Fuel                           Vehicle              1          0.1%\n",
      "Dues & grants                  Income               1          0.1%\n",
      "Pets, animals                  Shopping             1          0.1%\n",
      "Checks, coupons                Income               1          0.1%\n",
      "Long distance                  Transportation       1          0.1%\n",
      "Vehicle insurance              Vehicle              1          0.1%\n",
      "Advisory                       Financial expenses   1          0.1%\n",
      "Fines                          Financial expenses   1          0.1%\n",
      "\n",
      "Subcategories with low counts (<5):\n",
      "  - Lottery, gambling: 4 transactions\n",
      "  - Alcohol, tobacco: 4 transactions\n",
      "  - Jewels, accessories: 3 transactions\n",
      "  - Active sport, fitness: 2 transactions\n",
      "  - Postal services: 2 transactions\n",
      "  - Savings: 1 transactions\n",
      "  - Fuel: 1 transactions\n",
      "  - Dues & grants: 1 transactions\n",
      "  - Pets, animals: 1 transactions\n",
      "  - Checks, coupons: 1 transactions\n",
      "  - Long distance: 1 transactions\n",
      "  - Vehicle insurance: 1 transactions\n",
      "  - Advisory: 1 transactions\n",
      "  - Fines: 1 transactions\n",
      "\n",
      "--------------------------------------------------\n",
      "Test Data Distribution Statistics:\n",
      "--------------------------------------------------\n",
      "\n",
      "Categories (11 unique):\n",
      "Category                  Count      Percentage\n",
      "---------------------------------------------\n",
      "Life & Entertainment      90         28.9%\n",
      "Shopping                  55         17.7%\n",
      "Income                    35         11.3%\n",
      "Housing                   33         10.6%\n",
      "Food & Beverages          30         9.6%\n",
      "Communication, PC         21         6.8%\n",
      "Transportation            18         5.8%\n",
      "Financial expenses        14         4.5%\n",
      "TRANSFER                  10         3.2%\n",
      "Investments               3          1.0%\n",
      "Vehicle                   2          0.6%\n",
      "\n",
      "Subcategories (45 unique):\n",
      "Subcategory                    Category             Count      Percentage\n",
      "----------------------------------------------------------------------\n",
      "Wellness, beauty               Life & Entertainment 10         3.2%\n",
      "TRANSFER                       TRANSFER             10         3.2%\n",
      "Restaurant, fast-food          Food & Beverages     10         3.2%\n",
      "Maintenance, repairs           Housing              10         3.2%\n",
      "Software, apps, games          Communication, PC    10         3.2%\n",
      "Internet                       Communication, PC    10         3.2%\n",
      "Interests, dividends           Income               10         3.2%\n",
      "Home, garden                   Shopping             10         3.2%\n",
      "Holiday, trips, hotels         Life & Entertainment 10         3.2%\n",
      "Hobbies                        Life & Entertainment 10         3.2%\n",
      "Health and beauty              Shopping             10         3.2%\n",
      "Public transport               Transportation       10         3.2%\n",
      "Groceries                      Food & Beverages     10         3.2%\n",
      "Energy, utilities              Housing              10         3.2%\n",
      "Drug-store, chemist            Shopping             10         3.2%\n",
      "Wage, invoices                 Income               10         3.2%\n",
      "Bar, cafe, drink, snacks       Food & Beverages     10         3.2%\n",
      "Books, audio, subscriptions    Life & Entertainment 10         3.2%\n",
      "Charges, Fees                  Financial expenses   10         3.2%\n",
      "Charity, gifts                 Life & Entertainment 10         3.2%\n",
      "Refunds (tax, purchase)        Income               10         3.2%\n",
      "TV, Streaming                  Life & Entertainment 10         3.2%\n",
      "Education, development         Life & Entertainment 10         3.2%\n",
      "Electronics, accessories       Shopping             10         3.2%\n",
      "Gifts, joy                     Shopping             9          2.9%\n",
      "Rent                           Housing              9          2.9%\n",
      "Taxi                           Transportation       8          2.6%\n",
      "Health care, doctor            Life & Entertainment 7          2.3%\n",
      "Life events                    Life & Entertainment 5          1.6%\n",
      "Gifts                          Income               4          1.3%\n",
      "Stationery, tools              Shopping             4          1.3%\n",
      "Services                       Housing              3          1.0%\n",
      "Insurances                     Financial expenses   3          1.0%\n",
      "Lottery, gambling              Life & Entertainment 3          1.0%\n",
      "Financial investments          Investments          3          1.0%\n",
      "Culture, sport events          Life & Entertainment 3          1.0%\n",
      "Rentals                        Vehicle              2          0.6%\n",
      "Postal services                Communication, PC    1          0.3%\n",
      "Pets, animals                  Shopping             1          0.3%\n",
      "Mortgage                       Housing              1          0.3%\n",
      "Jewels, accessories            Shopping             1          0.3%\n",
      "Advisory                       Financial expenses   1          0.3%\n",
      "Dues & grants                  Income               1          0.3%\n",
      "Alcohol, tobacco               Life & Entertainment 1          0.3%\n",
      "Active sport, fitness          Life & Entertainment 1          0.3%\n",
      "\n",
      "Subcategories with low counts (<5):\n",
      "  - Gifts: 4 transactions\n",
      "  - Stationery, tools: 4 transactions\n",
      "  - Services: 3 transactions\n",
      "  - Insurances: 3 transactions\n",
      "  - Lottery, gambling: 3 transactions\n",
      "  - Financial investments: 3 transactions\n",
      "  - Culture, sport events: 3 transactions\n",
      "  - Rentals: 2 transactions\n",
      "  - Postal services: 1 transactions\n",
      "  - Pets, animals: 1 transactions\n",
      "  - Mortgage: 1 transactions\n",
      "  - Jewels, accessories: 1 transactions\n",
      "  - Advisory: 1 transactions\n",
      "  - Dues & grants: 1 transactions\n",
      "  - Alcohol, tobacco: 1 transactions\n",
      "  - Active sport, fitness: 1 transactions\n",
      "\n",
      "--------------------------------------------------\n",
      "Summary Statistics:\n",
      "--------------------------------------------------\n",
      "Total transactions in original data: 12388\n",
      "Total transactions in training data: 1467\n",
      "Total transactions in test data: 311\n",
      "Training data categories: 11\n",
      "Test data categories: 11\n",
      "Training data subcategories: 52\n",
      "Test data subcategories: 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17/4103278480.py:47: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  train_sampled = pd.concat([train_sampled, new_train_rows])\n",
      "/tmp/ipykernel_17/4103278480.py:48: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  test_sampled = pd.concat([test_sampled, new_test_rows])\n",
      "/tmp/ipykernel_17/4165388077.py:40: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  subcat_to_cat = df.groupby('subcategory')['category'].first().to_dict()\n",
      "/tmp/ipykernel_17/4165388077.py:40: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  subcat_to_cat = df.groupby('subcategory')['category'].first().to_dict()\n"
     ]
    }
   ],
   "source": [
    "# Connect to the database\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "db_conn = sqlite3.connect('/kaggle/working/transaction_categories.db')\n",
    "\n",
    "# Process the data\n",
    "df_train_sampled, df_test_sampled, df_categorized = process_transaction_data_combined(\n",
    "    df, \n",
    "    db_conn,\n",
    "    train_samples=50, \n",
    "    test_samples=10,\n",
    "    sample_csv_path=\"/kaggle/working/transaction_sample_review.csv\"\n",
    ")\n",
    "\n",
    "# Print distribution of categories and subcategories\n",
    "def print_distribution_stats(df, dataset_name=\"Dataset\"):\n",
    "    print(f\"\\n{'-'*50}\")\n",
    "    print(f\"{dataset_name} Distribution Statistics:\")\n",
    "    print(f\"{'-'*50}\")\n",
    "    \n",
    "    # Category distribution\n",
    "    category_counts = df['category'].value_counts()\n",
    "    print(f\"\\nCategories ({len(category_counts)} unique):\")\n",
    "    print(f\"{'Category':<25} {'Count':<10} {'Percentage':<10}\")\n",
    "    print(f\"{'-'*45}\")\n",
    "    \n",
    "    for category, count in category_counts.items():\n",
    "        percentage = count / len(df) * 100\n",
    "        print(f\"{category[:25]:<25} {count:<10} {percentage:.1f}%\")\n",
    "    \n",
    "    # Subcategory distribution\n",
    "    subcategory_counts = df['subcategory'].value_counts()\n",
    "    print(f\"\\nSubcategories ({len(subcategory_counts)} unique):\")\n",
    "    print(f\"{'Subcategory':<30} {'Category':<20} {'Count':<10} {'Percentage':<10}\")\n",
    "    print(f\"{'-'*70}\")\n",
    "    \n",
    "    # Create a mapping from subcategory to category for lookup\n",
    "    subcat_to_cat = df.groupby('subcategory')['category'].first().to_dict()\n",
    "    \n",
    "    for subcategory, count in subcategory_counts.items():\n",
    "        percentage = count / len(df) * 100\n",
    "        category = subcat_to_cat.get(subcategory, \"Unknown\")\n",
    "        print(f\"{subcategory[:30]:<30} {category[:20]:<20} {count:<10} {percentage:.1f}%\")\n",
    "    \n",
    "    # Find subcategories with low counts (potential data issues)\n",
    "    low_count_threshold = 5  # Adjust as needed\n",
    "    low_count_subcats = subcategory_counts[subcategory_counts < low_count_threshold]\n",
    "    if len(low_count_subcats) > 0:\n",
    "        print(f\"\\nSubcategories with low counts (<{low_count_threshold}):\")\n",
    "        for subcat, count in low_count_subcats.items():\n",
    "            print(f\"  - {subcat}: {count} transactions\")\n",
    "\n",
    "# Display sample of training data\n",
    "print(\"\\nSample of training data:\")\n",
    "print(df_train_sampled[['cleaned_note', 'category', 'subcategory']].head())\n",
    "\n",
    "# Print distribution statistics for both datasets\n",
    "print_distribution_stats(df_train_sampled, \"Training Data\")\n",
    "print_distribution_stats(df_test_sampled, \"Test Data\")\n",
    "\n",
    "# Additional summary statistics\n",
    "print(f\"\\n{'-'*50}\")\n",
    "print(f\"Summary Statistics:\")\n",
    "print(f\"{'-'*50}\")\n",
    "print(f\"Total transactions in original data: {len(df)}\")\n",
    "print(f\"Total transactions in training data: {len(df_train_sampled)}\")\n",
    "print(f\"Total transactions in test data: {len(df_test_sampled)}\")\n",
    "print(f\"Training data categories: {df_train_sampled['category'].nunique()}\")\n",
    "print(f\"Test data categories: {df_test_sampled['category'].nunique()}\")\n",
    "print(f\"Training data subcategories: {df_train_sampled['subcategory'].nunique()}\")\n",
    "print(f\"Test data subcategories: {df_test_sampled['subcategory'].nunique()}\")\n",
    "\n",
    "# Check for any subcategories in test but not in training\n",
    "train_subcats = set(df_train_sampled['subcategory'].unique())\n",
    "test_subcats = set(df_test_sampled['subcategory'].unique())\n",
    "test_only_subcats = test_subcats - train_subcats\n",
    "\n",
    "if test_only_subcats:\n",
    "    print(f\"\\nWarning: {len(test_only_subcats)} subcategories in test data but not in training data:\")\n",
    "    for subcat in test_only_subcats:\n",
    "        print(f\"  - {subcat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e386d07f",
   "metadata": {
    "papermill": {
     "duration": 0.007309,
     "end_time": "2025-04-18T06:47:34.302885",
     "exception": false,
     "start_time": "2025-04-18T06:47:34.295576",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Instruct the few-shot prompt\n",
    "I draft the prompt asking it to only use the subcategory from the loaded table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a1b521d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T06:47:34.319741Z",
     "iopub.status.busy": "2025-04-18T06:47:34.319384Z",
     "iopub.status.idle": "2025-04-18T06:47:34.324372Z",
     "shell.execute_reply": "2025-04-18T06:47:34.323423Z"
    },
    "papermill": {
     "duration": 0.015796,
     "end_time": "2025-04-18T06:47:34.326453",
     "exception": false,
     "start_time": "2025-04-18T06:47:34.310657",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    " import sqlite3\n",
    "\n",
    "db_file = \"transaction_categories.db\"\n",
    "db_conn = sqlite3.connect(db_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de551bc7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T06:47:34.343944Z",
     "iopub.status.busy": "2025-04-18T06:47:34.343475Z",
     "iopub.status.idle": "2025-04-18T06:47:34.356453Z",
     "shell.execute_reply": "2025-04-18T06:47:34.355309Z"
    },
    "papermill": {
     "duration": 0.024299,
     "end_time": "2025-04-18T06:47:34.358482",
     "exception": false,
     "start_time": "2025-04-18T06:47:34.334183",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "def predict_category_and_subcategory_few_shot(transaction_note):\n",
    "    \"\"\"\n",
    "    Predicts the category and subcategory for a transaction using few-shot prompting.\n",
    "    Ensures that the subcategory belongs to the selected category based on database mappings.\n",
    "    \n",
    "    Args:\n",
    "        transaction_note: The transaction text to classify\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (predicted_category, predicted_subcategory)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get the category-subcategory mapping\n",
    "        cursor = db_conn.cursor()\n",
    "        cursor.execute(\"\"\"\n",
    "        SELECT c.name as category, s.name as subcategory \n",
    "        FROM categories c\n",
    "        JOIN subcategories s ON c.category_id = s.category_id\n",
    "        ORDER BY c.name, s.name\n",
    "        \"\"\")\n",
    "        mappings = cursor.fetchall()\n",
    "        \n",
    "        # Create the hierarchy context\n",
    "        hierarchy_context = \"Category and subcategory hierarchy from the database:\\n\"\n",
    "        current_category = None\n",
    "        for category, subcategory in mappings:\n",
    "            if category != current_category:\n",
    "                hierarchy_context += f\"\\n{category}:\\n\"\n",
    "                current_category = category\n",
    "            hierarchy_context += f\"  - {subcategory}\\n\"\n",
    "        \n",
    "        # Create few-shot examples with clear category/subcategory relationships\n",
    "        examples = [\n",
    "            (\"AMAZON AUSYDNEY SOUTH Kindle Unlimited Subscription\", \"Life & Entertainment\", \"Books, audio, subscriptions\"),\n",
    "            (\"UBER EATS Melbourne AUS\", \"Food & Beverages\", \"Restaurant, fast-food\"),\n",
    "            (\"NETFLIX.COM SUBSCRIPTION\", \"Life & Entertainment\", \"TV, Streaming\"),\n",
    "            (\"COLES SUPERMARKET MELBOURNE\", \"Food & Beverages\", \"Groceries\"),\n",
    "            (\"TELSTRA MOBILE PAYMENT\", \"Communication, PC\", \"Telephony, mobile phone\")\n",
    "        ]\n",
    "            \n",
    "        # Build the few-shot examples part of the prompt\n",
    "        few_shot_examples = \"Here are some example transactions with their categories and subcategories:\\n\\n\"\n",
    "        for ex_note, ex_category, ex_subcategory in examples:\n",
    "            few_shot_examples += f\"Transaction: {ex_note}\\nCATEGORY: {ex_category}\\nSUBCATEGORY: {ex_subcategory}\\n\\n\"\n",
    "            \n",
    "        # System instruction with an emphasis on following the examples\n",
    "        system_instruct = \"\"\"\n",
    "        You are a financial transaction categorization assistant. You will analyze a transaction description and classify it into the appropriate category and subcategory.\n",
    "\n",
    "        Follow these steps exactly:\n",
    "        1. First, select the most appropriate CATEGORY from the available options\n",
    "        2. Then, select a SUBCATEGORY that belongs to the selected CATEGORY\n",
    "\n",
    "        Important: You must ensure the subcategory you select belongs to the category you chose. The database has specific parent-child relationships between categories and subcategories.\n",
    "\n",
    "        Study the provided examples carefully and follow the same pattern.\n",
    "\n",
    "        Your response must use this exact format:\n",
    "        CATEGORY: [selected category name]\n",
    "        SUBCATEGORY: [selected subcategory name]\n",
    "        \"\"\"\n",
    "        \n",
    "        # Make prediction with system instruction, hierarchy context, and few-shot examples\n",
    "        response = client.models.generate_content(\n",
    "            model=\"gemini-2.0-flash\",\n",
    "            config=types.GenerateContentConfig(\n",
    "                system_instruction=system_instruct,\n",
    "                temperature=0.2,  # Lower temperature for more consistent results\n",
    "            ),\n",
    "            contents=[\n",
    "                hierarchy_context,\n",
    "                few_shot_examples,\n",
    "                f\"Transaction description: {transaction_note}\\n\\nPlease categorize this transaction:\"\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        text = response.text.strip()\n",
    "        \n",
    "        # Extract category and subcategory\n",
    "        try:\n",
    "            category_line = [line for line in text.split('\\n') if line.startswith(\"CATEGORY:\")][0]\n",
    "            subcategory_line = [line for line in text.split('\\n') if line.startswith(\"SUBCATEGORY:\")][0]\n",
    "            \n",
    "            category = category_line.replace(\"CATEGORY:\", \"\").strip()\n",
    "            subcategory = subcategory_line.replace(\"SUBCATEGORY:\", \"\").strip()\n",
    "            \n",
    "            # Verify that the subcategory belongs to the category using the database\n",
    "            cursor.execute(\"\"\"\n",
    "            SELECT COUNT(*) FROM subcategories s\n",
    "            JOIN categories c ON s.category_id = c.category_id\n",
    "            WHERE c.name = ? AND s.name = ?\n",
    "            \"\"\", (category, subcategory))\n",
    "            \n",
    "            count = cursor.fetchone()[0]\n",
    "            if count == 0:\n",
    "                # If the model returned an invalid mapping, find a valid subcategory for the category\n",
    "                cursor.execute(\"\"\"\n",
    "                SELECT s.name FROM subcategories s\n",
    "                JOIN categories c ON s.category_id = c.category_id\n",
    "                WHERE c.name = ?\n",
    "                LIMIT 1\n",
    "                \"\"\", (category,))\n",
    "                \n",
    "                result = cursor.fetchone()\n",
    "                if result:\n",
    "                    # Use a valid subcategory for this category\n",
    "                    subcategory = result[0]\n",
    "                    return category, subcategory\n",
    "                else:\n",
    "                    # If category is invalid too, return error\n",
    "                    return \"(invalid category)\", \"(invalid subcategory)\"\n",
    "            \n",
    "            return category, subcategory\n",
    "            \n",
    "        except (IndexError, KeyError) as e:\n",
    "            return \"(parsing error)\", \"(parsing error)\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        return f\"(error)\", f\"(error: {str(e)})\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de5fea4",
   "metadata": {
    "papermill": {
     "duration": 0.007454,
     "end_time": "2025-04-18T06:47:34.373658",
     "exception": false,
     "start_time": "2025-04-18T06:47:34.366204",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Evaluate baseline performance\n",
    "\n",
    "Now I perform an evaluation on the available models to ensure I can measure how much the tuning helps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca9f5f18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T06:47:34.390580Z",
     "iopub.status.busy": "2025-04-18T06:47:34.390222Z",
     "iopub.status.idle": "2025-04-18T06:47:42.921998Z",
     "shell.execute_reply": "2025-04-18T06:47:42.920380Z"
    },
    "papermill": {
     "duration": 8.54329,
     "end_time": "2025-04-18T06:47:42.924596",
     "exception": false,
     "start_time": "2025-04-18T06:47:34.381306",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beafecc6905948ffb62fa1cee53913d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating 20 transactions...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category accuracy: 95.00%\n",
      "Subcategory accuracy: 60.00%\n",
      "Combined accuracy (both correct): 60.00%\n",
      "\n",
      "Sample predictions:\n",
      "Transaction: GOOGLE *DASHLANE G.CO/HELPPAY#...\n",
      "True category: Communication, PC\n",
      "Predicted category: Communication, PC\n",
      "Category correct: True\n",
      "True subcategory: Software, apps, games\n",
      "Predicted subcategory: Software, apps, games\n",
      "Subcategory correct: True\n",
      "\n",
      "Transaction: 1ST ENERGY PTY LTD SOUT SOUTHBANK...\n",
      "True category: Housing\n",
      "Predicted category: Housing\n",
      "Category correct: True\n",
      "True subcategory: Energy, utilities\n",
      "Predicted subcategory: Energy, utilities\n",
      "Subcategory correct: True\n",
      "\n",
      "Transaction: H & R BLOCK LIMITED THORNLEIGH AUSCard xx3780Value...\n",
      "True category: Financial expenses\n",
      "Predicted category: Financial expenses\n",
      "Category correct: True\n",
      "True subcategory: Advisory\n",
      "Predicted subcategory: Taxes\n",
      "Subcategory correct: False\n",
      "\n",
      "Transaction: Sent Hong Kong Federation of the Blind...\n",
      "True category: Shopping\n",
      "Predicted category: (invalid category)\n",
      "Category correct: False\n",
      "True subcategory: Gifts, joy\n",
      "Predicted subcategory: (invalid subcategory)\n",
      "Subcategory correct: False\n",
      "\n",
      "Transaction: AMAZON AUSYDNEY SOUTH CREDIT CARD PURCHASEOlaplex ...\n",
      "True category: Shopping\n",
      "Predicted category: Shopping\n",
      "Category correct: True\n",
      "True subcategory: Health and beauty\n",
      "Predicted subcategory: Health and beauty\n",
      "Subcategory correct: True\n",
      "\n",
      "Category confusion matrix:\n",
      "Predicted             (invalid category)  Communication, PC  \\\n",
      "True                                                          \n",
      "Communication, PC                      0                  3   \n",
      "Financial expenses                     0                  0   \n",
      "Food & Beverages                       0                  0   \n",
      "Housing                                0                  0   \n",
      "Life & Entertainment                   0                  0   \n",
      "Shopping                               1                  0   \n",
      "TRANSFER                               0                  0   \n",
      "\n",
      "Predicted             Financial expenses  Food & Beverages  Housing  \\\n",
      "True                                                                  \n",
      "Communication, PC                      0                 0        0   \n",
      "Financial expenses                     1                 0        0   \n",
      "Food & Beverages                       0                 2        0   \n",
      "Housing                                0                 0        2   \n",
      "Life & Entertainment                   0                 0        0   \n",
      "Shopping                               0                 0        0   \n",
      "TRANSFER                               0                 0        0   \n",
      "\n",
      "Predicted             Life & Entertainment  Shopping  TRANSFER  \n",
      "True                                                            \n",
      "Communication, PC                        0         0         0  \n",
      "Financial expenses                       0         0         0  \n",
      "Food & Beverages                         0         0         0  \n",
      "Housing                                  0         0         0  \n",
      "Life & Entertainment                     3         0         0  \n",
      "Shopping                                 0         7         0  \n",
      "TRANSFER                                 0         0         1  \n",
      "\n",
      "Most common subcategory error patterns:\n",
      "                  subcategory  predicted_subcategory  count\n",
      "12                   Advisory                  Taxes      1\n",
      "149              Home, garden           Leisure time      1\n",
      "79   Electronics, accessories           Leisure time      1\n",
      "120         Health and beauty    Drug-store, chemist      1\n",
      "105                Gifts, joy  (invalid subcategory)      1\n",
      "\n",
      "Error analysis by hierarchy:\n",
      "Correct category but wrong subcategory: 7 cases (35.00%)\n",
      "Wrong category: 1 cases (5.00%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17/2131392365.py:73: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  error_counts = error_patterns.groupby(['subcategory', 'predicted_subcategory']).size().reset_index(name='count')\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "from tqdm.rich import tqdm as tqdmr\n",
    "import warnings\n",
    "\n",
    "# Enable tqdm features on Pandas\n",
    "tqdmr.pandas()\n",
    "\n",
    "# Suppress the experimental warning\n",
    "warnings.filterwarnings(\"ignore\", category=tqdm.TqdmExperimentalWarning)\n",
    "\n",
    "# Further sample the test data to be mindful of the free-tier quota\n",
    "TEST_SAMPLE_SIZE = 20\n",
    "df_baseline_eval = df_test_sampled.sample(min(TEST_SAMPLE_SIZE, len(df_test_sampled)))\n",
    "\n",
    "# Ensure category column exists in test data\n",
    "if 'category' not in df_baseline_eval.columns:\n",
    "    # Add categories using the database mapping\n",
    "    cursor = db_conn.cursor()\n",
    "    cursor.execute(\"\"\"\n",
    "    SELECT s.name as subcategory, c.name as category \n",
    "    FROM subcategories s\n",
    "    JOIN categories c ON s.category_id = c.category_id\n",
    "    \"\"\")\n",
    "    subcat_to_cat = {row[0]: row[1] for row in cursor.fetchall()}\n",
    "    df_baseline_eval['category'] = df_baseline_eval['subcategory'].map(subcat_to_cat)\n",
    "\n",
    "print(f\"Evaluating {len(df_baseline_eval)} transactions...\")\n",
    "\n",
    "# Make predictions using the sampled data with progress bar\n",
    "# This will return both category and subcategory\n",
    "df_baseline_eval[['predicted_category', 'predicted_subcategory']] = df_baseline_eval['note'].progress_apply(\n",
    "    lambda x: pd.Series(predict_category_and_subcategory_few_shot(x))\n",
    ")\n",
    "\n",
    "# Calculate the accuracy for both category and subcategory\n",
    "category_accuracy = (df_baseline_eval['category'] == df_baseline_eval['predicted_category']).mean()\n",
    "subcategory_accuracy = (df_baseline_eval['subcategory'] == df_baseline_eval['predicted_subcategory']).mean()\n",
    "combined_accuracy = ((df_baseline_eval['category'] == df_baseline_eval['predicted_category']) & \n",
    "                     (df_baseline_eval['subcategory'] == df_baseline_eval['predicted_subcategory'])).mean()\n",
    "\n",
    "print(f\"Category accuracy: {category_accuracy:.2%}\")\n",
    "print(f\"Subcategory accuracy: {subcategory_accuracy:.2%}\")\n",
    "print(f\"Combined accuracy (both correct): {combined_accuracy:.2%}\")\n",
    "\n",
    "# Display some examples of predictions\n",
    "print(\"\\nSample predictions:\")\n",
    "sample_results = df_baseline_eval[['note', 'category', 'subcategory', \n",
    "                                  'predicted_category', 'predicted_subcategory']].sample(min(5, len(df_baseline_eval)))\n",
    "\n",
    "for idx, row in sample_results.iterrows():\n",
    "    print(f\"Transaction: {row['note'][:50]}...\")\n",
    "    print(f\"True category: {row['category']}\")\n",
    "    print(f\"Predicted category: {row['predicted_category']}\")\n",
    "    print(f\"Category correct: {row['category'] == row['predicted_category']}\")\n",
    "    print(f\"True subcategory: {row['subcategory']}\")\n",
    "    print(f\"Predicted subcategory: {row['predicted_subcategory']}\")\n",
    "    print(f\"Subcategory correct: {row['subcategory'] == row['predicted_subcategory']}\\n\")\n",
    "\n",
    "# Create a confusion matrix for categories\n",
    "print(\"Category confusion matrix:\")\n",
    "cat_matrix = pd.crosstab(\n",
    "    df_baseline_eval['category'], \n",
    "    df_baseline_eval['predicted_category'],\n",
    "    rownames=['True'], \n",
    "    colnames=['Predicted']\n",
    ")\n",
    "print(cat_matrix)\n",
    "\n",
    "# Create a confusion matrix for subcategories with errors\n",
    "print(\"\\nMost common subcategory error patterns:\")\n",
    "error_patterns = df_baseline_eval[df_baseline_eval['subcategory'] != df_baseline_eval['predicted_subcategory']]\n",
    "if len(error_patterns) > 0:\n",
    "    error_counts = error_patterns.groupby(['subcategory', 'predicted_subcategory']).size().reset_index(name='count')\n",
    "    error_counts = error_counts.sort_values('count', ascending=False)\n",
    "    print(error_counts.head(5))\n",
    "else:\n",
    "    print(\"No subcategory errors found in the evaluation set!\")\n",
    "\n",
    "# Analysis of hierarchical errors\n",
    "print(\"\\nError analysis by hierarchy:\")\n",
    "hierarchical_errors = df_baseline_eval[\n",
    "    (df_baseline_eval['category'] == df_baseline_eval['predicted_category']) & \n",
    "    (df_baseline_eval['subcategory'] != df_baseline_eval['predicted_subcategory'])\n",
    "]\n",
    "print(f\"Correct category but wrong subcategory: {len(hierarchical_errors)} cases ({len(hierarchical_errors)/len(df_baseline_eval):.2%})\")\n",
    "\n",
    "category_errors = df_baseline_eval[df_baseline_eval['category'] != df_baseline_eval['predicted_category']]\n",
    "print(f\"Wrong category: {len(category_errors)} cases ({len(category_errors)/len(df_baseline_eval):.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76ae6ba",
   "metadata": {
    "papermill": {
     "duration": 0.011184,
     "end_time": "2025-04-18T06:47:42.948862",
     "exception": false,
     "start_time": "2025-04-18T06:47:42.937678",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Train my model\n",
    "Here I train one model to learn how to set the category and train 11 models, one for each subcategory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f81e95f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T06:47:43.163213Z",
     "iopub.status.busy": "2025-04-18T06:47:43.162903Z",
     "iopub.status.idle": "2025-04-18T06:47:43.375026Z",
     "shell.execute_reply": "2025-04-18T06:47:43.373998Z"
    },
    "papermill": {
     "duration": 0.417122,
     "end_time": "2025-04-18T06:47:43.377344",
     "exception": false,
     "start_time": "2025-04-18T06:47:42.960222",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import time\n",
    "from google.api_core import retry\n",
    "from collections.abc import Iterable\n",
    "import pandas as pd\n",
    "from tqdm.rich import tqdm as tqdmr\n",
    "import warnings\n",
    "import tqdm\n",
    "\n",
    "# Path for storing model IDs\n",
    "MODEL_IDS_FILE = \"/kaggle/input/training/combined_model_id.json\"\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\", category=tqdm.TqdmExperimentalWarning)\n",
    "\n",
    "def prepare_combined_training_data(df):\n",
    "    \"\"\"\n",
    "    Prepare training data for fine-tuning with combined labels.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with transaction data\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with examples in the format required by the API\n",
    "    \"\"\"\n",
    "    # Prepare examples\n",
    "    training_examples = []\n",
    "    for _, row in df.iterrows():\n",
    "        training_examples.append({\n",
    "            \"textInput\": str(row['note']),\n",
    "            \"output\": str(row['combined_label'])\n",
    "        })\n",
    "    \n",
    "    print(f\"Created {len(training_examples)} training examples with combined labels\")\n",
    "    \n",
    "    return {\"examples\": training_examples}\n",
    "\n",
    "def get_model_ids():\n",
    "    \"\"\"Load model IDs from file or initialize empty structure\"\"\"\n",
    "    if os.path.exists(MODEL_IDS_FILE):\n",
    "        with open(MODEL_IDS_FILE, \"r\") as f:\n",
    "            return json.load(f)\n",
    "    return {\n",
    "        \"category_model\": None,\n",
    "        \"subcategory_models\": {}\n",
    "    }\n",
    "\n",
    "def save_model_ids(model_ids):\n",
    "    \"\"\"Save model IDs to file\"\"\"\n",
    "    with open(MODEL_IDS_FILE, \"w\") as f:\n",
    "        json.dump(model_ids, f, indent=2)\n",
    "\n",
    "def train_combined_model(df_train):\n",
    "    \"\"\"\n",
    "    Train a single model that predicts the combined \"Category/Subcategory\" label.\n",
    "    \n",
    "    Args:\n",
    "        df_train: DataFrame containing training data with 'combined_label' column\n",
    "        \n",
    "    Returns:\n",
    "        The model ID\n",
    "    \"\"\"\n",
    "    print(\"Starting combined model training process...\")\n",
    "    \n",
    "    # Ensure we have the combined label\n",
    "    if 'combined_label' not in df_train.columns:\n",
    "        if 'category' in df_train.columns and 'subcategory' in df_train.columns:\n",
    "            df_train['combined_label'] = df_train['category'] + \"/\" + df_train['subcategory']\n",
    "            print(\"Created combined labels from category and subcategory\")\n",
    "        else:\n",
    "            raise ValueError(\"Training data must include 'category' and 'subcategory' columns\")\n",
    "    \n",
    "    # Check if we already have a combined model\n",
    "    combined_model_id = None\n",
    "    model_file = \"/kaggle/input/training/combined_model_id.json\"\n",
    "    \n",
    "    try:\n",
    "        if os.path.exists(model_file):\n",
    "            with open(model_file, \"r\") as f:\n",
    "                saved_data = json.load(f)\n",
    "                combined_model_id = saved_data.get(\"combined_model\")\n",
    "                \n",
    "            if combined_model_id:\n",
    "                print(f\"Using existing combined model: {combined_model_id}\")\n",
    "                return combined_model_id\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading existing model ID: {e}\")\n",
    "    \n",
    "    # Prepare training data\n",
    "    training_data = prepare_combined_training_data(df_train)\n",
    "    \n",
    "    # Don't create model if we don't have enough examples\n",
    "    if len(training_data[\"examples\"]) < 5:\n",
    "        print(\"Skipping tuning due to insufficient data\")\n",
    "        return None\n",
    "    \n",
    "    # Start tuning\n",
    "    print(\"Starting new combined classification fine-tuning job\")\n",
    "    \n",
    "    # Create display name (must be under 40 chars)\n",
    "    display_name = f\"txn-combined-{datetime.datetime.now().strftime('%m%d')}\"\n",
    "    \n",
    "    try:\n",
    "        tuning_op = client.tunings.tune(\n",
    "            base_model=\"models/gemini-1.5-flash-001-tuning\",\n",
    "            training_dataset=training_data,\n",
    "            config=types.CreateTuningJobConfig(\n",
    "                tuned_model_display_name=display_name,\n",
    "                batch_size=8,\n",
    "                epoch_count=5,\n",
    "            ),\n",
    "        )\n",
    "        \n",
    "        combined_model_id = tuning_op.name\n",
    "        print(f\"Fine-tuning initiated. Model ID: {combined_model_id}\")\n",
    "        print(f\"Current status: {tuning_op.state}\")\n",
    "        \n",
    "        # Save model ID\n",
    "        with open(model_file, \"w\") as f:\n",
    "            json.dump({\"combined_model\": combined_model_id}, f, indent=2)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error starting tuning job: {e}\")\n",
    "        return None\n",
    "    \n",
    "    return combined_model_id\n",
    "\n",
    "# Get model status information\n",
    "def get_model_status(model_id):\n",
    "    if not model_id:\n",
    "        return {\"state\": \"NOT_FOUND\"}\n",
    "    \n",
    "    try:\n",
    "        model = client.tunings.get(name=model_id)\n",
    "        return {\n",
    "            \"state\": model.state.name,\n",
    "            \"success\": model.has_succeeded,\n",
    "            \"ended\": model.has_ended,\n",
    "            \"error\": model.error if hasattr(model, \"error\") else None,\n",
    "            \"create_time\": model.create_time,\n",
    "            \"progress\": model.progress if hasattr(model, \"progress\") else None,\n",
    "            \"model_id\": model_id\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"state\": \"ERROR\", \"error\": str(e), \"model_id\": model_id}\n",
    "        return {\"state\": \"ERROR\", \"error\": str(e)}\n",
    "\n",
    "\n",
    "\n",
    "def print_model_status(status_dict):\n",
    "    \"\"\"Pretty print model status information\"\"\"\n",
    "    state = status_dict.get(\"state\", \"UNKNOWN\")\n",
    "    create_time = status_dict.get(\"create_time\")\n",
    "    progress = status_dict.get(\"progress\")\n",
    "    model_id = status_dict.get(\"model_id\", \"Unknown\")\n",
    "    \n",
    "    print(f\"\\n=== COMBINED MODEL STATUS ===\")\n",
    "    print(f\"Model ID: {model_id}\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d39b2aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T06:47:43.395640Z",
     "iopub.status.busy": "2025-04-18T06:47:43.395076Z",
     "iopub.status.idle": "2025-04-18T06:47:43.404692Z",
     "shell.execute_reply": "2025-04-18T06:47:43.403675Z"
    },
    "papermill": {
     "duration": 0.020908,
     "end_time": "2025-04-18T06:47:43.406615",
     "exception": false,
     "start_time": "2025-04-18T06:47:43.385707",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting combined model training process...\n",
      "Using existing combined model: tunedModels/txncombined0418-5yec1xoakuo4\n",
      "Models a check_model_statuses()re now being trained. IDs saved to /kaggle/input/training/combined_model_id.json\n"
     ]
    }
   ],
   "source": [
    "model_ids = train_combined_model(df_train_sampled)\n",
    "print(f\"Models a check_model_statuses()re now being trained. IDs saved to {MODEL_IDS_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730e0e1d",
   "metadata": {
    "papermill": {
     "duration": 0.007985,
     "end_time": "2025-04-18T06:47:43.423514",
     "exception": false,
     "start_time": "2025-04-18T06:47:43.415529",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Monitoring progress\n",
    "Here I monitor whether this model has been tuned and ready to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "142d0453",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T06:47:43.442081Z",
     "iopub.status.busy": "2025-04-18T06:47:43.441677Z",
     "iopub.status.idle": "2025-04-18T06:47:43.742240Z",
     "shell.execute_reply": "2025-04-18T06:47:43.741178Z"
    },
    "papermill": {
     "duration": 0.312632,
     "end_time": "2025-04-18T06:47:43.744469",
     "exception": false,
     "start_time": "2025-04-18T06:47:43.431837",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'state': 'JOB_STATE_SUCCEEDED',\n",
       " 'success': True,\n",
       " 'ended': True,\n",
       " 'error': None,\n",
       " 'create_time': datetime.datetime(2025, 4, 18, 1, 45, 29, 731853, tzinfo=TzInfo(UTC)),\n",
       " 'progress': None,\n",
       " 'model_id': 'tunedModels/txncombined0418-5yec1xoakuo4'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_model_status(\"tunedModels/txncombined0418-5yec1xoakuo4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5370b812",
   "metadata": {
    "papermill": {
     "duration": 0.00825,
     "end_time": "2025-04-18T06:47:43.761418",
     "exception": false,
     "start_time": "2025-04-18T06:47:43.753168",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Evaluate Tuned Model\n",
    "Here I test and evaluate the performance of tuned models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f9efaf5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T06:47:43.781373Z",
     "iopub.status.busy": "2025-04-18T06:47:43.780886Z",
     "iopub.status.idle": "2025-04-18T06:47:43.797617Z",
     "shell.execute_reply": "2025-04-18T06:47:43.796561Z"
    },
    "papermill": {
     "duration": 0.029888,
     "end_time": "2025-04-18T06:47:43.799888",
     "exception": false,
     "start_time": "2025-04-18T06:47:43.770000",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_combined_model(df_test, model_id=None, sample_size=20):\n",
    "    \"\"\"\n",
    "    Evaluate the combined category/subcategory model\n",
    "    \n",
    "    Args:\n",
    "        df_test: Test DataFrame\n",
    "        model_id: Optional model ID to use\n",
    "        sample_size: Number of samples to evaluate\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with evaluation results\n",
    "    \"\"\"\n",
    "    # Get the model ID if not provided\n",
    "    if not model_id:\n",
    "        try:\n",
    "            with open(\"/kaggle/working/combined_model_id.json\", \"r\") as f:\n",
    "                model_id = json.load(f).get(\"combined_model\")\n",
    "                \n",
    "            if not model_id:\n",
    "                print(\"No combined model ID found. Please train a model first.\")\n",
    "                return None\n",
    "        except:\n",
    "            print(\"No combined model ID file found. Please train a model first.\")\n",
    "            return None\n",
    "    \n",
    "    # Sample the test data\n",
    "    if len(df_test) > sample_size:\n",
    "        eval_df = df_test.sample(sample_size)\n",
    "    else:\n",
    "        eval_df = df_test.copy()\n",
    "    \n",
    "    # Ensure we have the combined label\n",
    "    if 'combined_label' not in eval_df.columns:\n",
    "        eval_df['combined_label'] = eval_df['category'] + \"/\" + eval_df['subcategory']\n",
    "    \n",
    "    # Get valid combinations from the database\n",
    "    cursor = db_conn.cursor()\n",
    "    cursor.execute(\"\"\"\n",
    "    SELECT c.name || '/' || s.name as combined\n",
    "    FROM categories c\n",
    "    JOIN subcategories s ON c.category_id = s.category_id\n",
    "    \"\"\")\n",
    "    valid_combinations = [row[0] for row in cursor.fetchall()]\n",
    "    \n",
    "    # Make predictions\n",
    "    results = []\n",
    "    print(f\"Evaluating combined model on {len(eval_df)} test transactions...\")\n",
    "    \n",
    "    for idx, row in tqdmr(eval_df.iterrows(), total=len(eval_df)):\n",
    "        transaction = row['note']\n",
    "        true_combined = row['combined_label']\n",
    "        true_category, true_subcategory = true_combined.split('/', 1)\n",
    "        \n",
    "        try:\n",
    "            # Get prediction from model\n",
    "            response = client.models.generate_content(\n",
    "                model=model_id,\n",
    "                contents=transaction,\n",
    "                config=types.GenerateContentConfig(\n",
    "                    temperature=0.0,\n",
    "                    max_output_tokens=20\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            if response.candidates and response.candidates[0].content:\n",
    "                pred_combined = response.candidates[0].content.parts[0].text.strip()\n",
    "                \n",
    "                # Validate prediction against database\n",
    "                if pred_combined not in valid_combinations:\n",
    "                    # Try to find closest match\n",
    "                    matches = get_close_matches(pred_combined, valid_combinations, n=1, cutoff=0.6)\n",
    "                    if matches:\n",
    "                        print(f\"Corrected prediction: {pred_combined} → {matches[0]}\")\n",
    "                        pred_combined = matches[0]\n",
    "                    else:\n",
    "                        print(f\"Invalid prediction: {pred_combined} (no close match found)\")\n",
    "                        pred_combined = \"(invalid prediction)\"\n",
    "                \n",
    "                # Split prediction into category and subcategory\n",
    "                if \"/\" in pred_combined:\n",
    "                    pred_category, pred_subcategory = pred_combined.split('/', 1)\n",
    "                else:\n",
    "                    pred_category, pred_subcategory = pred_combined, \"(parsing error)\"\n",
    "            else:\n",
    "                pred_combined = \"(no response)\"\n",
    "                pred_category, pred_subcategory = \"(no response)\", \"(no response)\"\n",
    "                \n",
    "        except Exception as e:\n",
    "            error_message = str(e)\n",
    "            pred_combined = f\"(error: {error_message[:30]}...)\"\n",
    "            pred_category, pred_subcategory = \"(error)\", \"(error)\"\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            'transaction': transaction,\n",
    "            'true_combined': true_combined,\n",
    "            'pred_combined': pred_combined,\n",
    "            'combined_correct': true_combined == pred_combined,\n",
    "            'true_category': true_category,\n",
    "            'pred_category': pred_category,\n",
    "            'category_correct': true_category == pred_category,\n",
    "            'true_subcategory': true_subcategory, \n",
    "            'pred_subcategory': pred_subcategory,\n",
    "            'subcategory_correct': true_subcategory == pred_subcategory\n",
    "        })\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    combined_accuracy = results_df['combined_correct'].mean()\n",
    "    category_accuracy = results_df['category_correct'].mean()\n",
    "    subcategory_accuracy = results_df['subcategory_correct'].mean()\n",
    "    \n",
    "    print(f\"Combined label accuracy: {combined_accuracy:.2%}\")\n",
    "    print(f\"Category accuracy: {category_accuracy:.2%}\")\n",
    "    print(f\"Subcategory accuracy: {subcategory_accuracy:.2%}\")\n",
    "    \n",
    "    # Show example predictions\n",
    "    print(\"\\nSample predictions:\")\n",
    "    for idx, row in results_df.sample(min(5, len(results_df))).iterrows():\n",
    "        print(f\"Transaction: {row['transaction'][:50]}...\")\n",
    "        print(f\"True: {row['true_combined']}\")\n",
    "        print(f\"Predicted: {row['pred_combined']}\")\n",
    "        print(f\"Correct: {row['combined_correct']}\\n\")\n",
    "    \n",
    "    # Display confusion matrix for categories (if enough data)\n",
    "    if len(results_df) >= 5:\n",
    "        print(\"\\nCategory confusion matrix:\")\n",
    "        try:\n",
    "            cat_matrix = pd.crosstab(\n",
    "                results_df['true_category'], \n",
    "                results_df['pred_category'],\n",
    "                rownames=['True'], \n",
    "                colnames=['Predicted']\n",
    "            )\n",
    "            print(cat_matrix)\n",
    "        except Exception as e:\n",
    "            print(f\"Unable to generate confusion matrix: {e}\")\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7401edd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T06:47:43.819778Z",
     "iopub.status.busy": "2025-04-18T06:47:43.819381Z",
     "iopub.status.idle": "2025-04-18T06:48:00.750144Z",
     "shell.execute_reply": "2025-04-18T06:48:00.749090Z"
    },
    "papermill": {
     "duration": 16.944317,
     "end_time": "2025-04-18T06:48:00.752781",
     "exception": false,
     "start_time": "2025-04-18T06:47:43.808464",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing transaction data...\n",
      "Cleaning transaction notes...\n",
      "Adding category mapping...\n",
      "Warning: 377 rows have unmapped subcategories\n",
      "Unmapped subcategories: ['Clothes & shoes' 'Financial expenses' 'Food & Drinks' 'Free time'\n",
      " 'Housing' 'Life & Entertainment' 'Loan, interests' 'Missing' 'Others'\n",
      " 'Phone, cell phone' 'Shopping' 'Transportation']\n",
      "Added categories to 12388 transactions\n",
      "Creating combined labels...\n",
      "Filtering out unmapped subcategories...\n",
      "Filtered out 377 rows with unmapped subcategories\n",
      "Unmapped subcategories: ['Clothes & shoes' 'Financial expenses' 'Food & Drinks' 'Free time'\n",
      " 'Housing' 'Life & Entertainment' 'Loan, interests' 'Missing' 'Others'\n",
      " 'Phone, cell phone' 'Shopping' 'Transportation']\n",
      "Creating balanced samples (50 per subcategory for training)...\n",
      "Creating balanced samples by combined label...\n",
      "Added 0 additional rows from combined label sampling\n",
      "Saved sample data to /kaggle/working/transaction_sample_review.csv for review\n",
      "Original data: 12388 transactions\n",
      "Filtered data: 12011 transactions\n",
      "Unique combined labels: 52\n",
      "Balanced training data: 1467 transactions\n",
      "Balanced test data: 311 transactions\n",
      "\n",
      "Training combined model...\n",
      "Starting combined model training process...\n",
      "Using existing combined model: tunedModels/txncombined0418-5yec1xoakuo4\n",
      "\n",
      "Combined model ID: tunedModels/txncombined0418-5yec1xoakuo4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17/4103278480.py:47: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  train_sampled = pd.concat([train_sampled, new_train_rows])\n",
      "/tmp/ipykernel_17/4103278480.py:48: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  test_sampled = pd.concat([test_sampled, new_test_rows])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb0e0a46c132443b8a2fdc8e363417db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model state: JOB_STATE_SUCCEEDED\n",
      "\n",
      "Evaluating combined model...\n",
      "Evaluating combined model on 20 test transactions...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined label accuracy: 60.00%\n",
      "Category accuracy: 75.00%\n",
      "Subcategory accuracy: 60.00%\n",
      "\n",
      "Sample predictions:\n",
      "Transaction: Direct Debit 063899 LITTLE REAL ESTAOC1220365CT140...\n",
      "True: Housing/Rent\n",
      "Predicted: Housing/Rent\n",
      "Correct: True\n",
      "\n",
      "Transaction: Nikon Sportstar EX 10x25 D CF Binoculars...\n",
      "True: Shopping/Stationery, tools\n",
      "Predicted: Shopping/Electronics, accessories\n",
      "Correct: False\n",
      "\n",
      "Transaction: URBAN MAN 05FEB20 ATMA896 07:29:41 5055 VISAAUD UR...\n",
      "True: Life & Entertainment/Wellness, beauty\n",
      "Predicted: Financial expenses/Charges, Fees\n",
      "Correct: False\n",
      "\n",
      "Transaction: OLACABS MELBOURNE...\n",
      "True: Transportation/Taxi\n",
      "Predicted: Transportation/Taxi\n",
      "Correct: True\n",
      "\n",
      "Transaction: Officeworks 0336 Ringw Ringwood AU...\n",
      "True: Shopping/Stationery, tools\n",
      "Predicted: Shopping/Stationery, tools\n",
      "Correct: True\n",
      "\n",
      "\n",
      "Category confusion matrix:\n",
      "Predicted             (error)  Communication, PC  Financial expenses  \\\n",
      "True                                                                   \n",
      "Communication, PC           0                  1                   0   \n",
      "Food & Beverages            0                  0                   0   \n",
      "Housing                     0                  0                   0   \n",
      "Income                      0                  0                   0   \n",
      "Investments                 1                  0                   0   \n",
      "Life & Entertainment        1                  0                   1   \n",
      "Shopping                    0                  0                   0   \n",
      "Transportation              0                  0                   0   \n",
      "\n",
      "Predicted             Food & Beverages  Housing  Income  Life & Entertainment  \\\n",
      "True                                                                            \n",
      "Communication, PC                    0        0       0                     0   \n",
      "Food & Beverages                     2        0       0                     0   \n",
      "Housing                              0        5       0                     0   \n",
      "Income                               0        0       1                     0   \n",
      "Investments                          0        0       0                     0   \n",
      "Life & Entertainment                 0        0       0                     2   \n",
      "Shopping                             0        0       0                     0   \n",
      "Transportation                       0        0       0                     0   \n",
      "\n",
      "Predicted             Shopping  TRANSFER  Transportation  Vehicle  \n",
      "True                                                               \n",
      "Communication, PC            0         0               0        0  \n",
      "Food & Beverages             0         0               0        0  \n",
      "Housing                      0         0               0        0  \n",
      "Income                       0         0               0        0  \n",
      "Investments                  0         0               0        0  \n",
      "Life & Entertainment         0         1               0        1  \n",
      "Shopping                     3         0               0        0  \n",
      "Transportation               0         0               1        0  \n"
     ]
    }
   ],
   "source": [
    "def run_combined_model_workflow():\n",
    "    \"\"\"Run the complete workflow for the combined model approach\"\"\"\n",
    "    # Step 1: Process the data with combined labels\n",
    "    print(\"Processing transaction data...\")\n",
    "    df_train_sampled, df_test_sampled, df_categorized = process_transaction_data_combined(\n",
    "        df, \n",
    "        db_conn,\n",
    "        train_samples=50, \n",
    "        test_samples=10,\n",
    "        sample_csv_path=\"/kaggle/working/transaction_sample_review.csv\"\n",
    "    )\n",
    "    \n",
    "    # Step 2: Train the combined model\n",
    "    print(\"\\nTraining combined model...\")\n",
    "    combined_model_id = train_combined_model(df_train_sampled)\n",
    "    \n",
    "    # Step 3: Check model status\n",
    "    if combined_model_id:\n",
    "        print(f\"\\nCombined model ID: {combined_model_id}\")\n",
    "        try:\n",
    "            model = client.tunings.get(name=combined_model_id)\n",
    "            print(f\"Model state: {model.state.name}\")\n",
    "            \n",
    "            if model.has_succeeded:\n",
    "                # Step 4: Evaluate the model if it's ready\n",
    "                print(\"\\nEvaluating combined model...\")\n",
    "                results = evaluate_combined_model(df_test_sampled, combined_model_id, sample_size=20)\n",
    "                return results\n",
    "            else:\n",
    "                print(\"Model is not ready for evaluation yet. Check back later.\")\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            print(f\"Error checking model status: {e}\")\n",
    "            return None\n",
    "    else:\n",
    "        print(\"No model was created. Please check the errors above.\")\n",
    "        return None\n",
    "\n",
    "# Run the workflow\n",
    "results = run_combined_model_workflow()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "day-4-fine-tuning-a-custom-model.ipynb",
   "toc_visible": true
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7129309,
     "sourceId": 11456905,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 53.923319,
   "end_time": "2025-04-18T06:48:02.069332",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-18T06:47:08.146013",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "8be5ff78b1d440439dc8392faf0d2787": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b228bee4291447299323a1d22ab8c041": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "beafecc6905948ffb62fa1cee53913d8": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/output",
       "_model_module_version": "1.0.0",
       "_model_name": "OutputModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/output",
       "_view_module_version": "1.0.0",
       "_view_name": "OutputView",
       "layout": "IPY_MODEL_b228bee4291447299323a1d22ab8c041",
       "msg_id": "",
       "outputs": [
        {
         "data": {
          "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080\"> 100%</span> <span style=\"color: #729c1f; text-decoration-color: #729c1f\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #008000; text-decoration-color: #008000\">20/20 </span> [ <span style=\"color: #808000; text-decoration-color: #808000\">0:00:07</span> &lt; <span style=\"color: #008080; text-decoration-color: #008080\">0:00:00</span> , <span style=\"color: #800000; text-decoration-color: #800000\">2 it/s</span> ]\n</pre>\n",
          "text/plain": "\u001b[35m 100%\u001b[0m \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20/20 \u001b[0m [ \u001b[33m0:00:07\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m2 it/s\u001b[0m ]\n"
         },
         "metadata": {},
         "output_type": "display_data"
        }
       ]
      }
     },
     "eb0e0a46c132443b8a2fdc8e363417db": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/output",
       "_model_module_version": "1.0.0",
       "_model_name": "OutputModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/output",
       "_view_module_version": "1.0.0",
       "_view_name": "OutputView",
       "layout": "IPY_MODEL_8be5ff78b1d440439dc8392faf0d2787",
       "msg_id": "",
       "outputs": [
        {
         "data": {
          "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080\">  95%</span> <span style=\"color: #f92672; text-decoration-color: #f92672\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span><span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">╺━━━</span> <span style=\"color: #008000; text-decoration-color: #008000\">19/20 </span> [ <span style=\"color: #808000; text-decoration-color: #808000\">0:00:15</span> &lt; <span style=\"color: #008080; text-decoration-color: #008080\">0:00:01</span> , <span style=\"color: #800000; text-decoration-color: #800000\">1 it/s</span> ]\n</pre>\n",
          "text/plain": "\u001b[35m  95%\u001b[0m \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━\u001b[0m \u001b[32m19/20 \u001b[0m [ \u001b[33m0:00:15\u001b[0m < \u001b[36m0:00:01\u001b[0m , \u001b[31m1 it/s\u001b[0m ]\n"
         },
         "metadata": {},
         "output_type": "display_data"
        }
       ]
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
