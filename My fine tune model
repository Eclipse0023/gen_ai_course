{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"colab":{"name":"day-4-fine-tuning-a-custom-model.ipynb","toc_visible":true},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11389205,"sourceType":"datasetVersion","datasetId":7129309}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Categorise finance transactions\n\nIn life, my financial transactions are often categorised incorrectly in my budgeting app. I decided to find a better solution.\n\nIn this example, I will first try to categorise with an existing Gemini model using a zero-shot prompt and evaluate its performance. Then I will tune a model with the data categorised by me and evaluate its performance.","metadata":{"id":"4KDIFPAL2EnL"}},{"cell_type":"code","source":"# Install required libraries\n!pip uninstall -qqy jupyterlab  # Remove unused conflicting packages\n!pip install -U -q \"google-genai==1.7.0\"","metadata":{"execution":{"iopub.status.busy":"2025-04-13T21:23:39.282017Z","iopub.execute_input":"2025-04-13T21:23:39.282818Z","iopub.status.idle":"2025-04-13T21:23:50.482430Z","shell.execute_reply.started":"2025-04-13T21:23:39.282778Z","shell.execute_reply":"2025-04-13T21:23:50.481027Z"},"id":"9wafTyEH1_xF","trusted":true},"outputs":[],"execution_count":28},{"cell_type":"code","source":"# Import necessary libraries\nfrom google import genai\nfrom google.genai import types\n\ngenai.__version__","metadata":{"id":"T0CBG9xL2PvT","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T21:23:50.484467Z","iopub.execute_input":"2025-04-13T21:23:50.484829Z","iopub.status.idle":"2025-04-13T21:23:50.492377Z","shell.execute_reply.started":"2025-04-13T21:23:50.484792Z","shell.execute_reply":"2025-04-13T21:23:50.491396Z"}},"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"'1.7.0'"},"metadata":{}}],"execution_count":29},{"cell_type":"code","source":"# Set up the Google GenAI client\nfrom kaggle_secrets import UserSecretsClient\n\nGOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n\nclient = genai.Client(api_key=GOOGLE_API_KEY)","metadata":{"id":"VuJPY3GK2SLZ","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T21:23:50.493387Z","iopub.execute_input":"2025-04-13T21:23:50.493680Z","iopub.status.idle":"2025-04-13T21:23:50.790552Z","shell.execute_reply.started":"2025-04-13T21:23:50.493650Z","shell.execute_reply":"2025-04-13T21:23:50.789497Z"}},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":"## Load the subcategory and category table\nIn this step, I load the subcategory and category table.","metadata":{}},{"cell_type":"markdown","source":"## Load the subcategory and category table\nIn this step, I load the subcategory, category table.","metadata":{"execution":{"iopub.status.busy":"2025-04-13T12:10:19.728661Z","iopub.execute_input":"2025-04-13T12:10:19.729115Z","iopub.status.idle":"2025-04-13T12:10:19.736220Z","shell.execute_reply.started":"2025-04-13T12:10:19.729076Z","shell.execute_reply":"2025-04-13T12:10:19.734777Z"}}},{"cell_type":"code","source":"import sqlite3\n\n# Connect to your database\ndb_conn = sqlite3.connect('/kaggle/working/transaction_categories.db')\ncursor = db_conn.cursor()\n\n# Create the tables\ncursor.execute('''\nCREATE TABLE IF NOT EXISTS categories (\n    category_id INTEGER PRIMARY KEY AUTOINCREMENT,\n    name VARCHAR(100) NOT NULL UNIQUE,\n    description TEXT,\n    display_order INT DEFAULT 100,\n    is_active BOOLEAN DEFAULT 1,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n)\n''')\n\ncursor.execute('''\nCREATE TABLE IF NOT EXISTS subcategories (\n    subcategory_id INTEGER PRIMARY KEY AUTOINCREMENT,\n    category_id INTEGER NOT NULL,\n    name VARCHAR(100) NOT NULL,\n    description TEXT,\n    display_order INT DEFAULT 100,\n    is_active BOOLEAN DEFAULT 1,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    FOREIGN KEY (category_id) REFERENCES categories(category_id)\n)\n''')\n\n\n# Insert main categories\ncategories = [\n    ('Food & Beverages', 'Expenses related to food and drinks', 10),\n    ('Shopping', 'Retail purchases and shopping expenses', 20),\n    ('Housing', 'Home-related expenses including rent and utilities', 30),\n    ('Transportation', 'Public and private transportation costs', 40),\n    ('Vehicle', 'Car and vehicle related expenses', 50),\n    ('Life & Entertainment', 'Leisure activities and entertainment', 60),\n    ('Communication, PC', 'Internet, phone and computer expenses', 70),\n    ('Financial expenses', 'Banking fees, loans, and financial costs', 80),\n    ('Investments', 'Investment-related transactions', 90),\n    ('Income', 'All sources of incoming money', 100),\n    ('TRANSFER', 'Money transfers between accounts', 110)\n]\n\ncursor.executemany('INSERT OR IGNORE INTO categories (name, description, display_order) VALUES (?, ?, ?)', categories)\n\n# Insert subcategories for Food & Beverages\nfood_subcategories = [\n    (1, 'Bar, cafe, drink, snacks', 10),\n    (1, 'Groceries', 20),\n    (1, 'Restaurant, fast-food', 30)\n]\ncursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', food_subcategories)\n\n# Insert subcategories for Shopping\nshopping_subcategories = [\n    (2, 'Clothes & Footwear', 10),\n    (2, 'Drug-store, chemist', 20),\n    (2, 'Electronics, accessories', 30),\n    (2, 'Gifts, joy', 40),\n    (2, 'Health and beauty', 50),\n    (2, 'Home, garden', 60),\n    (2, 'Jewels, accessories', 70),\n    (2, 'Kids', 80),\n    (2, 'Leisure time', 90),\n    (2, 'Pets, animals', 100),\n    (2, 'Stationery, tools', 110)\n]\ncursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', shopping_subcategories)\n\nhousing_subcategories=[\n    (3, 'Energy, utilities', 10),\n    (3, 'Maintenance, repairs', 20),\n    (3, 'Mortgage', 30),\n    (3, 'Property insurance', 40),\n    (3, 'Rent', 50),\n    (3, 'Services', 60)\n]\ncursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', housing_subcategories)\n\n\ntransportation_subcategories=[\n    (4, 'Business trips', 10),\n    (4, 'Long distance', 20),\n    (4, 'Public transport', 30),\n    (4, 'Taxi', 40)\n]\ncursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', transportation_subcategories)\n\nvehicle_subcategories=[\n    (5, 'Fuel', 10),\n    (5, 'Leasing', 20),\n    (5, 'Parking', 30),\n    (5, 'Rentals', 40),\n    (5, 'Vehicle insurance', 50),\n    (5, 'Vehicle maintenance', 60)\n]\ncursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', vehicle_subcategories)\n\nlife_subcategories=[\n    (6, 'Active sport, fitness', 10),\n    (6, 'Alcohol, tobacco', 20),\n    (6, 'Books, audio, subscriptions', 30),\n    (6, 'Charity, gifts', 40),\n    (6, 'Culture, sport events', 50),\n    (6, 'Education, development', 60),\n    (6, 'Health care, doctor', 70),\n    (6, 'Hobbies', 80),\n    (6, 'Holiday, trips, hotels', 90),\n    (6, 'Life events', 100),\n    (6, 'Lottery, gambling', 110),\n    (6, 'TV, Streaming', 120),\n    (6, 'Wellness, beauty', 130)\n]\ncursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', life_subcategories)\n\ncommunication_subcategories=[\n    (7, 'Internet', 10),\n    (7, 'Postal services', 20),\n    (7, 'Software, apps, games', 30),\n    (7, 'Telephony, mobile phone', 40)\n]\ncursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', communication_subcategories)\n\nfinancial_subcategories=[\n    (8, 'Advisory', 10),\n    (8, 'Charges, Fees', 20),\n    (8, 'Child Support', 30),\n    (8, 'Fines', 40),\n    (8, 'Insurances', 50),\n    (8, 'Loans, interests', 60),\n    (8, 'Taxes', 70)\n]\ncursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', financial_subcategories)\n\ninvestments_subcategories=[\n    (9, 'Collections', 10),\n    (9, 'Financial investments', 20),\n    (9, 'Realty', 30),\n    (9, 'Savings', 40),\n    (9, 'Vehicles, chattels', 50)\n]\ncursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', investments_subcategories)\n\nincome_subcategories=[\n    (10, 'Checks, coupons', 10),\n    (10, 'Child Support', 20),\n    (10, 'Dues & grants', 30),\n    (10, 'Gifts', 40),\n    (10, 'Interests, dividends', 50),\n    (10, 'Lending, renting', 60),\n    (10, 'Lottery earning', 70),\n    (10, 'Refunds (tax, purchase)', 80),\n    (10, 'Rental income', 90),\n    (10, 'Sale', 100),\n    (10, 'Wage, invoices', 110)\n]\ncursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', income_subcategories)\n\ntransfer_subcatgories=[\n    (11, 'TRANSFER', 10),   \n]\ncursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', transfer_subcatgories)\n\n# Commit the changes\ndb_conn.commit()\nprint(\"Database schema created successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T21:23:50.803962Z","iopub.execute_input":"2025-04-13T21:23:50.804377Z","iopub.status.idle":"2025-04-13T21:23:50.835199Z","shell.execute_reply.started":"2025-04-13T21:23:50.804341Z","shell.execute_reply":"2025-04-13T21:23:50.834251Z"}},"outputs":[{"name":"stdout","text":"Database schema created successfully!\n","output_type":"stream"}],"execution_count":31},{"cell_type":"markdown","source":"In this step, I create the mapping beween category and subcategory","metadata":{}},{"cell_type":"code","source":"def setup_database_and_get_hierarchy(output_path=\"/kaggle/working/category_mapping.csv\"):\n    \"\"\"\n    Initialize database, return category hierarchy, and output a simple mapping CSV.\n    \n    Args:\n        output_path: Path to save the mapping CSV\n        \n    Returns:\n        tuple: (db_connection, category_hierarchy_dict, subcategory_to_category_mapping)\n    \"\"\"\n    import sqlite3\n    import pandas as pd\n    \n    print(\"Setting up database and extracting category hierarchy...\")\n    \n    # Create database connection\n    db_conn = sqlite3.connect('/kaggle/working/transaction_categories.db')\n    cursor = db_conn.cursor()\n    \n    # Create tables and populate data if needed (your existing code)\n    # ... (Keep your existing table creation and population code)\n    \n    # Get complete hierarchy in one operation\n    cursor.execute(\"\"\"\n    SELECT \n        c.name as category, \n        s.name as subcategory\n    FROM categories c\n    JOIN subcategories s ON c.category_id = s.category_id\n    ORDER BY c.display_order, s.display_order\n    \"\"\")\n    \n    # Convert query results to DataFrame\n    results = cursor.fetchall()\n    mapping_df = pd.DataFrame(results, columns=['category', 'subcategory'])\n    \n    # Process results into usable format for return values\n    category_hierarchy = {}\n    subcat_to_cat_mapping = {}\n    \n    for category, subcategory in results:\n        # Build hierarchy dictionary\n        if category not in category_hierarchy:\n            category_hierarchy[category] = []\n        category_hierarchy[category].append(subcategory)\n        \n        # Build mapping dictionary\n        subcat_to_cat_mapping[subcategory] = category\n    \n    # Save to CSV file\n    mapping_df.to_csv(output_path, index=False)\n    \n    # Print summary\n    print(f\"\\nCategory-subcategory mapping saved to {output_path}\")\n    print(f\"Found {len(mapping_df['category'].unique())} categories and {len(mapping_df)} subcategories\")\n    print(\"\\nSample of mapping:\")\n    print(mapping_df.head(5))\n    \n    db_conn.commit()\n    \n    return db_conn, category_hierarchy, subcat_to_cat_mapping","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T21:23:50.836524Z","iopub.execute_input":"2025-04-13T21:23:50.836862Z","iopub.status.idle":"2025-04-13T21:23:50.844654Z","shell.execute_reply.started":"2025-04-13T21:23:50.836831Z","shell.execute_reply":"2025-04-13T21:23:50.843655Z"}},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":"## Load the dataset\n\nI have uploaded transaction data categorised by me. Then I group it into training data and test data.","metadata":{"id":"peFm0w_0c1CO"}},{"cell_type":"code","source":"# Load and preprocess transaction data\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Load your data\nfile_path = \"/kaggle/input/training/categorized_transaction.csv\"\ndf = pd.read_csv(file_path)\n\n# Split into train and test sets (80/20 split)\ndf_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n\n# Display the subcategories (labels) in your dataset\nsubcategories = df['subcategory'].unique()\nprint(f\"Number of subcategories: {len(subcategories)}\")\nprint(\"Sample subcategories:\", subcategories[:10])  # Show first 10 subcategories\n\n# Quick look at note examples\nprint(\"\\nSample notes:\")\nfor i, note in enumerate(df['note'].head(3)):\n    print(f\"{i+1}. {note} → {df['subcategory'].iloc[i]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T21:23:50.845813Z","iopub.execute_input":"2025-04-13T21:23:50.846110Z","iopub.status.idle":"2025-04-13T21:23:50.908321Z","shell.execute_reply.started":"2025-04-13T21:23:50.846082Z","shell.execute_reply":"2025-04-13T21:23:50.907404Z"}},"outputs":[{"name":"stdout","text":"Number of subcategories: 64\nSample subcategories: ['Active sport, fitness' 'Advisory' 'Alcohol, tobacco'\n 'Bar, cafe, drink, snacks' 'Books, audio, subscriptions' 'Charges, Fees'\n 'Charity, gifts' 'Checks, coupons' 'Clothes & shoes'\n 'Culture, sport events']\n\nSample notes:\n1. AMAZON AUSYDNEY SOUTH CREDIT CARD PURCHASEAmazon Basics High-Density Round Foam Roller for Exercise and Recovery - 61cm, Blue Speckled → Active sport, fitness\n2. 02 DEC 20 - $98.00 LULULEMON ATHLETICA AUSTRAlbert Park [Eff Date: 30 NOV 20] → Active sport, fitness\n3. REBEL MELBOURNE CTRL MELBOURNE VI AUSTap and Pay xx3173Value Date_ 17/03/2018 → Active sport, fitness\n","output_type":"stream"}],"execution_count":33},{"cell_type":"markdown","source":"## Clean the data","metadata":{}},{"cell_type":"code","source":"import re\nimport pandas as pd\n\ndef clean_transaction_note(note):\n    \"\"\"\n    Clean transaction notes to remove common bank formatting, dates, card numbers, etc.\n    \"\"\"\n    # Handle None or empty strings\n    if note is None or pd.isna(note) or note == \"\":\n        return \"\"\n    \n    # Convert to string if needed\n    text = str(note)\n    \n    # Replace non-ASCII characters\n    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n    \n    # Extract main part of the transaction (before common transaction markers)\n    transaction_markers = r'\\s+(?:CREDIT CARD PURCHASE|EFTPOS|Value Date|tap and Pay|Card Purchase|CARD PURCHASE)'\n    parts = re.split(transaction_markers, text, flags=re.IGNORECASE)\n    main_text = parts[0] if parts else text\n    \n    # Clean amount figures and currency symbols\n    main_text = re.sub(r'(?:[$€£¥]|AUD|USD|EUR|GBP|NZD)\\s*[-+]?(?:\\d{1,3}(?:,\\d{3})*|\\d+)(?:\\.\\d{1,2})?', '', main_text)\n    main_text = re.sub(r'\\b[-+]?(?:\\d{1,3}(?:,\\d{3})*|\\d+)(?:\\.\\d{1,2})?\\b', '', main_text)\n    \n    # Remove card numbers (masked or full)\n    main_text = re.sub(r'(?:x{2,4}|X{2,4})\\d{4}|\\b\\d{4}[\\s-]?\\d{4}[\\s-]?\\d{4}[\\s-]?\\d{4}\\b', '', main_text)\n    \n    # Remove date patterns\n    date_pattern = r'(?:\\d{1,2}\\s+(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\\s+\\d{2,4}|' + \\\n                   r'(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\\s+\\d{1,2}(?:st|nd|rd|th)?\\s+\\d{2,4}|' + \\\n                   r'\\d{1,2}[-/.]\\d{1,2}[-/.]\\d{2,4}|' + \\\n                   r'\\d{4}[-/.]\\d{1,2}[-/.]\\d{1,2}|' + \\\n                   r'\\[Eff\\s+Date:.*?\\]|' + \\\n                   r'Value\\s+Date[_:]\\s*\\d{1,2}[-/.]\\d{1,2}[-/.]\\d{2,4})'\n    main_text = re.sub(date_pattern, '', main_text, flags=re.IGNORECASE)\n    \n    # Clean whitespace and punctuation\n    main_text = re.sub(r'\\s+', ' ', main_text)\n    main_text = re.sub(r'[\\s,.-]+$|^[\\s,.-]+', '', main_text)\n    main_text = re.sub(r'\\s+([,.])', r'\\1', main_text)\n    \n    return main_text.strip()[:500]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T21:23:50.910061Z","iopub.execute_input":"2025-04-13T21:23:50.910662Z","iopub.status.idle":"2025-04-13T21:23:50.920408Z","shell.execute_reply.started":"2025-04-13T21:23:50.910582Z","shell.execute_reply":"2025-04-13T21:23:50.919416Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"def add_category_column(df, db_conn):\n    \"\"\"\n    Add category column to DataFrame based on subcategory using database mapping.\n    \"\"\"\n    if 'category' in df.columns:\n        print(\"Category column already exists\")\n        return df\n    \n    try:\n        # Query the database for subcategory to category mapping\n        cursor = db_conn.cursor()\n        cursor.execute(\"\"\"\n        SELECT s.name as subcategory, c.name as category \n        FROM subcategories s\n        JOIN categories c ON s.category_id = c.category_id\n        \"\"\")\n        \n        # Create mapping dictionary\n        subcat_to_cat = {row[0]: row[1] for row in cursor.fetchall()}\n        \n        # Add category column\n        df_with_category = df.copy()\n        df_with_category['category'] = df['subcategory'].map(subcat_to_cat)\n        \n        # Check for unmapped subcategories\n        missing_count = df_with_category['category'].isna().sum()\n        if missing_count > 0:\n            print(f\"Warning: {missing_count} rows have unmapped subcategories\")\n            unmapped = df[df['subcategory'].map(lambda x: x not in subcat_to_cat)]['subcategory'].unique()\n            print(f\"Unmapped subcategories: {unmapped}\")\n            \n        # Fill missing with placeholder\n        df_with_category['category'] = df_with_category['category'].fillna(\"Unknown\")\n        \n        print(f\"Added categories to {len(df_with_category)} transactions\")\n        return df_with_category\n        \n    except Exception as e:\n        print(f\"Error getting category mapping: {e}\")\n        # Create placeholder category column if needed\n        df_copy = df.copy()\n        df_copy['category'] = \"Unknown\"\n        return df_copy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T21:23:50.921723Z","iopub.execute_input":"2025-04-13T21:23:50.922123Z","iopub.status.idle":"2025-04-13T21:23:50.935675Z","shell.execute_reply.started":"2025-04-13T21:23:50.922082Z","shell.execute_reply":"2025-04-13T21:23:50.934648Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"def sample_balanced_data(df, samples_per_subcategory):\n    \"\"\"\n    Create a balanced dataset by sampling evenly across subcategories.\n    If a subcategory has fewer than the requested samples, uses all available rows.\n    \"\"\"\n    # Group by subcategory and sample\n    sampled_df = (\n        df.groupby(\"subcategory\")[df.columns]\n        .apply(lambda x: x.sample(min(len(x), samples_per_subcategory)))\n        .reset_index(drop=True)\n    )\n    \n    # Convert subcategory to category type for efficiency\n    sampled_df[\"subcategory\"] = sampled_df[\"subcategory\"].astype(\"category\")\n    \n    return sampled_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T21:23:50.937025Z","iopub.execute_input":"2025-04-13T21:23:50.937701Z","iopub.status.idle":"2025-04-13T21:23:50.946580Z","shell.execute_reply.started":"2025-04-13T21:23:50.937651Z","shell.execute_reply":"2025-04-13T21:23:50.945793Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"def filter_unmapped_subcategories(df, db_conn):\n    \"\"\"\n    Filter out rows with subcategories that don't have a mapping in the database.\n    \n    Args:\n        df: DataFrame containing transaction data\n        db_conn: Database connection\n        \n    Returns:\n        DataFrame with only mapped subcategories\n    \"\"\"\n    # Get all valid subcategories from the database\n    cursor = db_conn.cursor()\n    cursor.execute(\"SELECT name FROM subcategories\")\n    valid_subcategories = [row[0] for row in cursor.fetchall()]\n    \n    # Filter the DataFrame to only include rows with valid subcategories\n    df_filtered = df[df['subcategory'].isin(valid_subcategories)].copy()\n    \n    # Report how many rows were filtered out\n    filtered_count = len(df) - len(df_filtered)\n    print(f\"Filtered out {filtered_count} rows with unmapped subcategories\")\n    print(f\"Unmapped subcategories: {df[~df['subcategory'].isin(valid_subcategories)]['subcategory'].unique()}\")\n    \n    return df_filtered","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T21:23:50.949521Z","iopub.execute_input":"2025-04-13T21:23:50.949963Z","iopub.status.idle":"2025-04-13T21:23:50.961309Z","shell.execute_reply.started":"2025-04-13T21:23:50.949931Z","shell.execute_reply":"2025-04-13T21:23:50.960413Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"def process_transaction_data(df, db_conn, train_samples=50, test_samples=10, sample_csv_path=None):\n    \"\"\"\n    Process transaction data through all steps: cleaning, categorizing, sampling, and filtering unmapped subcategories.\n    \n    Args:\n        df: DataFrame with transaction data\n        db_conn: Database connection for category mapping\n        train_samples: Number of samples per subcategory for training\n        test_samples: Number of samples per subcategory for testing\n        sample_csv_path: Path to save a sample CSV for review\n        \n    Returns:\n        Tuple of (train_df, test_df, df_with_categories)\n    \"\"\"\n    # Make a copy to avoid modifying the original\n    df_copy = df.copy()\n    \n    # Step 1: Clean transaction notes\n    print(\"Cleaning transaction notes...\")\n    df_copy['cleaned_note'] = df_copy['note'].apply(clean_transaction_note)\n    \n    # Step 2: Add category column\n    print(\"Adding category mapping...\")\n    df_with_categories = add_category_column(df_copy, db_conn)\n    \n    # Step 3: Filter out unmapped subcategories\n    print(\"Filtering out unmapped subcategories...\")\n    df_filtered = filter_unmapped_subcategories(df_with_categories, db_conn)\n    \n    # Step 4: Split into train and test data\n    train_df, test_df = train_test_split(df_filtered, test_size=0.2, random_state=42)\n    \n    # Step 5: Sample balanced datasets\n    print(f\"Creating balanced samples ({train_samples} per subcategory for training)...\")\n    train_sampled = sample_balanced_data(train_df, train_samples)\n    test_sampled = sample_balanced_data(test_df, test_samples)\n    \n    # Step 6: Save sample for review if requested\n    if sample_csv_path:\n        # Take a small sample from each subcategory for review\n        review_sample = sample_balanced_data(df_filtered, 2)\n        # Include original and cleaned notes for comparison\n        review_sample = review_sample[['note', 'cleaned_note', 'category', 'subcategory']]\n        review_sample.to_csv(sample_csv_path, index=False)\n        print(f\"Saved sample data to {sample_csv_path} for review\")\n    \n    # Print statistics\n    print(f\"Original data: {len(df)} transactions\")\n    print(f\"Filtered data: {len(df_filtered)} transactions\")\n    print(f\"Balanced training data: {len(train_sampled)} transactions ({train_sampled['subcategory'].nunique()} subcategories)\")\n    print(f\"Balanced test data: {len(test_sampled)} transactions ({test_sampled['subcategory'].nunique()} subcategories)\")\n    \n    return train_sampled, test_sampled, df_filtered","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T21:23:50.962567Z","iopub.execute_input":"2025-04-13T21:23:50.962921Z","iopub.status.idle":"2025-04-13T21:23:50.974080Z","shell.execute_reply.started":"2025-04-13T21:23:50.962880Z","shell.execute_reply":"2025-04-13T21:23:50.973146Z"}},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":"## Sample the dataset\nNow sample the data. I will keep 50 rows for each subcategory for training.","metadata":{"id":"03lDs1O4ZQ0-"}},{"cell_type":"code","source":"# Connect to the database\nimport sqlite3\nimport pandas as pd\nfrom collections import Counter\n\ndb_conn = sqlite3.connect('/kaggle/working/transaction_categories.db')\n\n# Process the data\ndf_train_sampled, df_test_sampled, df_categorized = process_transaction_data(\n    df, \n    db_conn,\n    train_samples=50, \n    test_samples=10,\n    sample_csv_path=\"/kaggle/working/transaction_sample_review.csv\"\n)\n\n# Print distribution of categories and subcategories\ndef print_distribution_stats(df, dataset_name=\"Dataset\"):\n    print(f\"\\n{'-'*50}\")\n    print(f\"{dataset_name} Distribution Statistics:\")\n    print(f\"{'-'*50}\")\n    \n    # Category distribution\n    category_counts = df['category'].value_counts()\n    print(f\"\\nCategories ({len(category_counts)} unique):\")\n    print(f\"{'Category':<25} {'Count':<10} {'Percentage':<10}\")\n    print(f\"{'-'*45}\")\n    \n    for category, count in category_counts.items():\n        percentage = count / len(df) * 100\n        print(f\"{category[:25]:<25} {count:<10} {percentage:.1f}%\")\n    \n    # Subcategory distribution\n    subcategory_counts = df['subcategory'].value_counts()\n    print(f\"\\nSubcategories ({len(subcategory_counts)} unique):\")\n    print(f\"{'Subcategory':<30} {'Category':<20} {'Count':<10} {'Percentage':<10}\")\n    print(f\"{'-'*70}\")\n    \n    # Create a mapping from subcategory to category for lookup\n    subcat_to_cat = df.groupby('subcategory')['category'].first().to_dict()\n    \n    for subcategory, count in subcategory_counts.items():\n        percentage = count / len(df) * 100\n        category = subcat_to_cat.get(subcategory, \"Unknown\")\n        print(f\"{subcategory[:30]:<30} {category[:20]:<20} {count:<10} {percentage:.1f}%\")\n    \n    # Find subcategories with low counts (potential data issues)\n    low_count_threshold = 5  # Adjust as needed\n    low_count_subcats = subcategory_counts[subcategory_counts < low_count_threshold]\n    if len(low_count_subcats) > 0:\n        print(f\"\\nSubcategories with low counts (<{low_count_threshold}):\")\n        for subcat, count in low_count_subcats.items():\n            print(f\"  - {subcat}: {count} transactions\")\n\n# Display sample of training data\nprint(\"\\nSample of training data:\")\nprint(df_train_sampled[['cleaned_note', 'category', 'subcategory']].head())\n\n# Print distribution statistics for both datasets\nprint_distribution_stats(df_train_sampled, \"Training Data\")\nprint_distribution_stats(df_test_sampled, \"Test Data\")\n\n# Additional summary statistics\nprint(f\"\\n{'-'*50}\")\nprint(f\"Summary Statistics:\")\nprint(f\"{'-'*50}\")\nprint(f\"Total transactions in original data: {len(df)}\")\nprint(f\"Total transactions in training data: {len(df_train_sampled)}\")\nprint(f\"Total transactions in test data: {len(df_test_sampled)}\")\nprint(f\"Training data categories: {df_train_sampled['category'].nunique()}\")\nprint(f\"Test data categories: {df_test_sampled['category'].nunique()}\")\nprint(f\"Training data subcategories: {df_train_sampled['subcategory'].nunique()}\")\nprint(f\"Test data subcategories: {df_test_sampled['subcategory'].nunique()}\")\n\n# Check for any subcategories in test but not in training\ntrain_subcats = set(df_train_sampled['subcategory'].unique())\ntest_subcats = set(df_test_sampled['subcategory'].unique())\ntest_only_subcats = test_subcats - train_subcats\n\nif test_only_subcats:\n    print(f\"\\nWarning: {len(test_only_subcats)} subcategories in test data but not in training data:\")\n    for subcat in test_only_subcats:\n        print(f\"  - {subcat}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T21:23:50.975702Z","iopub.execute_input":"2025-04-13T21:23:50.976790Z","iopub.status.idle":"2025-04-13T21:23:51.535947Z","shell.execute_reply.started":"2025-04-13T21:23:50.976744Z","shell.execute_reply":"2025-04-13T21:23:51.534906Z"}},"outputs":[{"name":"stdout","text":"Cleaning transaction notes...\nAdding category mapping...\nWarning: 377 rows have unmapped subcategories\nUnmapped subcategories: ['Clothes & shoes' 'Financial expenses' 'Food & Drinks' 'Free time'\n 'Housing' 'Life & Entertainment' 'Loan, interests' 'Missing' 'Others'\n 'Phone, cell phone' 'Shopping' 'Transportation']\nAdded categories to 12388 transactions\nFiltering out unmapped subcategories...\nFiltered out 377 rows with unmapped subcategories\nUnmapped subcategories: ['Clothes & shoes' 'Financial expenses' 'Food & Drinks' 'Free time'\n 'Housing' 'Life & Entertainment' 'Loan, interests' 'Missing' 'Others'\n 'Phone, cell phone' 'Shopping' 'Transportation']\nCreating balanced samples (50 per subcategory for training)...\nSaved sample data to /kaggle/working/transaction_sample_review.csv for review\nOriginal data: 12388 transactions\nFiltered data: 12011 transactions\nBalanced training data: 1467 transactions (52 subcategories)\nBalanced test data: 311 transactions (45 subcategories)\n\nSample of training data:\n                                        cleaned_note              category  \\\n0         DEC - LULULEMON ATHLETICA AUSTRAlbert Park  Life & Entertainment   \n1  REBEL MELBOURNE CTRL MELBOURNE VI AUSTap and P...  Life & Entertainment   \n2  STATE TRUSTEES LIMIT MELBOURNE AUSCard Value D...    Financial expenses   \n3                              BWS HWATHORN HWATHORN  Life & Entertainment   \n4                       LIQUORLAND EASTLAND RINGWOOD  Life & Entertainment   \n\n             subcategory  \n0  Active sport, fitness  \n1  Active sport, fitness  \n2               Advisory  \n3       Alcohol, tobacco  \n4       Alcohol, tobacco  \n\n--------------------------------------------------\nTraining Data Distribution Statistics:\n--------------------------------------------------\n\nCategories (11 unique):\nCategory                  Count      Percentage\n---------------------------------------------\nLife & Entertainment      375        25.6%\nShopping                  261        17.8%\nHousing                   160        10.9%\nIncome                    157        10.7%\nFood & Beverages          150        10.2%\nCommunication, PC         102        7.0%\nTransportation            83         5.7%\nFinancial expenses        63         4.3%\nTRANSFER                  50         3.4%\nInvestments               47         3.2%\nVehicle                   19         1.3%\n\nSubcategories (52 unique):\nSubcategory                    Category             Count      Percentage\n----------------------------------------------------------------------\nInterests, dividends           Income               50         3.4%\nSoftware, apps, games          Communication, PC    50         3.4%\nHome, garden                   Shopping             50         3.4%\nHealth and beauty              Shopping             50         3.4%\nGroceries                      Food & Beverages     50         3.4%\nInternet                       Communication, PC    50         3.4%\nMaintenance, repairs           Housing              50         3.4%\nPublic transport               Transportation       50         3.4%\nRefunds (tax, purchase)        Income               50         3.4%\nEnergy, utilities              Housing              50         3.4%\nRestaurant, fast-food          Food & Beverages     50         3.4%\nDrug-store, chemist            Shopping             50         3.4%\nTRANSFER                       TRANSFER             50         3.4%\nTV, Streaming                  Life & Entertainment 50         3.4%\nCharity, gifts                 Life & Entertainment 50         3.4%\nCharges, Fees                  Financial expenses   50         3.4%\nBooks, audio, subscriptions    Life & Entertainment 50         3.4%\nBar, cafe, drink, snacks       Food & Beverages     50         3.4%\nWage, invoices                 Income               50         3.4%\nHobbies                        Life & Entertainment 50         3.4%\nFinancial investments          Investments          46         3.1%\nElectronics, accessories       Shopping             44         3.0%\nHoliday, trips, hotels         Life & Entertainment 43         2.9%\nWellness, beauty               Life & Entertainment 41         2.8%\nGifts, joy                     Shopping             37         2.5%\nRent                           Housing              36         2.5%\nEducation, development         Life & Entertainment 34         2.3%\nTaxi                           Transportation       32         2.2%\nStationery, tools              Shopping             26         1.8%\nCulture, sport events          Life & Entertainment 21         1.4%\nServices                       Housing              18         1.2%\nHealth care, doctor            Life & Entertainment 14         1.0%\nLife events                    Life & Entertainment 12         0.8%\nInsurances                     Financial expenses   11         0.7%\nRentals                        Vehicle              11         0.7%\nParking                        Vehicle              6          0.4%\nMortgage                       Housing              6          0.4%\nGifts                          Income               5          0.3%\nLottery, gambling              Life & Entertainment 4          0.3%\nAlcohol, tobacco               Life & Entertainment 4          0.3%\nJewels, accessories            Shopping             3          0.2%\nActive sport, fitness          Life & Entertainment 2          0.1%\nPostal services                Communication, PC    2          0.1%\nSavings                        Investments          1          0.1%\nFuel                           Vehicle              1          0.1%\nDues & grants                  Income               1          0.1%\nPets, animals                  Shopping             1          0.1%\nChecks, coupons                Income               1          0.1%\nLong distance                  Transportation       1          0.1%\nVehicle insurance              Vehicle              1          0.1%\nAdvisory                       Financial expenses   1          0.1%\nFines                          Financial expenses   1          0.1%\n\nSubcategories with low counts (<5):\n  - Lottery, gambling: 4 transactions\n  - Alcohol, tobacco: 4 transactions\n  - Jewels, accessories: 3 transactions\n  - Active sport, fitness: 2 transactions\n  - Postal services: 2 transactions\n  - Savings: 1 transactions\n  - Fuel: 1 transactions\n  - Dues & grants: 1 transactions\n  - Pets, animals: 1 transactions\n  - Checks, coupons: 1 transactions\n  - Long distance: 1 transactions\n  - Vehicle insurance: 1 transactions\n  - Advisory: 1 transactions\n  - Fines: 1 transactions\n\n--------------------------------------------------\nTest Data Distribution Statistics:\n--------------------------------------------------\n\nCategories (11 unique):\nCategory                  Count      Percentage\n---------------------------------------------\nLife & Entertainment      90         28.9%\nShopping                  55         17.7%\nIncome                    35         11.3%\nHousing                   33         10.6%\nFood & Beverages          30         9.6%\nCommunication, PC         21         6.8%\nTransportation            18         5.8%\nFinancial expenses        14         4.5%\nTRANSFER                  10         3.2%\nInvestments               3          1.0%\nVehicle                   2          0.6%\n\nSubcategories (45 unique):\nSubcategory                    Category             Count      Percentage\n----------------------------------------------------------------------\nWellness, beauty               Life & Entertainment 10         3.2%\nTRANSFER                       TRANSFER             10         3.2%\nRestaurant, fast-food          Food & Beverages     10         3.2%\nMaintenance, repairs           Housing              10         3.2%\nSoftware, apps, games          Communication, PC    10         3.2%\nInternet                       Communication, PC    10         3.2%\nInterests, dividends           Income               10         3.2%\nHome, garden                   Shopping             10         3.2%\nHoliday, trips, hotels         Life & Entertainment 10         3.2%\nHobbies                        Life & Entertainment 10         3.2%\nHealth and beauty              Shopping             10         3.2%\nPublic transport               Transportation       10         3.2%\nGroceries                      Food & Beverages     10         3.2%\nEnergy, utilities              Housing              10         3.2%\nDrug-store, chemist            Shopping             10         3.2%\nWage, invoices                 Income               10         3.2%\nBar, cafe, drink, snacks       Food & Beverages     10         3.2%\nBooks, audio, subscriptions    Life & Entertainment 10         3.2%\nCharges, Fees                  Financial expenses   10         3.2%\nCharity, gifts                 Life & Entertainment 10         3.2%\nRefunds (tax, purchase)        Income               10         3.2%\nTV, Streaming                  Life & Entertainment 10         3.2%\nEducation, development         Life & Entertainment 10         3.2%\nElectronics, accessories       Shopping             10         3.2%\nGifts, joy                     Shopping             9          2.9%\nRent                           Housing              9          2.9%\nTaxi                           Transportation       8          2.6%\nHealth care, doctor            Life & Entertainment 7          2.3%\nLife events                    Life & Entertainment 5          1.6%\nGifts                          Income               4          1.3%\nStationery, tools              Shopping             4          1.3%\nServices                       Housing              3          1.0%\nInsurances                     Financial expenses   3          1.0%\nLottery, gambling              Life & Entertainment 3          1.0%\nFinancial investments          Investments          3          1.0%\nCulture, sport events          Life & Entertainment 3          1.0%\nRentals                        Vehicle              2          0.6%\nPostal services                Communication, PC    1          0.3%\nPets, animals                  Shopping             1          0.3%\nMortgage                       Housing              1          0.3%\nJewels, accessories            Shopping             1          0.3%\nAdvisory                       Financial expenses   1          0.3%\nDues & grants                  Income               1          0.3%\nAlcohol, tobacco               Life & Entertainment 1          0.3%\nActive sport, fitness          Life & Entertainment 1          0.3%\n\nSubcategories with low counts (<5):\n  - Gifts: 4 transactions\n  - Stationery, tools: 4 transactions\n  - Services: 3 transactions\n  - Insurances: 3 transactions\n  - Lottery, gambling: 3 transactions\n  - Financial investments: 3 transactions\n  - Culture, sport events: 3 transactions\n  - Rentals: 2 transactions\n  - Postal services: 1 transactions\n  - Pets, animals: 1 transactions\n  - Mortgage: 1 transactions\n  - Jewels, accessories: 1 transactions\n  - Advisory: 1 transactions\n  - Dues & grants: 1 transactions\n  - Alcohol, tobacco: 1 transactions\n  - Active sport, fitness: 1 transactions\n\n--------------------------------------------------\nSummary Statistics:\n--------------------------------------------------\nTotal transactions in original data: 12388\nTotal transactions in training data: 1467\nTotal transactions in test data: 311\nTraining data categories: 11\nTest data categories: 11\nTraining data subcategories: 52\nTest data subcategories: 45\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_30/2278908751.py:40: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  subcat_to_cat = df.groupby('subcategory')['category'].first().to_dict()\n/tmp/ipykernel_30/2278908751.py:40: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  subcat_to_cat = df.groupby('subcategory')['category'].first().to_dict()\n","output_type":"stream"}],"execution_count":39},{"cell_type":"markdown","source":"## Instruct the zero-shot prompt\nI draft the prompt asking it to only use the subcategory from the loaded table.","metadata":{}},{"cell_type":"code","source":" import sqlite3\n\ndb_file = \"transaction_categories.db\"\ndb_conn = sqlite3.connect(db_file)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T21:23:51.537386Z","iopub.execute_input":"2025-04-13T21:23:51.537794Z","iopub.status.idle":"2025-04-13T21:23:51.543310Z","shell.execute_reply.started":"2025-04-13T21:23:51.537752Z","shell.execute_reply":"2025-04-13T21:23:51.542215Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"def predict_category_and_subcategory(transaction_note):\n    \"\"\"\n    Predicts the category and subcategory for a transaction using the zero-shot system instruction approach.\n    Ensures that the subcategory belongs to the selected category based on database mappings.\n    \n    Args:\n        transaction_note: The transaction text to classify\n        \n    Returns:\n        tuple: (predicted_category, predicted_subcategory)\n    \"\"\"\n    try:\n        system_instruct = \"\"\"\n        You are a financial transaction categorization assistant. You will analyze a transaction description and classify it into the appropriate category and subcategory.\n\n        Follow these steps exactly:\n        1. First, select the most appropriate CATEGORY from the available options\n        2. Then, select a SUBCATEGORY that belongs to the selected CATEGORY\n\n        Important: You must ensure the subcategory you select belongs to the category you chose. The database has specific parent-child relationships between categories and subcategories.\n\n        Your response must use this exact format:\n        CATEGORY: [selected category name]\n        SUBCATEGORY: [selected subcategory name]\n        \"\"\"\n        \n        # First, get all category-subcategory mappings from the database to provide context\n        cursor = db_conn.cursor()\n        cursor.execute(\"\"\"\n        SELECT c.name as category, s.name as subcategory \n        FROM categories c\n        JOIN subcategories s ON c.category_id = s.category_id\n        ORDER BY c.name, s.name\n        \"\"\")\n        mappings = cursor.fetchall()\n        \n        # Create context about the hierarchical structure\n        hierarchy_context = \"Category and subcategory hierarchy from the database:\\n\"\n        current_category = None\n        for category, subcategory in mappings:\n            if category != current_category:\n                hierarchy_context += f\"\\n{category}:\\n\"\n                current_category = category\n            hierarchy_context += f\"  - {subcategory}\\n\"\n        \n        # Make prediction with system instruction and hierarchy context\n        response = client.models.generate_content(\n            model=\"gemini-2.0-flash\",\n            config=types.GenerateContentConfig(\n                system_instruction=system_instruct,\n                temperature=0.2,  # Lower temperature for more consistent results\n            ),\n            contents=[\n                hierarchy_context,\n                f\"Transaction description: {transaction_note}\\n\\nPlease categorize this transaction:\"\n            ]\n        )\n        \n        text = response.text.strip()\n        \n        # Extract category and subcategory\n        try:\n            category_line = [line for line in text.split('\\n') if line.startswith(\"CATEGORY:\")][0]\n            subcategory_line = [line for line in text.split('\\n') if line.startswith(\"SUBCATEGORY:\")][0]\n            \n            category = category_line.replace(\"CATEGORY:\", \"\").strip()\n            subcategory = subcategory_line.replace(\"SUBCATEGORY:\", \"\").strip()\n            \n            # Verify that the subcategory belongs to the category using the database\n            cursor.execute(\"\"\"\n            SELECT COUNT(*) FROM subcategories s\n            JOIN categories c ON s.category_id = c.category_id\n            WHERE c.name = ? AND s.name = ?\n            \"\"\", (category, subcategory))\n            \n            count = cursor.fetchone()[0]\n            if count == 0:\n                # If the model returned an invalid mapping, find a valid subcategory for the category\n                cursor.execute(\"\"\"\n                SELECT s.name FROM subcategories s\n                JOIN categories c ON s.category_id = c.category_id\n                WHERE c.name = ?\n                LIMIT 1\n                \"\"\", (category,))\n                \n                result = cursor.fetchone()\n                if result:\n                    # Use a valid subcategory for this category\n                    subcategory = result[0]\n                    return category, subcategory\n                else:\n                    # If category is invalid too, return error\n                    return \"(invalid category)\", \"(invalid subcategory)\"\n            \n            return category, subcategory\n            \n        except (IndexError, KeyError) as e:\n            return \"(parsing error)\", \"(parsing error)\"\n            \n    except Exception as e:\n        return f\"(error)\", f\"(error: {str(e)})\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T21:23:51.544577Z","iopub.execute_input":"2025-04-13T21:23:51.544931Z","iopub.status.idle":"2025-04-13T21:23:51.558962Z","shell.execute_reply.started":"2025-04-13T21:23:51.544870Z","shell.execute_reply":"2025-04-13T21:23:51.558163Z"}},"outputs":[],"execution_count":41},{"cell_type":"markdown","source":"## Evaluate baseline performance\n\nNow I perform an evaluation on the available models to ensure I can measure how much the tuning helps.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport tqdm\nfrom tqdm.rich import tqdm as tqdmr\nimport warnings\n\ndef evaluate_baseline(df_test_sampled, predict_function, sample_size=20, db_conn=None):\n    \"\"\"\n    Run baseline zero-shot evaluation with output format matching the tuned model evaluation.\n    \n    Args:\n        df_test_sampled: Test DataFrame with notes and labeled categories/subcategories\n        predict_function: Function that takes transaction_note and returns (category, subcategory)\n        sample_size: Number of samples to evaluate\n        db_conn: Optional database connection for category mapping\n        \n    Returns:\n        DataFrame with evaluation results\n    \"\"\"\n    # Suppress warnings\n    warnings.filterwarnings(\"ignore\", category=tqdm.TqdmExperimentalWarning)\n    \n    # Enable tqdm features on Pandas\n    tqdmr.pandas()\n    \n    # Sample the test data\n    if len(df_test_sampled) > sample_size:\n        df_baseline_eval = df_test_sampled.sample(sample_size)\n    else:\n        df_baseline_eval = df_test_sampled.copy()\n    \n    # Ensure category column exists in test data\n    if 'category' not in df_baseline_eval.columns and db_conn is not None:\n        cursor = db_conn.cursor()\n        cursor.execute(\"\"\"\n        SELECT s.name as subcategory, c.name as category \n        FROM subcategories s\n        JOIN categories c ON s.category_id = c.category_id\n        \"\"\")\n        subcat_to_cat = {row[0]: row[1] for row in cursor.fetchall()}\n        df_baseline_eval['category'] = df_baseline_eval['subcategory'].map(subcat_to_cat)\n    \n    print(f\"Evaluating {len(df_baseline_eval)} transactions...\")\n    \n    # Make predictions with progress bar\n    df_baseline_eval[['predicted_category', 'predicted_subcategory']] = df_baseline_eval['note'].progress_apply(\n        lambda x: pd.Series(predict_function(x))\n    )\n    \n    # Calculate accuracies\n    category_accuracy = (df_baseline_eval['category'] == df_baseline_eval['predicted_category']).mean()\n    subcategory_accuracy = (df_baseline_eval['subcategory'] == df_baseline_eval['predicted_subcategory']).mean()\n    combined_accuracy = ((df_baseline_eval['category'] == df_baseline_eval['predicted_category']) & \n                     (df_baseline_eval['subcategory'] == df_baseline_eval['predicted_subcategory'])).mean()\n    \n    print(f\"Category accuracy: {category_accuracy:.2%}\")\n    print(f\"Subcategory accuracy: {subcategory_accuracy:.2%}\")\n    print(f\"Combined accuracy (both correct): {combined_accuracy:.2%}\")\n    \n    # Display sample predictions\n    print(\"\\nSample predictions:\")\n    sample_results = df_baseline_eval[['note', 'category', 'subcategory', \n                                  'predicted_category', 'predicted_subcategory']].sample(min(5, len(df_baseline_eval)))\n    \n    for idx, row in sample_results.iterrows():\n        print(f\"Transaction: {row['note'][:50]}...\")\n        print(f\"True category: {row['category']}\")\n        print(f\"Predicted category: {row['predicted_category']}\")\n        print(f\"Category correct: {row['category'] == row['predicted_category']}\")\n        print(f\"True subcategory: {row['subcategory']}\")\n        print(f\"Predicted subcategory: {row['predicted_subcategory']}\")\n        print(f\"Subcategory correct: {row['subcategory'] == row['predicted_subcategory']}\\n\")\n    \n    # Create a confusion matrix for categories\n    print(\"Category confusion matrix:\")\n    cat_matrix = pd.crosstab(\n        df_baseline_eval['category'], \n        df_baseline_eval['predicted_category'],\n        rownames=['True'], \n        colnames=['Predicted']\n    )\n    print(cat_matrix)\n    \n    # Create a confusion matrix for subcategories with errors\n    print(\"\\nMost common subcategory error patterns:\")\n    error_patterns = df_baseline_eval[df_baseline_eval['subcategory'] != df_baseline_eval['predicted_subcategory']]\n    if len(error_patterns) > 0:\n        error_counts = error_patterns.groupby(['subcategory', 'predicted_subcategory']).size().reset_index(name='count')\n        error_counts = error_counts.sort_values('count', ascending=False)\n        print(error_counts.head(5))\n    else:\n        print(\"No subcategory errors found in the evaluation set!\")\n    \n    # Analysis of hierarchical errors\n    print(\"\\nError analysis by hierarchy:\")\n    hierarchical_errors = df_baseline_eval[\n        (df_baseline_eval['category'] == df_baseline_eval['predicted_category']) & \n        (df_baseline_eval['subcategory'] != df_baseline_eval['predicted_subcategory'])\n    ]\n    print(f\"Correct category but wrong subcategory: {len(hierarchical_errors)} cases ({len(hierarchical_errors)/len(df_baseline_eval):.2%})\")\n    \n    category_errors = df_baseline_eval[df_baseline_eval['category'] != df_baseline_eval['predicted_category']]\n    print(f\"Wrong category: {len(category_errors)} cases ({len(category_errors)/len(df_baseline_eval):.2%})\")\n    \n    # Add additional columns to match the tuned model format\n    df_baseline_eval['category_correct'] = df_baseline_eval['category'] == df_baseline_eval['predicted_category'] \n    df_baseline_eval['subcategory_correct'] = df_baseline_eval['subcategory'] == df_baseline_eval['predicted_subcategory']\n    df_baseline_eval['true_category'] = df_baseline_eval['category']\n    df_baseline_eval['true_subcategory'] = df_baseline_eval['subcategory']\n    df_baseline_eval['pred_category'] = df_baseline_eval['predicted_category']\n    df_baseline_eval['pred_subcategory'] = df_baseline_eval['predicted_subcategory']\n    df_baseline_eval['transaction'] = df_baseline_eval['note']\n    df_baseline_eval['confidence'] = 1.0  # Placeholder\n    \n    return df_baseline_eval","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T21:23:51.560335Z","iopub.execute_input":"2025-04-13T21:23:51.560698Z","iopub.status.idle":"2025-04-13T21:23:51.581469Z","shell.execute_reply.started":"2025-04-13T21:23:51.560667Z","shell.execute_reply":"2025-04-13T21:23:51.580583Z"}},"outputs":[],"execution_count":42},{"cell_type":"markdown","source":"## Tune my model\nNow I train the model with training data to tune it for assessment and potential use.","metadata":{}},{"cell_type":"code","source":"import os\nimport json\nimport datetime\nimport time\nfrom google.api_core import retry\nfrom collections.abc import Iterable\nimport pandas as pd\nfrom tqdm.rich import tqdm as tqdmr\nimport warnings\nimport tqdm\n\n# Path for storing model IDs\nMODEL_IDS_FILE = \"/kaggle/input/training/tuned_model_ids.json\"\n\n# Suppress warnings\nwarnings.filterwarnings(\"ignore\", category=tqdm.TqdmExperimentalWarning)\n\ndef prepare_training_data(df, task_type=\"subcategory\", parent_category=None):\n    \"\"\"\n    Prepare training data for fine-tuning based on task type.\n    \n    Args:\n        df: DataFrame with transaction data\n        task_type: \"category\" or \"subcategory\"\n        parent_category: Only needed for subcategory task, filters data for that category\n        \n    Returns:\n        Dictionary with examples in the format required by the API\n    \"\"\"\n    if task_type not in [\"category\", \"subcategory\"]:\n        raise ValueError(\"task_type must be 'category' or 'subcategory'\")\n    \n    # Filter by parent category if specified\n    if parent_category and task_type == \"subcategory\":\n        df = df[df['category'] == parent_category].copy()\n        \n    if len(df) == 0:\n        print(f\"Warning: No data available for {task_type}\" + \n              (f\" in category '{parent_category}'\" if parent_category else \"\"))\n        return {\"examples\": []}\n    \n    # Prepare examples\n    training_examples = []\n    for _, row in df.iterrows():\n        training_examples.append({\n            \"textInput\": str(row['note']),\n            \"output\": str(row['category' if task_type == 'category' else 'subcategory'])\n        })\n    \n    print(f\"Created {len(training_examples)} training examples for {task_type}\" + \n          (f\" in category '{parent_category}'\" if parent_category else \"\"))\n    \n    return {\"examples\": training_examples}\n\ndef get_model_ids():\n    \"\"\"Load model IDs from file or initialize empty structure\"\"\"\n    if os.path.exists(MODEL_IDS_FILE):\n        with open(MODEL_IDS_FILE, \"r\") as f:\n            return json.load(f)\n    return {\n        \"category_model\": None,\n        \"subcategory_models\": {}\n    }\n\ndef save_model_ids(model_ids):\n    \"\"\"Save model IDs to file\"\"\"\n    with open(MODEL_IDS_FILE, \"w\") as f:\n        json.dump(model_ids, f, indent=2)\n\ndef tune_model(df, task_type=\"category\", parent_category=None, \n              model_name_prefix=\"transaction-classifier\", batch_size=16, epoch_count=3):\n    \"\"\"\n    Tune a model for the given task type and save its ID.\n    \n    Args:\n        df: DataFrame with transaction data\n        task_type: \"category\" or \"subcategory\"\n        parent_category: Only needed for subcategory task, filters data for that category\n        model_name_prefix: Prefix for display name, for easier identification\n        batch_size: Batch size for tuning\n        epoch_count: Number of epochs for tuning\n        \n    Returns:\n        The model ID\n    \"\"\"\n    # Get existing model IDs\n    model_ids = get_model_ids()\n    \n    # Determine which model ID we're looking for\n    if task_type == \"category\":\n        model_id = model_ids[\"category_model\"]\n        model_key = \"category_model\"\n    else:  # subcategory\n        if parent_category is None:\n            raise ValueError(\"parent_category must be specified for subcategory task_type\")\n        model_id = model_ids[\"subcategory_models\"].get(parent_category)\n        model_key = parent_category\n    \n    # If model already exists, return it\n    if model_id:\n        print(f\"Using existing {task_type} model\" + \n              (f\" for '{parent_category}'\" if parent_category else \"\") + \n              f\": {model_id}\")\n        return model_id\n    \n    # Otherwise, check if there's a model in progress\n    queued_model = None\n    display_name = f\"{model_name_prefix}-{task_type}\" + (f\"-{parent_category.lower().replace(' ', '-')}\" if parent_category else \"\")\n    \n    # Format display name to be valid (alphanumeric chars, hyphens)\n    display_name = ''.join(c if c.isalnum() or c == '-' else '-' for c in display_name)\n    \n    print(f\"Looking for existing {task_type} tuning jobs\" + \n          (f\" for '{parent_category}'\" if parent_category else \"\"))\n    \n    # Look for existing models with this display name pattern\n    for m in reversed(client.tunings.list()):\n        if display_name.lower() in m.name.lower():\n            print(f\"Found potential model: {m.name} - state: {m.state.name}\")\n            \n            if m.state.name == 'JOB_STATE_SUCCEEDED':\n                model_id = m.name\n                print(f'Found completed model: {model_id}')\n                break\n            elif m.state.name in ['JOB_STATE_RUNNING', 'JOB_STATE_QUEUED'] and not queued_model:\n                queued_model = m.name\n                print(f'Found model in progress: {queued_model}')\n    \n    # Use queued model if none completed\n    if not model_id and queued_model:\n        model_id = queued_model\n        print(f'Using in-progress model: {model_id}')\n    \n    # Create new model if needed\n    if not model_id:\n        # Prepare training data\n        training_data = prepare_training_data(df, task_type, parent_category)\n        \n        # Don't create model if we don't have enough examples\n        if len(training_data[\"examples\"]) < 5:\n            print(f\"Skipping tuning for {task_type}\" + \n                  (f\" in category '{parent_category}'\" if parent_category else \"\") + \n                  \" due to insufficient data\")\n            return None\n        \n        # Start tuning\n        print(f\"Starting new {task_type} fine-tuning job\" + \n              (f\" for '{parent_category}'\" if parent_category else \"\"))\n        \n        try:\n            tuning_op = client.tunings.tune(\n                base_model=\"models/gemini-1.5-flash-001-tuning\",\n                training_dataset=training_data,\n                config=types.CreateTuningJobConfig(\n                    tuned_model_display_name=display_name,\n                    batch_size=batch_size,\n                    epoch_count=epoch_count,\n                ),\n            )\n            \n            model_id = tuning_op.name\n            print(f\"Fine-tuning initiated. Model ID: {model_id}\")\n            print(f\"Current status: {tuning_op.state}\")\n        except Exception as e:\n            print(f\"Error starting tuning job: {e}\")\n            return None\n    \n    # Update model IDs\n    if task_type == \"category\":\n        model_ids[\"category_model\"] = model_id\n    else:  # subcategory\n        model_ids[\"subcategory_models\"][parent_category] = model_id\n    \n    # Save updated model IDs\n    save_model_ids(model_ids)\n    \n    return model_id\n\ndef train_all_models(df_train):\n    \"\"\"\n    Train a complete hierarchical model system:\n    1. One model for category prediction\n    2. One model per category for subcategory prediction\n    \n    Args:\n        df_train: Training DataFrame\n    \n    Returns:\n        Dictionary of model IDs\n    \"\"\"\n    print(\"Starting hierarchical model training process...\")\n    \n    # Ensure we have category information\n    if 'category' not in df_train.columns:\n        raise ValueError(\"Training data must include 'category' column\")\n    \n    # 1. Train the category model\n    category_model_id = tune_model(df_train, task_type=\"category\")\n    print(f\"Category model: {category_model_id}\")\n    \n    # 2. Train category-specific subcategory models\n    unique_categories = df_train['category'].unique()\n    print(f\"Training {len(unique_categories)} category-specific models...\")\n    \n    for category in unique_categories:\n        # Check if we have enough data for this category\n        category_data = df_train[df_train['category'] == category]\n        if len(category_data) >= 10:  # Minimum threshold for training\n            subcategory_model_id = tune_model(\n                df_train, \n                task_type=\"subcategory\", \n                parent_category=category\n            )\n            print(f\"Subcategory model for '{category}': {subcategory_model_id}\")\n        else:\n            print(f\"Skipping '{category}' due to insufficient data ({len(category_data)} samples)\")\n    \n    # Return the updated model IDs\n    return get_model_ids()\n\n# Get model status information\ndef get_model_status(model_id):\n    \"\"\"Get the current status of a tuned model\"\"\"\n    if not model_id:\n        return {\"state\": \"NOT_FOUND\"}\n    \n    try:\n        model = client.tunings.get(name=model_id)\n        return {\n            \"state\": model.state.name,\n            \"success\": model.has_succeeded,\n            \"ended\": model.has_ended,\n            \"error\": model.error if hasattr(model, \"error\") else None,\n            \"create_time\": model.create_time,\n            \"progress\": model.progress if hasattr(model, \"progress\") else None\n        }\n    except Exception as e:\n        return {\"state\": \"ERROR\", \"error\": str(e)}\n\ndef get_all_model_statuses():\n    \"\"\"Get status information for all models\"\"\"\n    model_ids = get_model_ids()\n    statuses = {\n        \"category_model\": get_model_status(model_ids[\"category_model\"])\n    }\n    \n    subcategory_statuses = {}\n    for category, model_id in model_ids[\"subcategory_models\"].items():\n        subcategory_statuses[category] = get_model_status(model_id)\n    \n    statuses[\"subcategory_models\"] = subcategory_statuses\n    return statuses\n\ndef print_model_status(status_dict, model_type=\"Model\"):\n    \"\"\"Pretty print model status information\"\"\"\n    state = status_dict.get(\"state\", \"UNKNOWN\")\n    create_time = status_dict.get(\"create_time\")\n    progress = status_dict.get(\"progress\")\n    \n    status_text = state\n    \n    # Format creation time if available\n    time_str = \"\"\n    if create_time:\n        try:\n            # Convert to datetime if it's a string\n            if isinstance(create_time, str):\n                dt = datetime.fromisoformat(create_time.replace('Z', '+00:00'))\n            else:\n                dt = create_time\n                \n            time_str = f\", created {dt.strftime('%Y-%m-%d %H:%M:%S UTC')}\"\n        except:\n            time_str = f\", created {create_time}\"\n    \n    # Add progress if available\n    progress_str = f\", {progress}% complete\" if progress is not None else \"\"\n    \n    print(f\"{model_type} status: {status_text}{time_str}{progress_str}\")\n    \n    if status_dict.get(\"error\"):\n        print(f\"Error: {status_dict['error']}\")\n\ndef check_model_statuses():\n    \"\"\"Check and display status for all tuned models\"\"\"\n    try:\n        # Load model IDs\n        with open(\"/kaggle/input/training/tuned_model_ids.json\", \"r\") as f:\n            model_ids = json.load(f)\n        \n        # Get all statuses\n        statuses = get_all_model_statuses()\n        \n        print(\"\\n=== MODEL STATUS REPORT ===\\n\")\n        \n        # Category model status\n        category_model_id = model_ids.get(\"category_model\")\n        if category_model_id:\n            print(f\"Category Model ID: {category_model_id}\")\n            print_model_status(statuses[\"category_model\"], \"Category model\")\n        else:\n            print(\"No category model found\")\n        \n        # Subcategory models status\n        subcategory_models = model_ids.get(\"subcategory_models\", {})\n        if subcategory_models:\n            print(f\"\\nSubcategory Models ({len(subcategory_models)}):\")\n            \n            # Sort by status - succeeded first, then running, then queued\n            sorted_models = []\n            for category, model_id in subcategory_models.items():\n                status = statuses[\"subcategory_models\"].get(category, {})\n                state = status.get(\"state\", \"UNKNOWN\")\n                \n                # Define priority order for sorting\n                priority = {\n                    \"JOB_STATE_SUCCEEDED\": 0,\n                    \"JOB_STATE_RUNNING\": 1,\n                    \"JOB_STATE_QUEUED\": 2,\n                    \"ERROR\": 3,\n                    \"NOT_FOUND\": 4,\n                    \"UNKNOWN\": 5\n                }.get(state, 6)\n                \n                sorted_models.append((category, model_id, status, priority))\n            \n            # Sort by priority first, then by category name\n            sorted_models.sort(key=lambda x: (x[3], x[0]))\n            \n            for category, model_id, status, _ in sorted_models:\n                print(f\"\\n{category} Model ID: {model_id}\")\n                print_model_status(status, f\"{category} model\")\n        else:\n            print(\"\\nNo subcategory models found\")\n        \n        print(\"\\n=== END OF REPORT ===\")\n        \n        # Summary statistics\n        succeeded = sum(1 for cat, status in statuses[\"subcategory_models\"].items() \n                       if status.get(\"state\") == \"JOB_STATE_SUCCEEDED\")\n        running = sum(1 for cat, status in statuses[\"subcategory_models\"].items() \n                     if status.get(\"state\") == \"JOB_STATE_RUNNING\")\n        queued = sum(1 for cat, status in statuses[\"subcategory_models\"].items() \n                    if status.get(\"state\") == \"JOB_STATE_QUEUED\")\n        \n        total = len(subcategory_models)\n        category_status = statuses[\"category_model\"].get(\"state\", \"UNKNOWN\")\n        \n        print(f\"\\nSummary: Category model is {category_status}\")\n        print(f\"Subcategory models: {succeeded} succeeded, {running} running, {queued} queued, {total - succeeded - running - queued} other\")\n        \n    except FileNotFoundError:\n        print(\"No model IDs file found. Please run the training first.\")\n    except Exception as e:\n        print(f\"Error checking model status: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T21:23:51.583061Z","iopub.execute_input":"2025-04-13T21:23:51.583389Z","iopub.status.idle":"2025-04-13T21:23:51.618508Z","shell.execute_reply.started":"2025-04-13T21:23:51.583350Z","shell.execute_reply":"2025-04-13T21:23:51.617649Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"model_ids = train_all_models(df_train_sampled)\nprint(f\"Models are now being trained. IDs saved to {MODEL_IDS_FILE}\")","metadata":{"execution":{"iopub.status.busy":"2025-04-13T21:23:51.619813Z","iopub.execute_input":"2025-04-13T21:23:51.620521Z","iopub.status.idle":"2025-04-13T21:23:53.359381Z","shell.execute_reply.started":"2025-04-13T21:23:51.620474Z","shell.execute_reply":"2025-04-13T21:23:53.358423Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Starting hierarchical model training process...\nUsing existing category model: tunedModels/transactionclassifiercategory-976chq8zmv\nCategory model: tunedModels/transactionclassifiercategory-976chq8zmv\nTraining 11 category-specific models...\nUsing existing subcategory model for 'Life & Entertainment': tunedModels/txnsublifenentertainment-jy03vtag6m8r\nSubcategory model for 'Life & Entertainment': tunedModels/txnsublifenentertainment-jy03vtag6m8r\nUsing existing subcategory model for 'Financial expenses': tunedModels/txnsubfinancialexpenses-yc11w8ykyvvd\nSubcategory model for 'Financial expenses': tunedModels/txnsubfinancialexpenses-yc11w8ykyvvd\nUsing existing subcategory model for 'Food & Beverages': tunedModels/txnsubfoodnbeverages-ttx7p6g0cuc7\nSubcategory model for 'Food & Beverages': tunedModels/txnsubfoodnbeverages-ttx7p6g0cuc7\nUsing existing subcategory model for 'Income': tunedModels/txnsubincome-j64u1nf8izxu\nSubcategory model for 'Income': tunedModels/txnsubincome-j64u1nf8izxu\nUsing existing subcategory model for 'Shopping': tunedModels/txnsubshopping-cyzmp2twjntj\nSubcategory model for 'Shopping': tunedModels/txnsubshopping-cyzmp2twjntj\nUsing existing subcategory model for 'Housing': tunedModels/txnsubhousing-axlf21ww22py\nSubcategory model for 'Housing': tunedModels/txnsubhousing-axlf21ww22py\nUsing existing subcategory model for 'Investments': tunedModels/txnsubinvestments-adomr67a6tut\nSubcategory model for 'Investments': tunedModels/txnsubinvestments-adomr67a6tut\nUsing existing subcategory model for 'Vehicle': tunedModels/txnsubvehicle-hbzpjt22mbes\nSubcategory model for 'Vehicle': tunedModels/txnsubvehicle-hbzpjt22mbes\nUsing existing subcategory model for 'Communication, PC': tunedModels/txnsubcommunicationpc-miugmvo2f2z\nSubcategory model for 'Communication, PC': tunedModels/txnsubcommunicationpc-miugmvo2f2z\nUsing existing subcategory model for 'Transportation': tunedModels/txnsubtransportation-ask54pgcj0iv\nSubcategory model for 'Transportation': tunedModels/txnsubtransportation-ask54pgcj0iv\nLooking for existing subcategory tuning jobs for 'TRANSFER'\nCreated 50 training examples for subcategory in category 'TRANSFER'\nStarting new subcategory fine-tuning job for 'TRANSFER'\nError starting tuning job: 400 INVALID_ARGUMENT. {'error': {'code': 400, 'message': '* CreateTunedModelRequest.tuned_model.display_name: display_name must be up to 40 characters\\n', 'status': 'INVALID_ARGUMENT'}}\nSubcategory model for 'TRANSFER': None\nModels are now being trained. IDs saved to /kaggle/input/training/tuned_model_ids.json\n","output_type":"stream"}],"execution_count":44},{"cell_type":"code","source":"check_model_statuses()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T21:23:53.360826Z","iopub.execute_input":"2025-04-13T21:23:53.361772Z","iopub.status.idle":"2025-04-13T21:23:59.890570Z","shell.execute_reply.started":"2025-04-13T21:23:53.361715Z","shell.execute_reply":"2025-04-13T21:23:59.889627Z"}},"outputs":[{"name":"stdout","text":"\n=== MODEL STATUS REPORT ===\n\nCategory Model ID: tunedModels/transactionclassifiercategory-976chq8zmv\nCategory model status: JOB_STATE_SUCCEEDED, created 2025-04-13 14:04:27 UTC\n\nSubcategory Models (11):\n\nCommunication, PC Model ID: tunedModels/txnsubcommunicationpc-miugmvo2f2z\nCommunication, PC model status: JOB_STATE_SUCCEEDED, created 2025-04-13 14:07:50 UTC\n\nFinancial expenses Model ID: tunedModels/txnsubfinancialexpenses-yc11w8ykyvvd\nFinancial expenses model status: JOB_STATE_SUCCEEDED, created 2025-04-13 14:07:44 UTC\n\nFood & Beverages Model ID: tunedModels/txnsubfoodnbeverages-ttx7p6g0cuc7\nFood & Beverages model status: JOB_STATE_SUCCEEDED, created 2025-04-13 14:07:45 UTC\n\nHousing Model ID: tunedModels/txnsubhousing-axlf21ww22py\nHousing model status: JOB_STATE_SUCCEEDED, created 2025-04-13 14:07:48 UTC\n\nIncome Model ID: tunedModels/txnsubincome-j64u1nf8izxu\nIncome model status: JOB_STATE_SUCCEEDED, created 2025-04-13 14:07:47 UTC\n\nInvestments Model ID: tunedModels/txnsubinvestments-adomr67a6tut\nInvestments model status: JOB_STATE_SUCCEEDED, created 2025-04-13 14:07:49 UTC\n\nLife & Entertainment Model ID: tunedModels/txnsublifenentertainment-jy03vtag6m8r\nLife & Entertainment model status: JOB_STATE_SUCCEEDED, created 2025-04-13 14:07:43 UTC\n\nShopping Model ID: tunedModels/txnsubshopping-cyzmp2twjntj\nShopping model status: JOB_STATE_SUCCEEDED, created 2025-04-13 14:07:47 UTC\n\nTransfer Model ID: tunedModels/txnsubtransfer-u0v1si2rxove\nTransfer model status: JOB_STATE_SUCCEEDED, created 2025-04-13 14:07:51 UTC\n\nTransportation Model ID: tunedModels/txnsubtransportation-ask54pgcj0iv\nTransportation model status: JOB_STATE_SUCCEEDED, created 2025-04-13 14:07:50 UTC\n\nVehicle Model ID: tunedModels/txnsubvehicle-hbzpjt22mbes\nVehicle model status: JOB_STATE_SUCCEEDED, created 2025-04-13 14:07:49 UTC\n\n=== END OF REPORT ===\n\nSummary: Category model is JOB_STATE_SUCCEEDED\nSubcategory models: 11 succeeded, 0 running, 0 queued, 0 other\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"import json\nimport os\n\n# Path for storing model IDs\nMODEL_IDS_FILE = \"/kaggle/input/training/tuned_model_ids.json\"\n\ndef get_model_ids():\n    \"\"\"Load model IDs from file or initialize empty structure\"\"\"\n    if os.path.exists(MODEL_IDS_FILE):\n        with open(MODEL_IDS_FILE, \"r\") as f:\n            return json.load(f)\n    return {\n        \"category_model\": None,\n        \"subcategory_models\": {}\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T21:25:49.444604Z","iopub.execute_input":"2025-04-13T21:25:49.445537Z","iopub.status.idle":"2025-04-13T21:25:49.450514Z","shell.execute_reply.started":"2025-04-13T21:25:49.445500Z","shell.execute_reply":"2025-04-13T21:25:49.449579Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"import pandas as pd\nimport tqdm\nfrom tqdm.rich import tqdm as tqdmr\nimport warnings\n\ndef run_evaluation(df_test_sampled, model_ids, sample_size=20):\n    \"\"\"\n    Evaluate the hierarchical model system on test data with output format aligned \n    with baseline evaluation results for easy comparison.\n    \n    Args:\n        df_test_sampled: Test DataFrame with notes and labeled categories/subcategories\n        model_ids: Dictionary containing model IDs for prediction\n        sample_size: Number of samples to evaluate\n        \n    Returns:\n        DataFrame with evaluation results\n    \"\"\"\n    # Suppress warnings\n    warnings.filterwarnings(\"ignore\", category=tqdm.TqdmExperimentalWarning)\n    \n    print(f\"Evaluating hierarchical model on {sample_size} test transactions...\")\n    \n    # Sample data for evaluation\n    if len(df_test_sampled) > sample_size:\n        eval_df = df_test_sampled.sample(sample_size)\n    else:\n        eval_df = df_test_sampled.copy()\n    \n    # Make predictions\n    results = []\n    for idx, row in tqdmr(eval_df.iterrows(), total=len(eval_df)):\n        transaction = row['note']\n        true_category = row['category']\n        true_subcategory = row['subcategory']\n        \n        # Predict using the hierarchical model system\n        pred_category, pred_subcategory, confidence = categorize_transaction(transaction, model_ids)\n        \n        # Store results\n        results.append({\n            'transaction': transaction,\n            'true_category': true_category,\n            'pred_category': pred_category,\n            'category_correct': true_category == pred_category,\n            'true_subcategory': true_subcategory,\n            'pred_subcategory': pred_subcategory,\n            'subcategory_correct': true_subcategory == pred_subcategory,\n            'confidence': confidence\n        })\n    \n    # Convert to DataFrame\n    results_df = pd.DataFrame(results)\n    \n    # Calculate metrics (aligned with baseline evaluation format)\n    category_accuracy = results_df['category_correct'].mean()\n    subcategory_accuracy = results_df['subcategory_correct'].mean()\n    combined_accuracy = ((results_df['category_correct']) & \n                         (results_df['subcategory_correct'])).mean()\n    \n    print(f\"Category accuracy: {category_accuracy:.2%}\")\n    print(f\"Subcategory accuracy: {subcategory_accuracy:.2%}\")\n    print(f\"Combined accuracy (both correct): {combined_accuracy:.2%}\")\n    \n    # Display sample predictions (aligned with baseline format)\n    print(\"\\nSample predictions:\")\n    sample_results = results_df.sample(min(5, len(results_df)))\n    \n    for idx, row in sample_results.iterrows():\n        print(f\"Transaction: {row['transaction'][:50]}...\")\n        print(f\"True category: {row['true_category']}\")\n        print(f\"Predicted category: {row['pred_category']}\")\n        print(f\"Category correct: {row['category_correct']}\")\n        print(f\"True subcategory: {row['true_subcategory']}\")\n        print(f\"Predicted subcategory: {row['pred_subcategory']}\")\n        print(f\"Subcategory correct: {row['subcategory_correct']}\\n\")\n    \n    # Create a confusion matrix for categories\n    print(\"Category confusion matrix:\")\n    try:\n        cat_matrix = pd.crosstab(\n            results_df['true_category'], \n            results_df['pred_category'],\n            rownames=['True'], \n            colnames=['Predicted']\n        )\n        print(cat_matrix)\n    except Exception as e:\n        print(f\"Unable to generate confusion matrix: {e}\")\n    \n    # Create a confusion matrix for subcategories with errors\n    print(\"\\nMost common subcategory error patterns:\")\n    try:\n        error_patterns = results_df[results_df['subcategory_correct'] == False]\n        if len(error_patterns) > 0:\n            error_counts = error_patterns.groupby(['true_subcategory', 'pred_subcategory']).size().reset_index(name='count')\n            error_counts = error_counts.sort_values('count', ascending=False)\n            print(error_counts.head(5))\n        else:\n            print(\"No subcategory errors found in the evaluation set!\")\n    except Exception as e:\n        print(f\"Unable to analyze subcategory errors: {e}\")\n    \n    # Analysis of hierarchical errors (matching baseline format)\n    print(\"\\nError analysis by hierarchy:\")\n    hierarchical_errors = results_df[\n        (results_df['category_correct']) & \n        (~results_df['subcategory_correct'])\n    ]\n    print(f\"Correct category but wrong subcategory: {len(hierarchical_errors)} cases ({len(hierarchical_errors)/len(results_df):.2%})\")\n    \n    category_errors = results_df[~results_df['category_correct']]\n    print(f\"Wrong category: {len(category_errors)} cases ({len(category_errors)/len(results_df):.2%})\")\n    \n    return results_df\n\ndef categorize_transaction(transaction_note, model_ids):\n    \"\"\"\n    Full hierarchical prediction: predict category, then subcategory.\n    \n    Args:\n        transaction_note: The transaction text to classify\n        model_ids: Dictionary containing model IDs\n        \n    Returns:\n        tuple: (category, subcategory, confidence_score)\n    \"\"\"\n    # First, predict the category\n    category = predict_transaction_category(transaction_note, model_ids)\n    \n    # Handle category prediction errors\n    if category.startswith(\"(error\") or category.startswith(\"(no model\"):\n        return category, \"(category prediction failed)\", 0.0\n    \n    # Next, predict the subcategory using the category-specific model\n    subcategory = predict_transaction_subcategory(transaction_note, category, model_ids)\n    \n    # For now, we don't have confidence scores from the API\n    # Future enhancement: implement a confidence estimation method\n    confidence = 1.0 if not subcategory.startswith(\"(\") else 0.0\n    \n    return category, subcategory, confidence\n\ndef predict_transaction_category(transaction_note, model_ids):\n    \"\"\"Predict just the category using the category model\"\"\"\n    category_model_id = model_ids.get(\"category_model\")\n    \n    if not category_model_id:\n        return \"(no category model available)\"\n    \n    return predict_with_model(category_model_id, transaction_note)\n\ndef predict_transaction_subcategory(transaction_note, category, model_ids):\n    \"\"\"\n    Predict the subcategory using the appropriate model.\n    \n    If category is provided, uses the category-specific model.\n    If not, first predicts the category, then uses that model.\n    \"\"\"\n    # If category not provided, predict it first\n    if not category:\n        category = predict_transaction_category(transaction_note, model_ids)\n        if category.startswith(\"(\"):  # Error or no model\n            return category\n    \n    # Use category-specific subcategory model if available\n    subcategory_model_id = model_ids.get(\"subcategory_models\", {}).get(category)\n    \n    if not subcategory_model_id:\n        return f\"(no subcategory model for '{category}')\"\n    \n    return predict_with_model(subcategory_model_id, transaction_note)\n\ndef predict_with_model(model_id, transaction_note):\n    \"\"\"Make a prediction with a specific model ID\"\"\"\n    from google.api_core import retry\n    from google.genai import types\n    import genai\n    \n    # Define retry logic for API rate limits\n    is_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n    \n    @retry.Retry(predicate=is_retriable)\n    def make_prediction(model_id, text):\n        if not model_id:\n            return \"(no model available)\"\n        \n        try:\n            # Import client from the global scope\n            from __main__ import client\n            \n            response = client.models.generate_content(\n                model=model_id,\n                contents=text,\n                config=types.GenerateContentConfig(\n                    temperature=0.0,  # Use deterministic output for classification\n                    max_output_tokens=10  # Keep it short, we just need the category/subcategory\n                )\n            )\n            \n            if response.candidates and response.candidates[0].content:\n                return response.candidates[0].content.parts[0].text.strip()\n            else:\n                return \"(error: no response)\"\n        except Exception as e:\n            return f\"(error: {str(e)})\"\n    \n    return make_prediction(model_id, transaction_note)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T21:25:51.644960Z","iopub.execute_input":"2025-04-13T21:25:51.645403Z","iopub.status.idle":"2025-04-13T21:25:51.666823Z","shell.execute_reply.started":"2025-04-13T21:25:51.645369Z","shell.execute_reply":"2025-04-13T21:25:51.665761Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"results = run_evaluation(20)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T21:23:59.914908Z","iopub.execute_input":"2025-04-13T21:23:59.915236Z","iopub.status.idle":"2025-04-13T21:23:59.951207Z","shell.execute_reply.started":"2025-04-13T21:23:59.915206Z","shell.execute_reply":"2025-04-13T21:23:59.949921Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[47], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mrun_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n","\u001b[0;31mTypeError\u001b[0m: run_evaluation() missing 1 required positional argument: 'model_ids'"],"ename":"TypeError","evalue":"run_evaluation() missing 1 required positional argument: 'model_ids'","output_type":"error"}],"execution_count":47},{"cell_type":"code","source":"\n\n\n# 1. Run baseline evaluation\ndef compare_models(df_test_sampled, sample_size=20):\n    \"\"\"\n    Run both evaluations and compare the results.\n    \"\"\"\n    print(\"=\" * 50)\n    print(\"BASELINE MODEL EVALUATION (ZERO-SHOT CLASSIFICATION)\")\n    print(\"=\" * 50)\n    \n    # For baseline evaluation, use the predict_category_and_subcategory function\n    baseline_results = evaluate_baseline(\n        df_test_sampled=df_test_sampled,\n        predict_function=predict_category_and_subcategory,\n        sample_size=sample_size,\n        db_conn=db_conn\n    )\n    \n    print(\"\\n\\n\")\n    print(\"=\" * 50)\n    print(\"TUNED MODEL EVALUATION (FINE-TUNED CLASSIFICATION)\")\n    print(\"=\" * 50)\n    \n    # Get model IDs for tuned models\n    model_ids = get_model_ids()\n    \n    # Run the tuned model evaluation\n    tuned_results = run_evaluation(\n        df_test_sampled=df_test_sampled,\n        model_ids=model_ids,\n        sample_size=sample_size\n    )\n    \n    # Compare the results\n    print(\"\\n\\n\")\n    print(\"=\" * 50)\n    print(\"PERFORMANCE COMPARISON SUMMARY\")\n    print(\"=\" * 50)\n    \n    baseline_category_acc = baseline_results['category_correct'].mean()\n    baseline_subcat_acc = baseline_results['subcategory_correct'].mean()\n    baseline_combined_acc = (baseline_results['category_correct'] & baseline_results['subcategory_correct']).mean()\n    \n    tuned_category_acc = tuned_results['category_correct'].mean()\n    tuned_subcat_acc = tuned_results['subcategory_correct'].mean()\n    tuned_combined_acc = (tuned_results['category_correct'] & tuned_results['subcategory_correct']).mean()\n    \n    print(f\"{'Metric':<30} {'Baseline':<15} {'Tuned Model':<15} {'Improvement':<15}\")\n    print(\"-\" * 75)\n    \n    cat_improvement = (tuned_category_acc - baseline_category_acc) * 100\n    subcat_improvement = (tuned_subcat_acc - baseline_subcat_acc) * 100\n    combined_improvement = (tuned_combined_acc - baseline_combined_acc) * 100\n    \n    print(f\"{'Category Accuracy':<30} {baseline_category_acc:.2%}      {tuned_category_acc:.2%}      {cat_improvement:+.2f}%\")\n    print(f\"{'Subcategory Accuracy':<30} {baseline_subcat_acc:.2%}      {tuned_subcat_acc:.2%}      {subcat_improvement:+.2f}%\")\n    print(f\"{'Combined Accuracy':<30} {baseline_combined_acc:.2%}      {tuned_combined_acc:.2%}      {combined_improvement:+.2f}%\")\n    \n    return baseline_results, tuned_results\n\n# Example usage:\n# baseline_results, tuned_results = compare_models(df_test_sampled, sample_size=20)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T21:23:59.952634Z","iopub.status.idle":"2025-04-13T21:23:59.953308Z","shell.execute_reply.started":"2025-04-13T21:23:59.953038Z","shell.execute_reply":"2025-04-13T21:23:59.953065Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Creates a hierarchical model system with:\n\nOne top-level model for category prediction\nSeparate models for subcategory prediction within each category","metadata":{}},{"cell_type":"markdown","source":"from collections.abc import Iterable\nimport datetime\nimport time\nimport os\n\n# 1. Prepare your transaction data for fine-tuning\nprint(\"Preparing transaction data for fine-tuning...\")\n\n# Convert the DataFrame into the format expected by the API\ntraining_examples = []\nfor _, row in df_train_sampled.iterrows():\n    training_examples.append({\n        \"textInput\": str(row['note']),\n        \"output\": str(row['subcategory'])\n    })\n\nprint(f\"Created {len(training_examples)} training examples\")\nprint(f\"Sample example - Input: '{training_examples[0]['textInput'][:50]}...'\")\nprint(f\"Sample example - Output: '{training_examples[0]['output']}'\")\n\n# 2. Prepare the dataset in the required format\ntraining_data = {\"examples\": training_examples}\n\n# 3. Set up the fine-tuning job - find existing or create new\nmodel_id = None\n\ntry:\n    # Try to read previous model ID from file\n    try:\n        with open(\"tuned_model_id.txt\", \"r\") as f:\n            saved_model_id = f.read().strip()\n            if saved_model_id:\n                print(f\"Found previously saved model ID: {saved_model_id}\")\n                model_id = saved_model_id\n    except FileNotFoundError:\n        print(\"No previously saved model ID found.\")\n    \n    # If no saved ID, check for existing models\n    if not model_id:\n        queued_model = None\n        print(\"Checking for existing tuned models...\")\n        \n        # List models in reverse order (newest first)\n        for m in reversed(client.tunings.list()):\n            # Look for transaction classifier models with flexible matching\n            if (\"transaction\" in m.name.lower() or\n                m.name.startswith('tunedModels/personal-transaction-classifier-')):\n                \n                print(f\"Found potential model: {m.name} in state: {m.state.name}\")\n                \n                # If there is a completed model, use it\n                if m.state.name == 'JOB_STATE_SUCCEEDED':\n                    model_id = m.name\n                    print(f'Found existing completed model to reuse: {model_id}')\n                    break\n                elif m.state.name == 'JOB_STATE_RUNNING' and not queued_model:\n                    # If there's a model still running, remember it\n                    queued_model = m.name\n                    print(f'Found model still in progress: {queued_model}')\n        \n        # Use queued model if found and no completed model\n        if not model_id and queued_model:\n            model_id = queued_model\n            print(f'Using in-progress model: {model_id}')\n    \n    # Create new model if needed\n    if not model_id:\n        print(\"Starting new fine-tuning job...\")\n        tuning_op = client.tunings.tune(\n            base_model=\"models/gemini-1.5-flash-001-tuning\",\n            training_dataset=training_data,\n            config=types.CreateTuningJobConfig(\n                tuned_model_display_name=\"transaction-category-classifier\",  \n                batch_size=16,\n                epoch_count=3,\n            ),\n        )\n        \n        model_id = tuning_op.name\n        print(f\"Fine-tuning initiated. Model ID: {model_id}\")\n        print(f\"Current status: {tuning_op.state}\")\n        \n        # Poll for status updates (optional)\n        print(\"Initial training status:\")\n        print(f\"  - State: {tuning_op.state}\")\n        print(f\"  - Create time: {tuning_op.create_time}\")\n        if hasattr(tuning_op, 'progress') and tuning_op.progress:\n            print(f\"  - Progress: {tuning_op.progress}%\")\n    \n    # Save the model ID for later use\n    with open(\"tuned_model_id.txt\", \"w\") as f:\n        f.write(model_id)\n    \n    print(f\"\\nUsing model: {model_id}\")\n    print(\"This ID has been saved and will be used for predictions\")\n    \nexcept Exception as e:\n    print(f\"Error in fine-tuning process: {e}\")","metadata":{"execution":{"iopub.execute_input":"2025-04-10T05:48:56.388511Z","iopub.status.busy":"2025-04-10T05:48:56.387205Z","iopub.status.idle":"2025-04-10T05:48:58.145557Z","shell.execute_reply":"2025-04-10T05:48:58.144833Z","shell.execute_reply.started":"2025-04-10T05:48:56.388457Z"}}},{"cell_type":"markdown","source":"## Monitoring progress\nHere I monitor whether this model has been tuned and ready to use.","metadata":{}},{"cell_type":"code","source":"# 4. Monitor the fine-tuning progress\nstart_time = datetime.datetime.now(datetime.timezone.utc)\ntuned_model = client.tunings.get(name=model_id)\n\nwhile not tuned_model.has_ended:\n    print(f\"Current state: {tuned_model.state.name}\")\n    if hasattr(tuned_model, 'progress'):\n        print(f\"Progress: {tuned_model.progress}%\")\n    \n    time.sleep(60)  # Check every minute\n    tuned_model = client.tunings.get(name=model_id)\n\nprint(f\"Done! The model state is: {tuned_model.state.name}\")\n\nif not tuned_model.has_succeeded and tuned_model.error:\n    print(\"Error:\", tuned_model.error)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T21:23:59.954493Z","iopub.status.idle":"2025-04-13T21:23:59.955017Z","shell.execute_reply.started":"2025-04-13T21:23:59.954740Z","shell.execute_reply":"2025-04-13T21:23:59.954766Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Evaluate Tuned Model\nHere I test and evaluate the performance of the tuned model.","metadata":{}},{"cell_type":"markdown","source":"# 5. Test the tuned model with a sample transaction\ndef categorize_transaction(transaction_note):\n    \"\"\"Use the fine-tuned model to categorize a transaction.\"\"\"\n    response = client.models.generate_content(\n        model=model_id,\n        contents=transaction_note,\n        config=types.GenerateContentConfig(\n            temperature=0.0,  # Use deterministic output for classification\n            max_output_tokens=10,  # Keep it short, we just need the category\n        )\n    )\n    \n    if response.candidates and response.candidates[0].content:\n        return response.candidates[0].content.parts[0].text.strip()\n    else:\n        return \"(error)\"\n\n# Test with a sample transaction\nsample_transaction = \"AMAZON PRIME MEMBERSHIP ANNUAL RENEWAL\"\npredicted_category = categorize_transaction(sample_transaction)\nprint(f\"Transaction: {sample_transaction}\")\nprint(f\"Predicted category: {predicted_category}\")","metadata":{"execution":{"iopub.status.busy":"2025-04-13T13:39:24.131608Z","iopub.status.idle":"2025-04-13T13:39:24.131998Z","shell.execute_reply.started":"2025-04-13T13:39:24.131827Z","shell.execute_reply":"2025-04-13T13:39:24.131845Z"}}},{"cell_type":"markdown","source":"# 6. Evaluate the model on test data\nimport tqdm\nfrom tqdm.rich import tqdm as tqdmr\nimport warnings\n\n# Enable tqdm features on Pandas\ntqdmr.pandas()\n\n# Suppress the experimental warning\nwarnings.filterwarnings(\"ignore\", category=tqdm.TqdmExperimentalWarning)\n\n# Sample a subset of test data for evaluation\nTEST_SAMPLE_SIZE = 20\ndf_eval = df_test_sampled.sample(min(TEST_SAMPLE_SIZE, len(df_test_sampled)))\n\nprint(f\"Evaluating on {len(df_eval)} test transactions...\")\n\n# Make predictions with progress bar\ndf_eval['prediction'] = df_eval['note'].progress_apply(categorize_transaction)\n\n# Calculate accuracy\naccuracy = (df_eval['subcategory'] == df_eval['prediction']).mean()\nprint(f\"Model accuracy: {accuracy:.2%}\")\n\n# Display some examples\nprint(\"\\nSample predictions:\")\nfor idx, row in df_eval.sample(min(5, len(df_eval))).iterrows():\n    print(f\"Transaction: {row['note'][:50]}...\")\n    print(f\"True category: {row['subcategory']}\")\n    print(f\"Predicted: {row['prediction']}\")\n    print(f\"Correct: {row['subcategory'] == row['prediction']}\\n\")\n\n# Show error analysis\nerrors = df_eval[df_eval['subcategory'] != df_eval['prediction']]\nif len(errors) > 0:\n    print(f\"Found {len(errors)} misclassifications\")\n    print(\"Most common error patterns:\")\n    error_matrix = pd.crosstab(\n        errors['subcategory'], \n        errors['prediction'], \n        rownames=['True'], \n        colnames=['Predicted']\n    )\n    print(error_matrix)","metadata":{"execution":{"iopub.status.busy":"2025-04-13T13:39:24.134087Z","iopub.status.idle":"2025-04-13T13:39:24.134457Z","shell.execute_reply.started":"2025-04-13T13:39:24.134289Z","shell.execute_reply":"2025-04-13T13:39:24.134307Z"}}},{"cell_type":"markdown","source":"def list_tables() -> list[str]:\n    \"\"\"Retrieve the names of all tables in the database.\"\"\"\n    # Include print logging statements so you can see when functions are being called.\n    print(' - DB CALL: list_tables()')\n\n    cursor = db_conn.cursor()\n\n    # Fetch the table names.\n    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n\n    tables = cursor.fetchall()\n    return [t[0] for t in tables]\n\n\nlist_tables()","metadata":{"execution":{"iopub.status.busy":"2025-04-13T13:39:24.135559Z","iopub.status.idle":"2025-04-13T13:39:24.135975Z","shell.execute_reply.started":"2025-04-13T13:39:24.135795Z","shell.execute_reply":"2025-04-13T13:39:24.135815Z"}}},{"cell_type":"markdown","source":"def describe_table(table_name: str) -> list[tuple[str, str]]:\n    \"\"\"Look up the table schema.\n\n    Returns:\n      List of columns, where each entry is a tuple of (column, type).\n    \"\"\"\n    print(f' - DB CALL: describe_table({table_name})')\n\n    cursor = db_conn.cursor()\n\n    cursor.execute(f\"PRAGMA table_info({table_name});\")\n\n    schema = cursor.fetchall()\n    # [column index, column name, column type, ...]\n    return [(col[1], col[2]) for col in schema]","metadata":{"execution":{"iopub.status.busy":"2025-04-13T13:39:24.138228Z","iopub.status.idle":"2025-04-13T13:39:24.138587Z","shell.execute_reply.started":"2025-04-13T13:39:24.138420Z","shell.execute_reply":"2025-04-13T13:39:24.138438Z"}}},{"cell_type":"markdown","source":"def execute_query(sql: str) -> list[list[str]]:\n    \"\"\"Execute an SQL statement, returning the results.\"\"\"\n    print(f' - DB CALL: execute_query({sql})')\n\n    cursor = db_conn.cursor()\n\n    cursor.execute(sql)\n    return cursor.fetchall()","metadata":{"execution":{"iopub.status.busy":"2025-04-13T13:39:24.140844Z","iopub.status.idle":"2025-04-13T13:39:24.141229Z","shell.execute_reply.started":"2025-04-13T13:39:24.141056Z","shell.execute_reply":"2025-04-13T13:39:24.141075Z"}}},{"cell_type":"markdown","source":"# These are the database interaction tools defined earlier\ndb_tools = [list_tables, describe_table, execute_query]\n\n# System instruction for the AI to understand what it needs to do\ninstruction = \"\"\"You are a helpful chatbot that can interact with an SQL database for financial transactions. \nYou will first use list_tables to see what tables are present, describe_table to understand the\nschema, and execute_query to issue an SQL SELECT query to retrieve all category-subcategory combinations.\"\"\"\n\n# Create the Google Genai client\nclient = genai.Client(api_key=GOOGLE_API_KEY)\n\nfrom google.api_core import retry\nimport pandas as pd\n\n# Define retry logic for API rate limits\nis_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n\n# User query to get direct category-subcategory combinations\nuser_query = \"\"\"1. Find all unique combinations of category and subcategory from the database.\n2. Execute a SQL query that joins the categories and subcategories tables.\n3. Return the full category name and full subcategory name.\n4. Format your response as simple tabular data that can be saved as CSV.\"\"\"\n\n# Function to get category-subcategory combinations with retry logic\n@retry.Retry(predicate=is_retriable)\ndef get_category_subcategory_combinations():\n    response = client.models.generate_content(\n        model=\"gemini-2.0-flash\",\n        contents=user_query,\n        config=types.GenerateContentConfig(\n            system_instruction=instruction,\n            tools=db_tools,\n        ),\n    )\n    return response.text","metadata":{"execution":{"iopub.status.busy":"2025-04-13T13:39:24.142985Z","iopub.status.idle":"2025-04-13T13:39:24.143358Z","shell.execute_reply.started":"2025-04-13T13:39:24.143188Z","shell.execute_reply":"2025-04-13T13:39:24.143207Z"}}}]}