{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"colab":{"name":"day-4-fine-tuning-a-custom-model.ipynb","toc_visible":true},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11393223,"sourceType":"datasetVersion","datasetId":7129309}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Categorise finance transactions\n\nIn life, my financial transactions are often categorised incorrectly in my budgeting app. I decided to find a better solution.\n\nIn this example, I will first try to categorise with an existing Gemini model using a few-shot prompt and evaluate its performance. Then I will tune a model with the data categorised by me and evaluate its performance","metadata":{}},{"cell_type":"markdown","source":"## Load the subcategory and category table\nIn this step, I load the subcategory, category table.","metadata":{}},{"cell_type":"code","source":"import sqlite3\n\n# Connect to your database\ndb_conn = sqlite3.connect('/kaggle/working/transaction_categories.db')\ncursor = db_conn.cursor()\n\n# Create the tables\ncursor.execute('''\nCREATE TABLE IF NOT EXISTS categories (\n    category_id INTEGER PRIMARY KEY AUTOINCREMENT,\n    name VARCHAR(100) NOT NULL UNIQUE,\n    description TEXT,\n    display_order INT DEFAULT 100,\n    is_active BOOLEAN DEFAULT 1,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n)\n''')\n\ncursor.execute('''\nCREATE TABLE IF NOT EXISTS subcategories (\n    subcategory_id INTEGER PRIMARY KEY AUTOINCREMENT,\n    category_id INTEGER NOT NULL,\n    name VARCHAR(100) NOT NULL,\n    description TEXT,\n    display_order INT DEFAULT 100,\n    is_active BOOLEAN DEFAULT 1,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    FOREIGN KEY (category_id) REFERENCES categories(category_id)\n)\n''')\n\n\n# Insert main categories\ncategories = [\n    ('Food & Beverages', 'Expenses related to food and drinks', 10),\n    ('Shopping', 'Retail purchases and shopping expenses', 20),\n    ('Housing', 'Home-related expenses including rent and utilities', 30),\n    ('Transportation', 'Public and private transportation costs', 40),\n    ('Vehicle', 'Car and vehicle related expenses', 50),\n    ('Life & Entertainment', 'Leisure activities and entertainment', 60),\n    ('Communication, PC', 'Internet, phone and computer expenses', 70),\n    ('Financial expenses', 'Banking fees, loans, and financial costs', 80),\n    ('Investments', 'Investment-related transactions', 90),\n    ('Income', 'All sources of incoming money', 100),\n    ('TRANSFER', 'Money transfers between accounts', 110)\n]\n\ncursor.executemany('INSERT OR IGNORE INTO categories (name, description, display_order) VALUES (?, ?, ?)', categories)\n\n# Insert subcategories for Food & Beverages\nfood_subcategories = [\n    (1, 'Bar, cafe, drink, snacks', 10),\n    (1, 'Groceries', 20),\n    (1, 'Restaurant, fast-food', 30)\n]\ncursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', food_subcategories)\n\n# Insert subcategories for Shopping\nshopping_subcategories = [\n    (2, 'Clothes & Footwear', 10),\n    (2, 'Drug-store, chemist', 20),\n    (2, 'Electronics, accessories', 30),\n    (2, 'Gifts, joy', 40),\n    (2, 'Health and beauty', 50),\n    (2, 'Home, garden', 60),\n    (2, 'Jewels, accessories', 70),\n    (2, 'Kids', 80),\n    (2, 'Leisure time', 90),\n    (2, 'Pets, animals', 100),\n    (2, 'Stationery, tools', 110)\n]\ncursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', shopping_subcategories)\n\nhousing_subcategories=[\n    (3, 'Energy, utilities', 10),\n    (3, 'Maintenance, repairs', 20),\n    (3, 'Mortgage', 30),\n    (3, 'Property insurance', 40),\n    (3, 'Rent', 50),\n    (3, 'Services', 60)\n]\ncursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', housing_subcategories)\n\n\ntransportation_subcategories=[\n    (4, 'Business trips', 10),\n    (4, 'Long distance', 20),\n    (4, 'Public transport', 30),\n    (4, 'Taxi', 40)\n]\ncursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', transportation_subcategories)\n\nvehicle_subcategories=[\n    (5, 'Fuel', 10),\n    (5, 'Leasing', 20),\n    (5, 'Parking', 30),\n    (5, 'Rentals', 40),\n    (5, 'Vehicle insurance', 50),\n    (5, 'Vehicle maintenance', 60)\n]\ncursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', vehicle_subcategories)\n\nlife_subcategories=[\n    (6, 'Active sport, fitness', 10),\n    (6, 'Alcohol, tobacco', 20),\n    (6, 'Books, audio, subscriptions', 30),\n    (6, 'Charity, gifts', 40),\n    (6, 'Culture, sport events', 50),\n    (6, 'Education, development', 60),\n    (6, 'Health care, doctor', 70),\n    (6, 'Hobbies', 80),\n    (6, 'Holiday, trips, hotels', 90),\n    (6, 'Life events', 100),\n    (6, 'Lottery, gambling', 110),\n    (6, 'TV, Streaming', 120),\n    (6, 'Wellness, beauty', 130)\n]\ncursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', life_subcategories)\n\ncommunication_subcategories=[\n    (7, 'Internet', 10),\n    (7, 'Postal services', 20),\n    (7, 'Software, apps, games', 30),\n    (7, 'Telephony, mobile phone', 40)\n]\ncursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', communication_subcategories)\n\nfinancial_subcategories=[\n    (8, 'Advisory', 10),\n    (8, 'Charges, Fees', 20),\n    (8, 'Child Support', 30),\n    (8, 'Fines', 40),\n    (8, 'Insurances', 50),\n    (8, 'Loans, interests', 60),\n    (8, 'Taxes', 70)\n]\ncursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', financial_subcategories)\n\ninvestments_subcategories=[\n    (9, 'Collections', 10),\n    (9, 'Financial investments', 20),\n    (9, 'Realty', 30),\n    (9, 'Savings', 40),\n    (9, 'Vehicles, chattels', 50)\n]\ncursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', investments_subcategories)\n\nincome_subcategories=[\n    (10, 'Checks, coupons', 10),\n    (10, 'Child Support', 20),\n    (10, 'Dues & grants', 30),\n    (10, 'Gifts', 40),\n    (10, 'Interests, dividends', 50),\n    (10, 'Lending, renting', 60),\n    (10, 'Lottery earning', 70),\n    (10, 'Refunds (tax, purchase)', 80),\n    (10, 'Rental income', 90),\n    (10, 'Sale', 100),\n    (10, 'Wage, invoices', 110)\n]\ncursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', income_subcategories)\n\ntransfer_subcatgories=[\n    (11, 'TRANSFER', 10),   \n]\ncursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', transfer_subcatgories)\n\n# Commit the changes\ndb_conn.commit()\nprint(\"Database schema created successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T00:10:12.442834Z","iopub.execute_input":"2025-04-18T00:10:12.443244Z","iopub.status.idle":"2025-04-18T00:10:12.475904Z","shell.execute_reply.started":"2025-04-18T00:10:12.443207Z","shell.execute_reply":"2025-04-18T00:10:12.474779Z"}},"outputs":[{"name":"stdout","text":"Database schema created successfully!\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"In this step, I create the mapping beween category and subcategory","metadata":{}},{"cell_type":"code","source":"def setup_database_and_get_hierarchy(output_path=\"/kaggle/working/category_mapping.csv\"):\n    \"\"\"\n    Initialize database, return category hierarchy, and output a simple mapping CSV.\n    \n    Args:\n        output_path: Path to save the mapping CSV\n        \n    Returns:\n        tuple: (db_connection, category_hierarchy_dict, subcategory_to_category_mapping)\n    \"\"\"\n    import sqlite3\n    import pandas as pd\n    \n    print(\"Setting up database and extracting category hierarchy...\")\n    \n    # Create database connection\n    db_conn = sqlite3.connect('/kaggle/working/transaction_categories.db')\n    cursor = db_conn.cursor()\n    \n    # Create tables and populate data if needed (your existing code)\n    # ... (Keep your existing table creation and population code)\n    \n    # Get complete hierarchy in one operation\n    cursor.execute(\"\"\"\n    SELECT \n        c.name as category, \n        s.name as subcategory\n    FROM categories c\n    JOIN subcategories s ON c.category_id = s.category_id\n    ORDER BY c.display_order, s.display_order\n    \"\"\")\n    \n    # Convert query results to DataFrame\n    results = cursor.fetchall()\n    mapping_df = pd.DataFrame(results, columns=['category', 'subcategory'])\n    \n    # Process results into usable format for return values\n    category_hierarchy = {}\n    subcat_to_cat_mapping = {}\n    \n    for category, subcategory in results:\n        # Build hierarchy dictionary\n        if category not in category_hierarchy:\n            category_hierarchy[category] = []\n        category_hierarchy[category].append(subcategory)\n        \n        # Build mapping dictionary\n        subcat_to_cat_mapping[subcategory] = category\n    \n    # Save to CSV file\n    mapping_df.to_csv(output_path, index=False)\n    \n    # Print summary\n    print(f\"\\nCategory-subcategory mapping saved to {output_path}\")\n    print(f\"Found {len(mapping_df['category'].unique())} categories and {len(mapping_df)} subcategories\")\n    print(\"\\nSample of mapping:\")\n    print(mapping_df.head(5))\n    \n    db_conn.commit()\n    \n    return db_conn, category_hierarchy, subcat_to_cat_mapping","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T00:10:12.478621Z","iopub.execute_input":"2025-04-18T00:10:12.479114Z","iopub.status.idle":"2025-04-18T00:10:12.487950Z","shell.execute_reply.started":"2025-04-18T00:10:12.479060Z","shell.execute_reply":"2025-04-18T00:10:12.486593Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"## Load the dataset\n\nI have uploaded transaction data categorised by me. Then I group it into training data and test data.","metadata":{"id":"peFm0w_0c1CO"}},{"cell_type":"code","source":"# Load and preprocess transaction data\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Load your data\nfile_path = \"/kaggle/input/training/categorized_transaction.csv\"\ndf = pd.read_csv(file_path)\n\n# Split into train and test sets (80/20 split)\ndf_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n\n# Display the subcategories (labels) in your dataset\nsubcategories = df['subcategory'].unique()\nprint(f\"Number of subcategories: {len(subcategories)}\")\nprint(\"Sample subcategories:\", subcategories[:10])  # Show first 10 subcategories\n\n# Quick look at note examples\nprint(\"\\nSample notes:\")\nfor i, note in enumerate(df['note'].head(3)):\n    print(f\"{i+1}. {note} → {df['subcategory'].iloc[i]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T00:10:12.489438Z","iopub.execute_input":"2025-04-18T00:10:12.489794Z","iopub.status.idle":"2025-04-18T00:10:12.541791Z","shell.execute_reply.started":"2025-04-18T00:10:12.489759Z","shell.execute_reply":"2025-04-18T00:10:12.540369Z"}},"outputs":[{"name":"stdout","text":"Number of subcategories: 64\nSample subcategories: ['Active sport, fitness' 'Advisory' 'Alcohol, tobacco'\n 'Bar, cafe, drink, snacks' 'Books, audio, subscriptions' 'Charges, Fees'\n 'Charity, gifts' 'Checks, coupons' 'Clothes & shoes'\n 'Culture, sport events']\n\nSample notes:\n1. AMAZON AUSYDNEY SOUTH CREDIT CARD PURCHASEAmazon Basics High-Density Round Foam Roller for Exercise and Recovery - 61cm, Blue Speckled → Active sport, fitness\n2. 02 DEC 20 - $98.00 LULULEMON ATHLETICA AUSTRAlbert Park [Eff Date: 30 NOV 20] → Active sport, fitness\n3. REBEL MELBOURNE CTRL MELBOURNE VI AUSTap and Pay xx3173Value Date_ 17/03/2018 → Active sport, fitness\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"## Clean the data","metadata":{}},{"cell_type":"code","source":"import re\nimport pandas as pd\n\ndef clean_transaction_note(note):\n    \"\"\"\n    Clean transaction notes to remove common bank formatting, dates, card numbers, etc.\n    \"\"\"\n    # Handle None or empty strings\n    if note is None or pd.isna(note) or note == \"\":\n        return \"\"\n    \n    # Convert to string if needed\n    text = str(note)\n    \n    # Replace non-ASCII characters\n    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n    \n    # Extract main part of the transaction (before common transaction markers)\n    transaction_markers = r'\\s+(?:CREDIT CARD PURCHASE|EFTPOS|Value Date|tap and Pay|Card Purchase|CARD PURCHASE)'\n    parts = re.split(transaction_markers, text, flags=re.IGNORECASE)\n    main_text = parts[0] if parts else text\n    \n    # Clean amount figures and currency symbols\n    main_text = re.sub(r'(?:[$€£¥]|AUD|USD|EUR|GBP|NZD)\\s*[-+]?(?:\\d{1,3}(?:,\\d{3})*|\\d+)(?:\\.\\d{1,2})?', '', main_text)\n    main_text = re.sub(r'\\b[-+]?(?:\\d{1,3}(?:,\\d{3})*|\\d+)(?:\\.\\d{1,2})?\\b', '', main_text)\n    \n    # Remove card numbers (masked or full)\n    main_text = re.sub(r'(?:x{2,4}|X{2,4})\\d{4}|\\b\\d{4}[\\s-]?\\d{4}[\\s-]?\\d{4}[\\s-]?\\d{4}\\b', '', main_text)\n    \n    # Remove date patterns\n    date_pattern = r'(?:\\d{1,2}\\s+(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\\s+\\d{2,4}|' + \\\n                   r'(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\\s+\\d{1,2}(?:st|nd|rd|th)?\\s+\\d{2,4}|' + \\\n                   r'\\d{1,2}[-/.]\\d{1,2}[-/.]\\d{2,4}|' + \\\n                   r'\\d{4}[-/.]\\d{1,2}[-/.]\\d{1,2}|' + \\\n                   r'\\[Eff\\s+Date:.*?\\]|' + \\\n                   r'Value\\s+Date[_:]\\s*\\d{1,2}[-/.]\\d{1,2}[-/.]\\d{2,4})'\n    main_text = re.sub(date_pattern, '', main_text, flags=re.IGNORECASE)\n    \n    # Clean whitespace and punctuation\n    main_text = re.sub(r'\\s+', ' ', main_text)\n    main_text = re.sub(r'[\\s,.-]+$|^[\\s,.-]+', '', main_text)\n    main_text = re.sub(r'\\s+([,.])', r'\\1', main_text)\n    \n    return main_text.strip()[:500]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T00:10:12.696994Z","iopub.execute_input":"2025-04-18T00:10:12.697428Z","iopub.status.idle":"2025-04-18T00:10:12.706421Z","shell.execute_reply.started":"2025-04-18T00:10:12.697393Z","shell.execute_reply":"2025-04-18T00:10:12.705044Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"def add_category_column(df, db_conn):\n    \"\"\"\n    Add category column to DataFrame based on subcategory using database mapping.\n    \"\"\"\n    if 'category' in df.columns:\n        print(\"Category column already exists\")\n        return df\n    \n    try:\n        # Query the database for subcategory to category mapping\n        cursor = db_conn.cursor()\n        cursor.execute(\"\"\"\n        SELECT s.name as subcategory, c.name as category \n        FROM subcategories s\n        JOIN categories c ON s.category_id = c.category_id\n        \"\"\")\n        \n        # Create mapping dictionary\n        subcat_to_cat = {row[0]: row[1] for row in cursor.fetchall()}\n        \n        # Add category column\n        df_with_category = df.copy()\n        df_with_category['category'] = df['subcategory'].map(subcat_to_cat)\n        \n        # Check for unmapped subcategories\n        missing_count = df_with_category['category'].isna().sum()\n        if missing_count > 0:\n            print(f\"Warning: {missing_count} rows have unmapped subcategories\")\n            unmapped = df[df['subcategory'].map(lambda x: x not in subcat_to_cat)]['subcategory'].unique()\n            print(f\"Unmapped subcategories: {unmapped}\")\n            \n        # Fill missing with placeholder\n        df_with_category['category'] = df_with_category['category'].fillna(\"Unknown\")\n        \n        print(f\"Added categories to {len(df_with_category)} transactions\")\n        return df_with_category\n        \n    except Exception as e:\n        print(f\"Error getting category mapping: {e}\")\n        # Create placeholder category column if needed\n        df_copy = df.copy()\n        df_copy['category'] = \"Unknown\"\n        return df_copy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T00:10:12.708774Z","iopub.execute_input":"2025-04-18T00:10:12.709169Z","iopub.status.idle":"2025-04-18T00:10:12.723465Z","shell.execute_reply.started":"2025-04-18T00:10:12.709123Z","shell.execute_reply":"2025-04-18T00:10:12.722140Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"def sample_balanced_data_combined(df, samples_per_label):\n    \"\"\"\n    Create a balanced dataset by sampling evenly across combined labels.\n    If a label has fewer than the requested samples, uses all available rows.\n    \"\"\"\n    # Group by combined label and sample\n    sampled_df = (\n        df.groupby(\"combined_label\")[df.columns]\n        .apply(lambda x: x.sample(min(len(x), samples_per_label)))\n        .reset_index(drop=True)\n    )\n    \n    # Convert to category type for efficiency\n    sampled_df[\"combined_label\"] = sampled_df[\"combined_label\"].astype(\"category\")\n    \n    return sampled_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T00:10:12.725058Z","iopub.execute_input":"2025-04-18T00:10:12.725468Z","iopub.status.idle":"2025-04-18T00:10:12.738721Z","shell.execute_reply.started":"2025-04-18T00:10:12.725435Z","shell.execute_reply":"2025-04-18T00:10:12.737628Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"def filter_unmapped_subcategories(df, db_conn):\n    \"\"\"\n    Filter out rows with subcategories that don't have a mapping in the database.\n    \n    Args:\n        df: DataFrame containing transaction data\n        db_conn: Database connection\n        \n    Returns:\n        DataFrame with only mapped subcategories\n    \"\"\"\n    # Get all valid subcategories from the database\n    cursor = db_conn.cursor()\n    cursor.execute(\"SELECT name FROM subcategories\")\n    valid_subcategories = [row[0] for row in cursor.fetchall()]\n    \n    # Filter the DataFrame to only include rows with valid subcategories\n    df_filtered = df[df['subcategory'].isin(valid_subcategories)].copy()\n    \n    # Report how many rows were filtered out\n    filtered_count = len(df) - len(df_filtered)\n    print(f\"Filtered out {filtered_count} rows with unmapped subcategories\")\n    print(f\"Unmapped subcategories: {df[~df['subcategory'].isin(valid_subcategories)]['subcategory'].unique()}\")\n    \n    return df_filtered","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T00:10:12.740319Z","iopub.execute_input":"2025-04-18T00:10:12.741207Z","iopub.status.idle":"2025-04-18T00:10:12.757170Z","shell.execute_reply.started":"2025-04-18T00:10:12.741154Z","shell.execute_reply":"2025-04-18T00:10:12.755984Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"def process_transaction_data_combined(df, db_conn, train_samples=50, test_samples=10, sample_csv_path=None):\n    \"\"\"\n    Process transaction data with combined category/subcategory labels\n    \"\"\"\n    # Make a copy to avoid modifying the original\n    df_copy = df.copy()\n    \n    # Step 1: Clean transaction notes\n    print(\"Cleaning transaction notes...\")\n    df_copy['cleaned_note'] = df_copy['note'].apply(clean_transaction_note)\n    \n    # Step 2: Add category column if needed\n    if 'category' not in df_copy.columns:\n        print(\"Adding category mapping...\")\n        df_copy = add_category_column(df_copy, db_conn)\n    \n    # Step 3: Create combined category/subcategory label\n    print(\"Creating combined labels...\")\n    df_copy['combined_label'] = df_copy['category'] + \"/\" + df_copy['subcategory']\n    \n    # Step 4: Filter out unmapped subcategories\n    print(\"Filtering out unmapped subcategories...\")\n    df_filtered = filter_unmapped_subcategories(df_copy, db_conn)\n    \n    # Step 5: Split into train and test data\n    train_df, test_df = train_test_split(df_filtered, test_size=0.2, random_state=42)\n    \n    # Step 6: Sample balanced datasets by subcategory first\n    print(f\"Creating balanced samples ({train_samples} per subcategory for training)...\")\n    train_sampled = sample_balanced_data(train_df, train_samples)\n    test_sampled = sample_balanced_data(test_df, test_samples)\n    \n    # Step 7: Create balanced datasets by combined label (optional)\n    print(\"Creating balanced samples by combined label...\")\n    try:\n        train_combined = sample_balanced_data_combined(train_df, max(5, train_samples // 3))\n        test_combined = sample_balanced_data_combined(test_df, max(2, test_samples // 3))\n        \n        # Merge the combined sampled data back into the main samples\n        # Only keep new rows that aren't already in the subcategory-balanced set\n        train_existing_indices = set(train_sampled.index)\n        test_existing_indices = set(test_sampled.index)\n        \n        new_train_rows = train_combined[~train_combined.index.isin(train_existing_indices)]\n        new_test_rows = test_combined[~test_combined.index.isin(test_existing_indices)]\n        \n        train_sampled = pd.concat([train_sampled, new_train_rows])\n        test_sampled = pd.concat([test_sampled, new_test_rows])\n        \n        print(f\"Added {len(new_train_rows)} additional rows from combined label sampling\")\n    except Exception as e:\n        print(f\"Skipping combined label sampling due to error: {e}\")\n    \n    # Step 8: Save sample for review if requested\n    if sample_csv_path:\n        # Take a small sample for review\n        review_sample = train_sampled.sample(min(len(train_sampled), 20))\n        # Include original, cleaned notes and combined label\n        review_sample = review_sample[['note', 'cleaned_note', 'category', 'subcategory', 'combined_label']]\n        review_sample.to_csv(sample_csv_path, index=False)\n        print(f\"Saved sample data to {sample_csv_path} for review\")\n    \n    # Print statistics\n    combined_labels_count = df_filtered['combined_label'].nunique()\n    print(f\"Original data: {len(df)} transactions\")\n    print(f\"Filtered data: {len(df_filtered)} transactions\")\n    print(f\"Unique combined labels: {combined_labels_count}\")\n    print(f\"Balanced training data: {len(train_sampled)} transactions\")\n    print(f\"Balanced test data: {len(test_sampled)} transactions\")\n    \n    return train_sampled, test_sampled, df_filtered","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T00:10:12.760526Z","iopub.execute_input":"2025-04-18T00:10:12.761021Z","iopub.status.idle":"2025-04-18T00:10:12.776205Z","shell.execute_reply.started":"2025-04-18T00:10:12.760971Z","shell.execute_reply":"2025-04-18T00:10:12.775115Z"}},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"## Sample the dataset\nNow sample the data. I will keep 50 rows for each subcategory for training.","metadata":{"id":"03lDs1O4ZQ0-"}},{"cell_type":"code","source":"# Connect to the database\nimport sqlite3\nimport pandas as pd\nfrom collections import Counter\n\ndb_conn = sqlite3.connect('/kaggle/working/transaction_categories.db')\n\n# Process the data\ndf_train_sampled, df_test_sampled, df_categorized = process_transaction_data_combined(\n    df, \n    db_conn,\n    train_samples=50, \n    test_samples=10,\n    sample_csv_path=\"/kaggle/working/transaction_sample_review.csv\"\n)\n\n# Print distribution of categories and subcategories\ndef print_distribution_stats(df, dataset_name=\"Dataset\"):\n    print(f\"\\n{'-'*50}\")\n    print(f\"{dataset_name} Distribution Statistics:\")\n    print(f\"{'-'*50}\")\n    \n    # Category distribution\n    category_counts = df['category'].value_counts()\n    print(f\"\\nCategories ({len(category_counts)} unique):\")\n    print(f\"{'Category':<25} {'Count':<10} {'Percentage':<10}\")\n    print(f\"{'-'*45}\")\n    \n    for category, count in category_counts.items():\n        percentage = count / len(df) * 100\n        print(f\"{category[:25]:<25} {count:<10} {percentage:.1f}%\")\n    \n    # Subcategory distribution\n    subcategory_counts = df['subcategory'].value_counts()\n    print(f\"\\nSubcategories ({len(subcategory_counts)} unique):\")\n    print(f\"{'Subcategory':<30} {'Category':<20} {'Count':<10} {'Percentage':<10}\")\n    print(f\"{'-'*70}\")\n    \n    # Create a mapping from subcategory to category for lookup\n    subcat_to_cat = df.groupby('subcategory')['category'].first().to_dict()\n    \n    for subcategory, count in subcategory_counts.items():\n        percentage = count / len(df) * 100\n        category = subcat_to_cat.get(subcategory, \"Unknown\")\n        print(f\"{subcategory[:30]:<30} {category[:20]:<20} {count:<10} {percentage:.1f}%\")\n    \n    # Find subcategories with low counts (potential data issues)\n    low_count_threshold = 5  # Adjust as needed\n    low_count_subcats = subcategory_counts[subcategory_counts < low_count_threshold]\n    if len(low_count_subcats) > 0:\n        print(f\"\\nSubcategories with low counts (<{low_count_threshold}):\")\n        for subcat, count in low_count_subcats.items():\n            print(f\"  - {subcat}: {count} transactions\")\n\n# Display sample of training data\nprint(\"\\nSample of training data:\")\nprint(df_train_sampled[['cleaned_note', 'category', 'subcategory']].head())\n\n# Print distribution statistics for both datasets\nprint_distribution_stats(df_train_sampled, \"Training Data\")\nprint_distribution_stats(df_test_sampled, \"Test Data\")\n\n# Additional summary statistics\nprint(f\"\\n{'-'*50}\")\nprint(f\"Summary Statistics:\")\nprint(f\"{'-'*50}\")\nprint(f\"Total transactions in original data: {len(df)}\")\nprint(f\"Total transactions in training data: {len(df_train_sampled)}\")\nprint(f\"Total transactions in test data: {len(df_test_sampled)}\")\nprint(f\"Training data categories: {df_train_sampled['category'].nunique()}\")\nprint(f\"Test data categories: {df_test_sampled['category'].nunique()}\")\nprint(f\"Training data subcategories: {df_train_sampled['subcategory'].nunique()}\")\nprint(f\"Test data subcategories: {df_test_sampled['subcategory'].nunique()}\")\n\n# Check for any subcategories in test but not in training\ntrain_subcats = set(df_train_sampled['subcategory'].unique())\ntest_subcats = set(df_test_sampled['subcategory'].unique())\ntest_only_subcats = test_subcats - train_subcats\n\nif test_only_subcats:\n    print(f\"\\nWarning: {len(test_only_subcats)} subcategories in test data but not in training data:\")\n    for subcat in test_only_subcats:\n        print(f\"  - {subcat}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T00:10:12.795183Z","iopub.execute_input":"2025-04-18T00:10:12.795674Z","iopub.status.idle":"2025-04-18T00:10:13.459662Z","shell.execute_reply.started":"2025-04-18T00:10:12.795625Z","shell.execute_reply":"2025-04-18T00:10:13.458558Z"}},"outputs":[{"name":"stdout","text":"Cleaning transaction notes...\nAdding category mapping...\nWarning: 377 rows have unmapped subcategories\nUnmapped subcategories: ['Clothes & shoes' 'Financial expenses' 'Food & Drinks' 'Free time'\n 'Housing' 'Life & Entertainment' 'Loan, interests' 'Missing' 'Others'\n 'Phone, cell phone' 'Shopping' 'Transportation']\nAdded categories to 12388 transactions\nCreating combined labels...\nFiltering out unmapped subcategories...\nFiltered out 377 rows with unmapped subcategories\nUnmapped subcategories: ['Clothes & shoes' 'Financial expenses' 'Food & Drinks' 'Free time'\n 'Housing' 'Life & Entertainment' 'Loan, interests' 'Missing' 'Others'\n 'Phone, cell phone' 'Shopping' 'Transportation']\nCreating balanced samples (50 per subcategory for training)...\nCreating balanced samples by combined label...\nAdded 0 additional rows from combined label sampling\nSaved sample data to /kaggle/working/transaction_sample_review.csv for review\nOriginal data: 12388 transactions\nFiltered data: 12011 transactions\nUnique combined labels: 52\nBalanced training data: 1467 transactions\nBalanced test data: 311 transactions\n\nSample of training data:\n                                        cleaned_note              category  \\\n0         DEC - LULULEMON ATHLETICA AUSTRAlbert Park  Life & Entertainment   \n1  REBEL MELBOURNE CTRL MELBOURNE VI AUSTap and P...  Life & Entertainment   \n2  STATE TRUSTEES LIMIT MELBOURNE AUSCard Value D...    Financial expenses   \n3                       LIQUORLAND EASTLAND RINGWOOD  Life & Entertainment   \n4                              BWS BOX HILL BOX HILL  Life & Entertainment   \n\n             subcategory  \n0  Active sport, fitness  \n1  Active sport, fitness  \n2               Advisory  \n3       Alcohol, tobacco  \n4       Alcohol, tobacco  \n\n--------------------------------------------------\nTraining Data Distribution Statistics:\n--------------------------------------------------\n\nCategories (11 unique):\nCategory                  Count      Percentage\n---------------------------------------------\nLife & Entertainment      375        25.6%\nShopping                  261        17.8%\nHousing                   160        10.9%\nIncome                    157        10.7%\nFood & Beverages          150        10.2%\nCommunication, PC         102        7.0%\nTransportation            83         5.7%\nFinancial expenses        63         4.3%\nTRANSFER                  50         3.4%\nInvestments               47         3.2%\nVehicle                   19         1.3%\n\nSubcategories (52 unique):\nSubcategory                    Category             Count      Percentage\n----------------------------------------------------------------------\nInterests, dividends           Income               50         3.4%\nSoftware, apps, games          Communication, PC    50         3.4%\nHome, garden                   Shopping             50         3.4%\nHealth and beauty              Shopping             50         3.4%\nGroceries                      Food & Beverages     50         3.4%\nInternet                       Communication, PC    50         3.4%\nMaintenance, repairs           Housing              50         3.4%\nPublic transport               Transportation       50         3.4%\nRefunds (tax, purchase)        Income               50         3.4%\nEnergy, utilities              Housing              50         3.4%\nRestaurant, fast-food          Food & Beverages     50         3.4%\nDrug-store, chemist            Shopping             50         3.4%\nTRANSFER                       TRANSFER             50         3.4%\nTV, Streaming                  Life & Entertainment 50         3.4%\nCharity, gifts                 Life & Entertainment 50         3.4%\nCharges, Fees                  Financial expenses   50         3.4%\nBooks, audio, subscriptions    Life & Entertainment 50         3.4%\nBar, cafe, drink, snacks       Food & Beverages     50         3.4%\nWage, invoices                 Income               50         3.4%\nHobbies                        Life & Entertainment 50         3.4%\nFinancial investments          Investments          46         3.1%\nElectronics, accessories       Shopping             44         3.0%\nHoliday, trips, hotels         Life & Entertainment 43         2.9%\nWellness, beauty               Life & Entertainment 41         2.8%\nGifts, joy                     Shopping             37         2.5%\nRent                           Housing              36         2.5%\nEducation, development         Life & Entertainment 34         2.3%\nTaxi                           Transportation       32         2.2%\nStationery, tools              Shopping             26         1.8%\nCulture, sport events          Life & Entertainment 21         1.4%\nServices                       Housing              18         1.2%\nHealth care, doctor            Life & Entertainment 14         1.0%\nLife events                    Life & Entertainment 12         0.8%\nInsurances                     Financial expenses   11         0.7%\nRentals                        Vehicle              11         0.7%\nParking                        Vehicle              6          0.4%\nMortgage                       Housing              6          0.4%\nGifts                          Income               5          0.3%\nLottery, gambling              Life & Entertainment 4          0.3%\nAlcohol, tobacco               Life & Entertainment 4          0.3%\nJewels, accessories            Shopping             3          0.2%\nActive sport, fitness          Life & Entertainment 2          0.1%\nPostal services                Communication, PC    2          0.1%\nSavings                        Investments          1          0.1%\nFuel                           Vehicle              1          0.1%\nDues & grants                  Income               1          0.1%\nPets, animals                  Shopping             1          0.1%\nChecks, coupons                Income               1          0.1%\nLong distance                  Transportation       1          0.1%\nVehicle insurance              Vehicle              1          0.1%\nAdvisory                       Financial expenses   1          0.1%\nFines                          Financial expenses   1          0.1%\n\nSubcategories with low counts (<5):\n  - Lottery, gambling: 4 transactions\n  - Alcohol, tobacco: 4 transactions\n  - Jewels, accessories: 3 transactions\n  - Active sport, fitness: 2 transactions\n  - Postal services: 2 transactions\n  - Savings: 1 transactions\n  - Fuel: 1 transactions\n  - Dues & grants: 1 transactions\n  - Pets, animals: 1 transactions\n  - Checks, coupons: 1 transactions\n  - Long distance: 1 transactions\n  - Vehicle insurance: 1 transactions\n  - Advisory: 1 transactions\n  - Fines: 1 transactions\n\n--------------------------------------------------\nTest Data Distribution Statistics:\n--------------------------------------------------\n\nCategories (11 unique):\nCategory                  Count      Percentage\n---------------------------------------------\nLife & Entertainment      90         28.9%\nShopping                  55         17.7%\nIncome                    35         11.3%\nHousing                   33         10.6%\nFood & Beverages          30         9.6%\nCommunication, PC         21         6.8%\nTransportation            18         5.8%\nFinancial expenses        14         4.5%\nTRANSFER                  10         3.2%\nInvestments               3          1.0%\nVehicle                   2          0.6%\n\nSubcategories (45 unique):\nSubcategory                    Category             Count      Percentage\n----------------------------------------------------------------------\nWellness, beauty               Life & Entertainment 10         3.2%\nTRANSFER                       TRANSFER             10         3.2%\nRestaurant, fast-food          Food & Beverages     10         3.2%\nMaintenance, repairs           Housing              10         3.2%\nSoftware, apps, games          Communication, PC    10         3.2%\nInternet                       Communication, PC    10         3.2%\nInterests, dividends           Income               10         3.2%\nHome, garden                   Shopping             10         3.2%\nHoliday, trips, hotels         Life & Entertainment 10         3.2%\nHobbies                        Life & Entertainment 10         3.2%\nHealth and beauty              Shopping             10         3.2%\nPublic transport               Transportation       10         3.2%\nGroceries                      Food & Beverages     10         3.2%\nEnergy, utilities              Housing              10         3.2%\nDrug-store, chemist            Shopping             10         3.2%\nWage, invoices                 Income               10         3.2%\nBar, cafe, drink, snacks       Food & Beverages     10         3.2%\nBooks, audio, subscriptions    Life & Entertainment 10         3.2%\nCharges, Fees                  Financial expenses   10         3.2%\nCharity, gifts                 Life & Entertainment 10         3.2%\nRefunds (tax, purchase)        Income               10         3.2%\nTV, Streaming                  Life & Entertainment 10         3.2%\nEducation, development         Life & Entertainment 10         3.2%\nElectronics, accessories       Shopping             10         3.2%\nGifts, joy                     Shopping             9          2.9%\nRent                           Housing              9          2.9%\nTaxi                           Transportation       8          2.6%\nHealth care, doctor            Life & Entertainment 7          2.3%\nLife events                    Life & Entertainment 5          1.6%\nGifts                          Income               4          1.3%\nStationery, tools              Shopping             4          1.3%\nServices                       Housing              3          1.0%\nInsurances                     Financial expenses   3          1.0%\nLottery, gambling              Life & Entertainment 3          1.0%\nFinancial investments          Investments          3          1.0%\nCulture, sport events          Life & Entertainment 3          1.0%\nRentals                        Vehicle              2          0.6%\nPostal services                Communication, PC    1          0.3%\nPets, animals                  Shopping             1          0.3%\nMortgage                       Housing              1          0.3%\nJewels, accessories            Shopping             1          0.3%\nAdvisory                       Financial expenses   1          0.3%\nDues & grants                  Income               1          0.3%\nAlcohol, tobacco               Life & Entertainment 1          0.3%\nActive sport, fitness          Life & Entertainment 1          0.3%\n\nSubcategories with low counts (<5):\n  - Gifts: 4 transactions\n  - Stationery, tools: 4 transactions\n  - Services: 3 transactions\n  - Insurances: 3 transactions\n  - Lottery, gambling: 3 transactions\n  - Financial investments: 3 transactions\n  - Culture, sport events: 3 transactions\n  - Rentals: 2 transactions\n  - Postal services: 1 transactions\n  - Pets, animals: 1 transactions\n  - Mortgage: 1 transactions\n  - Jewels, accessories: 1 transactions\n  - Advisory: 1 transactions\n  - Dues & grants: 1 transactions\n  - Alcohol, tobacco: 1 transactions\n  - Active sport, fitness: 1 transactions\n\n--------------------------------------------------\nSummary Statistics:\n--------------------------------------------------\nTotal transactions in original data: 12388\nTotal transactions in training data: 1467\nTotal transactions in test data: 311\nTraining data categories: 11\nTest data categories: 11\nTraining data subcategories: 52\nTest data subcategories: 45\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_30/4103278480.py:47: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n  train_sampled = pd.concat([train_sampled, new_train_rows])\n/tmp/ipykernel_30/4103278480.py:48: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n  test_sampled = pd.concat([test_sampled, new_test_rows])\n/tmp/ipykernel_30/4165388077.py:40: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  subcat_to_cat = df.groupby('subcategory')['category'].first().to_dict()\n/tmp/ipykernel_30/4165388077.py:40: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  subcat_to_cat = df.groupby('subcategory')['category'].first().to_dict()\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"## Instruct the zero-shot prompt\nI draft the prompt asking it to only use the subcategory from the loaded table.","metadata":{}},{"cell_type":"code","source":" import sqlite3\n\ndb_file = \"transaction_categories.db\"\ndb_conn = sqlite3.connect(db_file)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T00:10:13.460981Z","iopub.execute_input":"2025-04-18T00:10:13.461352Z","iopub.status.idle":"2025-04-18T00:10:13.466494Z","shell.execute_reply.started":"2025-04-18T00:10:13.461312Z","shell.execute_reply":"2025-04-18T00:10:13.465248Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"def predict_category_and_subcategory(transaction_note):\n    \"\"\"\n    Predicts the category and subcategory for a transaction using the zero-shot system instruction approach.\n    Ensures that the subcategory belongs to the selected category based on database mappings.\n    \n    Args:\n        transaction_note: The transaction text to classify\n        \n    Returns:\n        tuple: (predicted_category, predicted_subcategory)\n    \"\"\"\n    try:\n        system_instruct = \"\"\"\n        You are a financial transaction categorization assistant. You will analyze a transaction description and classify it into the appropriate category and subcategory.\n\n        Follow these steps exactly:\n        1. First, select the most appropriate CATEGORY from the available options\n        2. Then, select a SUBCATEGORY that belongs to the selected CATEGORY\n\n        Important: You must ensure the subcategory you select belongs to the category you chose. The database has specific parent-child relationships between categories and subcategories.\n\n        Your response must use this exact format:\n        CATEGORY: [selected category name]\n        SUBCATEGORY: [selected subcategory name]\n        \"\"\"\n        \n        # First, get all category-subcategory mappings from the database to provide context\n        cursor = db_conn.cursor()\n        cursor.execute(\"\"\"\n        SELECT c.name as category, s.name as subcategory \n        FROM categories c\n        JOIN subcategories s ON c.category_id = s.category_id\n        ORDER BY c.name, s.name\n        \"\"\")\n        mappings = cursor.fetchall()\n        \n        # Create context about the hierarchical structure\n        hierarchy_context = \"Category and subcategory hierarchy from the database:\\n\"\n        current_category = None\n        for category, subcategory in mappings:\n            if category != current_category:\n                hierarchy_context += f\"\\n{category}:\\n\"\n                current_category = category\n            hierarchy_context += f\"  - {subcategory}\\n\"\n        \n        # Make prediction with system instruction and hierarchy context\n        response = client.models.generate_content(\n            model=\"gemini-2.0-flash\",\n            config=types.GenerateContentConfig(\n                system_instruction=system_instruct,\n                temperature=0.2,  # Lower temperature for more consistent results\n            ),\n            contents=[\n                hierarchy_context,\n                f\"Transaction description: {transaction_note}\\n\\nPlease categorize this transaction:\"\n            ]\n        )\n        \n        text = response.text.strip()\n        \n        # Extract category and subcategory\n        try:\n            category_line = [line for line in text.split('\\n') if line.startswith(\"CATEGORY:\")][0]\n            subcategory_line = [line for line in text.split('\\n') if line.startswith(\"SUBCATEGORY:\")][0]\n            \n            category = category_line.replace(\"CATEGORY:\", \"\").strip()\n            subcategory = subcategory_line.replace(\"SUBCATEGORY:\", \"\").strip()\n            \n            # Verify that the subcategory belongs to the category using the database\n            cursor.execute(\"\"\"\n            SELECT COUNT(*) FROM subcategories s\n            JOIN categories c ON s.category_id = c.category_id\n            WHERE c.name = ? AND s.name = ?\n            \"\"\", (category, subcategory))\n            \n            count = cursor.fetchone()[0]\n            if count == 0:\n                # If the model returned an invalid mapping, find a valid subcategory for the category\n                cursor.execute(\"\"\"\n                SELECT s.name FROM subcategories s\n                JOIN categories c ON s.category_id = c.category_id\n                WHERE c.name = ?\n                LIMIT 1\n                \"\"\", (category,))\n                \n                result = cursor.fetchone()\n                if result:\n                    # Use a valid subcategory for this category\n                    subcategory = result[0]\n                    return category, subcategory\n                else:\n                    # If category is invalid too, return error\n                    return \"(invalid category)\", \"(invalid subcategory)\"\n            \n            return category, subcategory\n            \n        except (IndexError, KeyError) as e:\n            return \"(parsing error)\", \"(parsing error)\"\n            \n    except Exception as e:\n        return f\"(error)\", f\"(error: {str(e)})\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T00:10:13.468267Z","iopub.execute_input":"2025-04-18T00:10:13.469092Z","iopub.status.idle":"2025-04-18T00:10:13.487112Z","shell.execute_reply.started":"2025-04-18T00:10:13.469041Z","shell.execute_reply":"2025-04-18T00:10:13.485643Z"}},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":"## Evaluate baseline performance\n\nNow I perform an evaluation on the available models to ensure I can measure how much the tuning helps.","metadata":{}},{"cell_type":"code","source":"import tqdm\nfrom tqdm.rich import tqdm as tqdmr\nimport warnings\n\n# Enable tqdm features on Pandas\ntqdmr.pandas()\n\n# Suppress the experimental warning\nwarnings.filterwarnings(\"ignore\", category=tqdm.TqdmExperimentalWarning)\n\n# Further sample the test data to be mindful of the free-tier quota\nTEST_SAMPLE_SIZE = 20\ndf_baseline_eval = df_test_sampled.sample(min(TEST_SAMPLE_SIZE, len(df_test_sampled)))\n\n# Ensure category column exists in test data\nif 'category' not in df_baseline_eval.columns:\n    # Add categories using the database mapping\n    cursor = db_conn.cursor()\n    cursor.execute(\"\"\"\n    SELECT s.name as subcategory, c.name as category \n    FROM subcategories s\n    JOIN categories c ON s.category_id = c.category_id\n    \"\"\")\n    subcat_to_cat = {row[0]: row[1] for row in cursor.fetchall()}\n    df_baseline_eval['category'] = df_baseline_eval['subcategory'].map(subcat_to_cat)\n\nprint(f\"Evaluating {len(df_baseline_eval)} transactions...\")\n\n# Make predictions using the sampled data with progress bar\n# This will return both category and subcategory\ndf_baseline_eval[['predicted_category', 'predicted_subcategory']] = df_baseline_eval['note'].progress_apply(\n    lambda x: pd.Series(predict_category_and_subcategory(x))\n)\n\n# Calculate the accuracy for both category and subcategory\ncategory_accuracy = (df_baseline_eval['category'] == df_baseline_eval['predicted_category']).mean()\nsubcategory_accuracy = (df_baseline_eval['subcategory'] == df_baseline_eval['predicted_subcategory']).mean()\ncombined_accuracy = ((df_baseline_eval['category'] == df_baseline_eval['predicted_category']) & \n                     (df_baseline_eval['subcategory'] == df_baseline_eval['predicted_subcategory'])).mean()\n\nprint(f\"Category accuracy: {category_accuracy:.2%}\")\nprint(f\"Subcategory accuracy: {subcategory_accuracy:.2%}\")\nprint(f\"Combined accuracy (both correct): {combined_accuracy:.2%}\")\n\n# Display some examples of predictions\nprint(\"\\nSample predictions:\")\nsample_results = df_baseline_eval[['note', 'category', 'subcategory', \n                                  'predicted_category', 'predicted_subcategory']].sample(min(5, len(df_baseline_eval)))\n\nfor idx, row in sample_results.iterrows():\n    print(f\"Transaction: {row['note'][:50]}...\")\n    print(f\"True category: {row['category']}\")\n    print(f\"Predicted category: {row['predicted_category']}\")\n    print(f\"Category correct: {row['category'] == row['predicted_category']}\")\n    print(f\"True subcategory: {row['subcategory']}\")\n    print(f\"Predicted subcategory: {row['predicted_subcategory']}\")\n    print(f\"Subcategory correct: {row['subcategory'] == row['predicted_subcategory']}\\n\")\n\n# Create a confusion matrix for categories\nprint(\"Category confusion matrix:\")\ncat_matrix = pd.crosstab(\n    df_baseline_eval['category'], \n    df_baseline_eval['predicted_category'],\n    rownames=['True'], \n    colnames=['Predicted']\n)\nprint(cat_matrix)\n\n# Create a confusion matrix for subcategories with errors\nprint(\"\\nMost common subcategory error patterns:\")\nerror_patterns = df_baseline_eval[df_baseline_eval['subcategory'] != df_baseline_eval['predicted_subcategory']]\nif len(error_patterns) > 0:\n    error_counts = error_patterns.groupby(['subcategory', 'predicted_subcategory']).size().reset_index(name='count')\n    error_counts = error_counts.sort_values('count', ascending=False)\n    print(error_counts.head(5))\nelse:\n    print(\"No subcategory errors found in the evaluation set!\")\n\n# Analysis of hierarchical errors\nprint(\"\\nError analysis by hierarchy:\")\nhierarchical_errors = df_baseline_eval[\n    (df_baseline_eval['category'] == df_baseline_eval['predicted_category']) & \n    (df_baseline_eval['subcategory'] != df_baseline_eval['predicted_subcategory'])\n]\nprint(f\"Correct category but wrong subcategory: {len(hierarchical_errors)} cases ({len(hierarchical_errors)/len(df_baseline_eval):.2%})\")\n\ncategory_errors = df_baseline_eval[df_baseline_eval['category'] != df_baseline_eval['predicted_category']]\nprint(f\"Wrong category: {len(category_errors)} cases ({len(category_errors)/len(df_baseline_eval):.2%})\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T00:10:13.488871Z","iopub.execute_input":"2025-04-18T00:10:13.489436Z","iopub.status.idle":"2025-04-18T00:10:13.836253Z","shell.execute_reply.started":"2025-04-18T00:10:13.489398Z","shell.execute_reply":"2025-04-18T00:10:13.835057Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output()","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eeff4b0f0730473c909a6dd991ec2eed"}},"metadata":{}},{"name":"stdout","text":"Evaluating 20 transactions...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Category accuracy: 0.00%\nSubcategory accuracy: 0.00%\nCombined accuracy (both correct): 0.00%\n\nSample predictions:\nTransaction: TRANSFER 1007520292322 PAYPAL AUSTRALIA 0945910 Z@...\nTrue category: Communication, PC\nPredicted category: (error)\nCategory correct: False\nTrue subcategory: Software, apps, games\nPredicted subcategory: (error: name 'client' is not defined)\nSubcategory correct: False\n\nTransaction: Pronamel Sensotive & Gentle Whitening Toothpaste, ...\nTrue category: Shopping\nPredicted category: (error)\nCategory correct: False\nTrue subcategory: Health and beauty\nPredicted subcategory: (error: name 'client' is not defined)\nSubcategory correct: False\n\nTransaction: V8039 13/12 PAYPAL *MUSEUMSBOAR XXXXXXXXXX XXXXXXX...\nTrue category: Life & Entertainment\nPredicted category: (error)\nCategory correct: False\nTrue subcategory: Hobbies\nPredicted subcategory: (error: name 'client' is not defined)\nSubcategory correct: False\n\nTransaction: ChatGPT Subscription...\nTrue category: Life & Entertainment\nPredicted category: (error)\nCategory correct: False\nTrue subcategory: TV, Streaming\nPredicted subcategory: (error: name 'client' is not defined)\nSubcategory correct: False\n\nTransaction: FIRST TAX & DUTY FREE TULLAMARINE...\nTrue category: Shopping\nPredicted category: (error)\nCategory correct: False\nTrue subcategory: Health and beauty\nPredicted subcategory: (error: name 'client' is not defined)\nSubcategory correct: False\n\nCategory confusion matrix:\nPredicted             (error)\nTrue                         \nCommunication, PC           1\nFood & Beverages            2\nIncome                      2\nInvestments                 1\nLife & Entertainment        7\nShopping                    7\n\nMost common subcategory error patterns:\n              subcategory                  predicted_subcategory  count\n17      Health and beauty  (error: name 'client' is not defined)      3\n41          TV, Streaming  (error: name 'client' is not defined)      3\n19                Hobbies  (error: name 'client' is not defined)      2\n44       Wellness, beauty  (error: name 'client' is not defined)      1\n38  Software, apps, games  (error: name 'client' is not defined)      1\n\nError analysis by hierarchy:\nCorrect category but wrong subcategory: 0 cases (0.00%)\nWrong category: 20 cases (100.00%)\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_30/3292486879.py:73: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  error_counts = error_patterns.groupby(['subcategory', 'predicted_subcategory']).size().reset_index(name='count')\n","output_type":"stream"}],"execution_count":25},{"cell_type":"markdown","source":"## Train my model\nHere I train one model to learn how to set the category and train 11 models, one for each subcategory.","metadata":{}},{"cell_type":"code","source":"import os\nimport json\nimport datetime\nimport time\nfrom google.api_core import retry\nfrom collections.abc import Iterable\nimport pandas as pd\nfrom tqdm.rich import tqdm as tqdmr\nimport warnings\nimport tqdm\n\n# Path for storing model IDs\nMODEL_IDS_FILE = \"/kaggle/working/tuned_model_ids.json\"\n\n# Suppress warnings\nwarnings.filterwarnings(\"ignore\", category=tqdm.TqdmExperimentalWarning)\n\ndef prepare_combined_training_data(df):\n    \"\"\"\n    Prepare training data for fine-tuning with combined labels.\n    \n    Args:\n        df: DataFrame with transaction data\n        \n    Returns:\n        Dictionary with examples in the format required by the API\n    \"\"\"\n    # Prepare examples\n    training_examples = []\n    for _, row in df.iterrows():\n        training_examples.append({\n            \"textInput\": str(row['note']),\n            \"output\": str(row['combined_label'])\n        })\n    \n    print(f\"Created {len(training_examples)} training examples with combined labels\")\n    \n    return {\"examples\": training_examples}\n\ndef get_model_ids():\n    \"\"\"Load model IDs from file or initialize empty structure\"\"\"\n    if os.path.exists(MODEL_IDS_FILE):\n        with open(MODEL_IDS_FILE, \"r\") as f:\n            return json.load(f)\n    return {\n        \"category_model\": None,\n        \"subcategory_models\": {}\n    }\n\ndef save_model_ids(model_ids):\n    \"\"\"Save model IDs to file\"\"\"\n    with open(MODEL_IDS_FILE, \"w\") as f:\n        json.dump(model_ids, f, indent=2)\n\ndef train_combined_model(df_train):\n    \"\"\"\n    Train a single model that predicts the combined \"Category/Subcategory\" label.\n    \n    Args:\n        df_train: DataFrame containing training data with 'combined_label' column\n        \n    Returns:\n        The model ID\n    \"\"\"\n    print(\"Starting combined model training process...\")\n    \n    # Ensure we have the combined label\n    if 'combined_label' not in df_train.columns:\n        if 'category' in df_train.columns and 'subcategory' in df_train.columns:\n            df_train['combined_label'] = df_train['category'] + \"/\" + df_train['subcategory']\n            print(\"Created combined labels from category and subcategory\")\n        else:\n            raise ValueError(\"Training data must include 'category' and 'subcategory' columns\")\n    \n    # Check if we already have a combined model\n    combined_model_id = None\n    model_file = \"/kaggle/working/combined_model_id.json\"\n    \n    try:\n        if os.path.exists(model_file):\n            with open(model_file, \"r\") as f:\n                saved_data = json.load(f)\n                combined_model_id = saved_data.get(\"combined_model\")\n                \n            if combined_model_id:\n                print(f\"Using existing combined model: {combined_model_id}\")\n                return combined_model_id\n    except Exception as e:\n        print(f\"Error loading existing model ID: {e}\")\n    \n    # Prepare training data\n    training_data = prepare_combined_training_data(df_train)\n    \n    # Don't create model if we don't have enough examples\n    if len(training_data[\"examples\"]) < 5:\n        print(\"Skipping tuning due to insufficient data\")\n        return None\n    \n    # Start tuning\n    print(\"Starting new combined classification fine-tuning job\")\n    \n    # Create display name (must be under 40 chars)\n    display_name = f\"txn-combined-{datetime.datetime.now().strftime('%m%d')}\"\n    \n    try:\n        tuning_op = client.tunings.tune(\n            base_model=\"models/gemini-1.5-flash-001-tuning\",\n            training_dataset=training_data,\n            config=types.CreateTuningJobConfig(\n                tuned_model_display_name=display_name,\n                batch_size=8,\n                epoch_count=5,\n            ),\n        )\n        \n        combined_model_id = tuning_op.name\n        print(f\"Fine-tuning initiated. Model ID: {combined_model_id}\")\n        print(f\"Current status: {tuning_op.state}\")\n        \n        # Save model ID\n        with open(model_file, \"w\") as f:\n            json.dump({\"combined_model\": combined_model_id}, f, indent=2)\n            \n    except Exception as e:\n        print(f\"Error starting tuning job: {e}\")\n        return None\n    \n    return combined_model_id\n\n# Get model status information\ndef get_model_status(model_id):\n    if not model_id:\n        return {\"state\": \"NOT_FOUND\"}\n    \n    try:\n        model = client.tunings.get(name=model_id)\n        return {\n            \"state\": model.state.name,\n            \"success\": model.has_succeeded,\n            \"ended\": model.has_ended,\n            \"error\": model.error if hasattr(model, \"error\") else None,\n            \"create_time\": model.create_time,\n            \"progress\": model.progress if hasattr(model, \"progress\") else None,\n            \"model_id\": model_id\n        }\n    except Exception as e:\n        return {\"state\": \"ERROR\", \"error\": str(e), \"model_id\": model_id}\n        return {\"state\": \"ERROR\", \"error\": str(e)}\n\n\n\ndef print_model_status(status_dict):\n    \"\"\"Pretty print model status information\"\"\"\n    state = status_dict.get(\"state\", \"UNKNOWN\")\n    create_time = status_dict.get(\"create_time\")\n    progress = status_dict.get(\"progress\")\n    model_id = status_dict.get(\"model_id\", \"Unknown\")\n    \n    print(f\"\\n=== COMBINED MODEL STATUS ===\")\n    print(f\"Model ID: {model_id}\")\n    \n\ndef check_model_statuses():\n    \"\"\"Check and display status for all tuned models\"\"\"\n    try:\n        # Load model IDs\n        with open(\"/kaggle/working/tuned_model_ids.json\", \"r\") as f:\n            combined_model_id = json.load(f)\n        \n        # Get all statuses\n        status = get_model_status(client, combined_model_id)\n        \n        # Print status\n        print_model_status(status)\n        \n    except Exception as e:\n        print(f\"Error checking model status: {e}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T00:35:51.516202Z","iopub.execute_input":"2025-04-18T00:35:51.516649Z","iopub.status.idle":"2025-04-18T00:35:51.537719Z","shell.execute_reply.started":"2025-04-18T00:35:51.516612Z","shell.execute_reply":"2025-04-18T00:35:51.536350Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"model_ids = train_combined_model(df_train_sampled)\nprint(f\"Models are now being trained. IDs saved to {MODEL_IDS_FILE}\")","metadata":{"execution":{"iopub.status.busy":"2025-04-18T00:36:04.805627Z","iopub.execute_input":"2025-04-18T00:36:04.806028Z","iopub.status.idle":"2025-04-18T00:36:04.906218Z","shell.execute_reply.started":"2025-04-18T00:36:04.805995Z","shell.execute_reply":"2025-04-18T00:36:04.905144Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Starting combined model training process...\nCreated 1467 training examples with combined labels\nStarting new combined classification fine-tuning job\nError starting tuning job: name 'client' is not defined\nModels are now being trained. IDs saved to /kaggle/working/tuned_model_ids.json\n","output_type":"stream"}],"execution_count":51},{"cell_type":"markdown","source":"## Monitoring progress\nHere I monitor whether this model has been tuned and ready to use.","metadata":{}},{"cell_type":"code","source":"from google import genai\n\nclient = genai.Client(api_key=YOUR_API_KEY)\n\n# Then train your model\nmodel_id = train_combined_model(client, your_dataframe)\n\ncheck_model_status(client)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T00:25:10.283940Z","iopub.execute_input":"2025-04-18T00:25:10.284394Z","iopub.status.idle":"2025-04-18T00:25:10.311228Z","shell.execute_reply.started":"2025-04-18T00:25:10.284356Z","shell.execute_reply":"2025-04-18T00:25:10.309769Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","Cell \u001b[0;32mIn[43], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m genai\n\u001b[1;32m      3\u001b[0m client \u001b[38;5;241m=\u001b[39m genai\u001b[38;5;241m.\u001b[39mClient(api_key\u001b[38;5;241m=\u001b[39mYOUR_API_KEY)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Then train your model\u001b[39;00m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'genai' from 'google' (unknown location)"],"ename":"ImportError","evalue":"cannot import name 'genai' from 'google' (unknown location)","output_type":"error"}],"execution_count":43},{"cell_type":"markdown","source":"## Evaluate Tuned Model\nHere I test and evaluate the performance of tuned models.","metadata":{}},{"cell_type":"code","source":"import json\nimport os\n\n# Path for storing model IDs\nMODEL_IDS_FILE = \"/kaggle/input/training/tuned_model_ids.json\"\n\ndef get_model_ids():\n    \"\"\"Load model IDs from file or initialize empty structure\"\"\"\n    if os.path.exists(MODEL_IDS_FILE):\n        with open(MODEL_IDS_FILE, \"r\") as f:\n            return json.load(f)\n    return {\n        \"category_model\": None,\n        \"subcategory_models\": {}\n    }\n\n\n# Define retry logic for API rate limits\nis_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n\n@retry.Retry(predicate=is_retriable)\ndef predict_combined_classification(transaction_note, model_id=None):\n    \"\"\"\n    Predict category and subcategory using a single combined model\n    \n    Args:\n        transaction_note: The transaction text to classify\n        model_id: Optional model ID to use\n        \n    Returns:\n        tuple: (category, subcategory)\n    \"\"\"\n    # Get model ID if not provided\n    if not model_id:\n        try:\n            with open(\"/kaggle/working/combined_model_id.json\", \"r\") as f:\n                model_id = json.load(f).get(\"combined_model\")\n        except:\n            return \"(no model available)\", \"(no model available)\"\n    \n    # Get valid category/subcategory combinations from database\n    cursor = db_conn.cursor()\n    cursor.execute(\"\"\"\n    SELECT c.name || '/' || s.name as combined\n    FROM categories c\n    JOIN subcategories s ON c.category_id = s.category_id\n    \"\"\")\n    valid_combinations = [row[0] for row in cursor.fetchall()]\n    \n    try:\n        # Get prediction from model\n        response = client.models.generate_content(\n            model=model_id,\n            contents=transaction_note,\n            config=types.GenerateContentConfig(\n                temperature=0.0,\n                max_output_tokens=20\n            )\n        )\n        \n        if response.candidates and response.candidates[0].content:\n            prediction = response.candidates[0].content.parts[0].text.strip()\n            \n            # Validate prediction against database\n            if prediction not in valid_combinations:\n                # Try to find closest match\n                from difflib import get_close_matches\n                matches = get_close_matches(prediction, valid_combinations, n=1, cutoff=0.6)\n                if matches:\n                    prediction = matches[0]\n                else:\n                    return \"(invalid prediction)\", \"(invalid prediction)\"\n            \n            # Split prediction into category and subcategory\n            parts = prediction.split('/', 1)\n            if len(parts) == 2:\n                return parts[0], parts[1]\n            else:\n                return prediction, \"(parsing error)\"\n        else:\n            return \"(no response)\", \"(no response)\"\n            \n    except Exception as e:\n        return f\"(error: {str(e)})\", f\"(error: {str(e)})\"\n\ndef predict_transaction_category(transaction_note):\n    \"\"\"Predict just the category using the category model with validation\"\"\"\n    model_ids = get_model_ids()\n    category_model_id = model_ids[\"category_model\"]\n    \n    if not category_model_id:\n        return \"(no category model available)\"\n    \n    # Get valid categories from database\n    cursor = db_conn.cursor()\n    cursor.execute(\"SELECT name FROM categories\")\n    valid_categories = [row[0] for row in cursor.fetchall()]\n    \n    return predict_with_model(category_model_id, transaction_note, valid_categories)\n\ndef predict_transaction_subcategory(transaction_note, category=None):\n    \"\"\"\n    Predict the subcategory using the appropriate model with validation.\n    \"\"\"\n    model_ids = get_model_ids()\n    \n    # If category not provided, predict it first\n    if not category:\n        category = predict_transaction_category(transaction_note)\n        if category.startswith(\"(\"):  # Error or no model\n            return category\n    \n    # Use category-specific subcategory model if available\n    subcategory_model_id = model_ids[\"subcategory_models\"].get(category)\n    \n    if not subcategory_model_id:\n        return f\"(no subcategory model for '{category}')\"\n    \n    # Get valid subcategories for this category from database\n    cursor = db_conn.cursor()\n    cursor.execute(\"\"\"\n    SELECT s.name FROM subcategories s\n    JOIN categories c ON s.category_id = c.category_id\n    WHERE c.name = ?\n    \"\"\", (category,))\n    valid_subcategories = [row[0] for row in cursor.fetchall()]\n    \n    return predict_with_model(subcategory_model_id, transaction_note, valid_subcategories)\n\ndef categorize_transaction(transaction_note):\n    \"\"\"\n    Full hierarchical prediction: predict category, then subcategory.\n    \n    Returns:\n        tuple: (category, subcategory, confidence_score)\n    \"\"\"\n    # First, predict the category\n    category = predict_transaction_category(transaction_note)\n    \n    # Handle category prediction errors\n    if category.startswith(\"(error\") or category.startswith(\"(no model\"):\n        return category, \"(category prediction failed)\", 0.0\n    \n    # Next, predict the subcategory using the category-specific model\n    subcategory = predict_transaction_subcategory(transaction_note, category)\n    \n    # For now, we don't have confidence scores from the API\n    # Future enhancement: implement a confidence estimation method\n    confidence = 1.0 if not subcategory.startswith(\"(\") else 0.0\n    \n    return category, subcategory, confidence\n\ndef evaluate_combined_model(df_test, model_id=None, sample_size=20):\n    \"\"\"\n    Evaluate the combined category/subcategory model\n    \n    Args:\n        df_test: Test DataFrame\n        model_id: Optional model ID to use\n        sample_size: Number of samples to evaluate\n        \n    Returns:\n        DataFrame with evaluation results\n    \"\"\"\n    # Get the model ID if not provided\n    if not model_id:\n        try:\n            with open(\"/kaggle/working/combined_model_id.json\", \"r\") as f:\n                model_id = json.load(f).get(\"combined_model\")\n                \n            if not model_id:\n                print(\"No combined model ID found. Please train a model first.\")\n                return None\n        except:\n            print(\"No combined model ID file found. Please train a model first.\")\n            return None\n    \n    # Sample the test data\n    if len(df_test) > sample_size:\n        eval_df = df_test.sample(sample_size)\n    else:\n        eval_df = df_test.copy()\n    \n    # Ensure we have the combined label\n    if 'combined_label' not in eval_df.columns:\n        eval_df['combined_label'] = eval_df['category'] + \"/\" + eval_df['subcategory']\n    \n    # Get valid combinations from the database\n    cursor = db_conn.cursor()\n    cursor.execute(\"\"\"\n    SELECT c.name || '/' || s.name as combined\n    FROM categories c\n    JOIN subcategories s ON c.category_id = s.category_id\n    \"\"\")\n    valid_combinations = [row[0] for row in cursor.fetchall()]\n    \n    # Make predictions\n    results = []\n    print(f\"Evaluating combined model on {len(eval_df)} test transactions...\")\n    \n    for idx, row in tqdmr(eval_df.iterrows(), total=len(eval_df)):\n        transaction = row['note']\n        true_combined = row['combined_label']\n        true_category, true_subcategory = true_combined.split('/', 1)\n        \n        try:\n            # Get prediction from model\n            response = client.models.generate_content(\n                model=model_id,\n                contents=transaction,\n                config=types.GenerateContentConfig(\n                    temperature=0.0,\n                    max_output_tokens=20\n                )\n            )\n            \n            if response.candidates and response.candidates[0].content:\n                pred_combined = response.candidates[0].content.parts[0].text.strip()\n                \n                # Validate prediction against database\n                if pred_combined not in valid_combinations:\n                    # Try to find closest match\n                    matches = get_close_matches(pred_combined, valid_combinations, n=1, cutoff=0.6)\n                    if matches:\n                        print(f\"Corrected prediction: {pred_combined} → {matches[0]}\")\n                        pred_combined = matches[0]\n                    else:\n                        print(f\"Invalid prediction: {pred_combined} (no close match found)\")\n                        pred_combined = \"(invalid prediction)\"\n                \n                # Split prediction into category and subcategory\n                if \"/\" in pred_combined:\n                    pred_category, pred_subcategory = pred_combined.split('/', 1)\n                else:\n                    pred_category, pred_subcategory = pred_combined, \"(parsing error)\"\n            else:\n                pred_combined = \"(no response)\"\n                pred_category, pred_subcategory = \"(no response)\", \"(no response)\"\n                \n        except Exception as e:\n            error_message = str(e)\n            pred_combined = f\"(error: {error_message[:30]}...)\"\n            pred_category, pred_subcategory = \"(error)\", \"(error)\"\n        \n        # Store results\n        results.append({\n            'transaction': transaction,\n            'true_combined': true_combined,\n            'pred_combined': pred_combined,\n            'combined_correct': true_combined == pred_combined,\n            'true_category': true_category,\n            'pred_category': pred_category,\n            'category_correct': true_category == pred_category,\n            'true_subcategory': true_subcategory, \n            'pred_subcategory': pred_subcategory,\n            'subcategory_correct': true_subcategory == pred_subcategory\n        })\n    \n    # Convert to DataFrame\n    results_df = pd.DataFrame(results)\n    \n    # Calculate metrics\n    combined_accuracy = results_df['combined_correct'].mean()\n    category_accuracy = results_df['category_correct'].mean()\n    subcategory_accuracy = results_df['subcategory_correct'].mean()\n    \n    print(f\"Combined label accuracy: {combined_accuracy:.2%}\")\n    print(f\"Category accuracy: {category_accuracy:.2%}\")\n    print(f\"Subcategory accuracy: {subcategory_accuracy:.2%}\")\n    \n    # Show example predictions\n    print(\"\\nSample predictions:\")\n    for idx, row in results_df.sample(min(5, len(results_df))).iterrows():\n        print(f\"Transaction: {row['transaction'][:50]}...\")\n        print(f\"True: {row['true_combined']}\")\n        print(f\"Predicted: {row['pred_combined']}\")\n        print(f\"Correct: {row['combined_correct']}\\n\")\n    \n    # Display confusion matrix for categories (if enough data)\n    if len(results_df) >= 5:\n        print(\"\\nCategory confusion matrix:\")\n        try:\n            cat_matrix = pd.crosstab(\n                results_df['true_category'], \n                results_df['pred_category'],\n                rownames=['True'], \n                colnames=['Predicted']\n            )\n            print(cat_matrix)\n        except Exception as e:\n            print(f\"Unable to generate confusion matrix: {e}\")\n    \n    return results_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T00:18:48.076798Z","iopub.execute_input":"2025-04-18T00:18:48.077209Z","iopub.status.idle":"2025-04-18T00:18:48.109192Z","shell.execute_reply.started":"2025-04-18T00:18:48.077174Z","shell.execute_reply":"2025-04-18T00:18:48.107846Z"}},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":"def list_tables() -> list[str]:\n    \"\"\"Retrieve the names of all tables in the database.\"\"\"\n    # Include print logging statements so you can see when functions are being called.\n    print(' - DB CALL: list_tables()')\n\n    cursor = db_conn.cursor()\n\n    # Fetch the table names.\n    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n\n    tables = cursor.fetchall()\n    return [t[0] for t in tables]\n\n\nlist_tables()","metadata":{"execution":{"iopub.status.busy":"2025-04-13T13:39:24.135559Z","iopub.status.idle":"2025-04-13T13:39:24.135975Z","shell.execute_reply.started":"2025-04-13T13:39:24.135795Z","shell.execute_reply":"2025-04-13T13:39:24.135815Z"}}},{"cell_type":"markdown","source":"def describe_table(table_name: str) -> list[tuple[str, str]]:\n    \"\"\"Look up the table schema.\n\n    Returns:\n      List of columns, where each entry is a tuple of (column, type).\n    \"\"\"\n    print(f' - DB CALL: describe_table({table_name})')\n\n    cursor = db_conn.cursor()\n\n    cursor.execute(f\"PRAGMA table_info({table_name});\")\n\n    schema = cursor.fetchall()\n    # [column index, column name, column type, ...]\n    return [(col[1], col[2]) for col in schema]","metadata":{"execution":{"iopub.status.busy":"2025-04-13T13:39:24.138228Z","iopub.status.idle":"2025-04-13T13:39:24.138587Z","shell.execute_reply.started":"2025-04-13T13:39:24.138420Z","shell.execute_reply":"2025-04-13T13:39:24.138438Z"}}},{"cell_type":"markdown","source":"def execute_query(sql: str) -> list[list[str]]:\n    \"\"\"Execute an SQL statement, returning the results.\"\"\"\n    print(f' - DB CALL: execute_query({sql})')\n\n    cursor = db_conn.cursor()\n\n    cursor.execute(sql)\n    return cursor.fetchall()","metadata":{"execution":{"iopub.status.busy":"2025-04-13T13:39:24.140844Z","iopub.status.idle":"2025-04-13T13:39:24.141229Z","shell.execute_reply.started":"2025-04-13T13:39:24.141056Z","shell.execute_reply":"2025-04-13T13:39:24.141075Z"}}},{"cell_type":"markdown","source":"# These are the database interaction tools defined earlier\ndb_tools = [list_tables, describe_table, execute_query]\n\n# System instruction for the AI to understand what it needs to do\ninstruction = \"\"\"You are a helpful chatbot that can interact with an SQL database for financial transactions. \nYou will first use list_tables to see what tables are present, describe_table to understand the\nschema, and execute_query to issue an SQL SELECT query to retrieve all category-subcategory combinations.\"\"\"\n\n# Create the Google Genai client\nclient = genai.Client(api_key=GOOGLE_API_KEY)\n\nfrom google.api_core import retry\nimport pandas as pd\n\n# Define retry logic for API rate limits\nis_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n\n# User query to get direct category-subcategory combinations\nuser_query = \"\"\"1. Find all unique combinations of category and subcategory from the database.\n2. Execute a SQL query that joins the categories and subcategories tables.\n3. Return the full category name and full subcategory name.\n4. Format your response as simple tabular data that can be saved as CSV.\"\"\"\n\n# Function to get category-subcategory combinations with retry logic\n@retry.Retry(predicate=is_retriable)\ndef get_category_subcategory_combinations():\n    response = client.models.generate_content(\n        model=\"gemini-2.0-flash\",\n        contents=user_query,\n        config=types.GenerateContentConfig(\n            system_instruction=instruction,\n            tools=db_tools,\n        ),\n    )\n    return response.text","metadata":{"execution":{"iopub.status.busy":"2025-04-13T13:39:24.142985Z","iopub.status.idle":"2025-04-13T13:39:24.143358Z","shell.execute_reply.started":"2025-04-13T13:39:24.143188Z","shell.execute_reply":"2025-04-13T13:39:24.143207Z"}}}]}