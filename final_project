{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11325548,"sourceType":"datasetVersion","datasetId":7084116},{"sourceId":11335753,"sourceType":"datasetVersion","datasetId":7091033}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip uninstall -qy jupyterlab  # Remove unused packages from Kaggle's base image that conflict\n!pip install -U -q \"google-genai==1.7.0\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T05:55:06.775684Z","iopub.execute_input":"2025-04-09T05:55:06.776124Z","iopub.status.idle":"2025-04-09T05:55:12.173887Z","shell.execute_reply.started":"2025-04-09T05:55:06.776088Z","shell.execute_reply":"2025-04-09T05:55:12.172581Z"}},"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Skipping jupyterlab as it is not installed.\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"from google import genai\nfrom google.genai import types\nfrom kaggle_secrets import UserSecretsClient\nimport pandas as pd\nimport json\nfrom tqdm.notebook import tqdm\nimport time\nfrom IPython.display import HTML, Markdown, display\n#https://www.kaggle.com/datasets/robbietli/transactions-20250104-test-data-csv","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T05:55:12.175609Z","iopub.execute_input":"2025-04-09T05:55:12.175930Z","iopub.status.idle":"2025-04-09T05:55:12.354844Z","shell.execute_reply.started":"2025-04-09T05:55:12.175900Z","shell.execute_reply":"2025-04-09T05:55:12.353747Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\n\nGOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T05:55:12.356641Z","iopub.execute_input":"2025-04-09T05:55:12.356946Z","iopub.status.idle":"2025-04-09T05:55:12.428813Z","shell.execute_reply.started":"2025-04-09T05:55:12.356922Z","shell.execute_reply":"2025-04-09T05:55:12.427572Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"client = genai.Client(api_key=GOOGLE_API_KEY)\n\n# You can optionally set your model configuration here too\nmodel_config = types.GenerateContentConfig(\n    temperature=0.1,\n    top_p=1,\n    max_output_tokens=250,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T05:55:12.430272Z","iopub.execute_input":"2025-04-09T05:55:12.430625Z","iopub.status.idle":"2025-04-09T05:55:12.559884Z","shell.execute_reply.started":"2025-04-09T05:55:12.430597Z","shell.execute_reply":"2025-04-09T05:55:12.558812Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"from google.api_core import retry\n\n\nis_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n\ngenai.models.Models.generate_content = retry.Retry(\n    predicate=is_retriable)(genai.models.Models.generate_content)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T05:55:12.560973Z","iopub.execute_input":"2025-04-09T05:55:12.561303Z","iopub.status.idle":"2025-04-09T05:55:12.566494Z","shell.execute_reply.started":"2025-04-09T05:55:12.561248Z","shell.execute_reply":"2025-04-09T05:55:12.565492Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# 3. Load your pre-categorized transactions\nprint(\"Loading training data...\")\ntraining_file_path = \"/kaggle/input/my-categorized-transactions/my_categorized_transactions.csv\"\n\ntry:\n    df_training = pd.read_csv(training_file_path)\n    print(f\"Successfully loaded {len(df_training)} training examples\")\n    \n    # Print sample to verify columns\n    print(\"\\nSample of training data:\")\n    print(df_training.head(2))\n    print(\"\\nColumns in training data:\", df_training.columns.tolist())\n    \nexcept FileNotFoundError:\n    print(f\"ERROR: Could not find file at {training_file_path}\")\n    print(\"Please ensure your CSV is uploaded to the correct location\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T05:55:12.567532Z","iopub.execute_input":"2025-04-09T05:55:12.567796Z","iopub.status.idle":"2025-04-09T05:55:12.644328Z","shell.execute_reply.started":"2025-04-09T05:55:12.567773Z","shell.execute_reply":"2025-04-09T05:55:12.643141Z"}},"outputs":[{"name":"stdout","text":"Loading training data...\nSuccessfully loaded 520 training examples\n\nSample of training data:\n                   account       category currency      type payment_type  \\\n0           Rob Bendigo CC  TV, Streaming      AUD  Expenses     TRANSFER   \n1  Rob NAB Credit CC #8958       TRANSFER      AUD  Expenses     TRANSFER   \n\n  payment_type_local                                               note  \\\n0      Bank transfer                     GOOGLE *YouTube 4,650-253-0000   \n1      Bank transfer  12 Pack Self Watering Spikes for Plants, Vacat...   \n\n               date  gps_latitude  gps_longitude  gps_accuracy_in_meters  \\\n0  31/10/2024 11:00           NaN            NaN                     NaN   \n1  31/10/2024 11:00           NaN            NaN                     NaN   \n\n   warranty_in_month  transfer               payee labels  envelope_id  \\\n0                  0     False             Youtube    NaN         6008   \n1                  0      True  Amazon Marketplace    NaN        20001   \n\n   custom_category  \n0            False  \n1            False  \n\nColumns in training data: ['account', 'category', 'currency', 'type', 'payment_type', 'payment_type_local', 'note', 'date', 'gps_latitude', 'gps_longitude', 'gps_accuracy_in_meters', 'warranty_in_month', 'transfer', 'payee', 'labels', 'envelope_id', 'custom_category']\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n  has_large_values = (abs_vals > 1e6).any()\n/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"# 4. Prepare training examples\nprint(\"\\nCreating training examples...\")\ntraining_examples = []\n\n# Check if the required columns exist\nif 'note' not in df_training.columns or 'category' not in df_training.columns:\n    print(\"ERROR: Training data must have 'note' and 'category' columns\")\n    print(f\"Available columns: {df_training.columns.tolist()}\")\nelse:\n    for _, row in df_training.iterrows():\n        # Create training example with correct fields\n        training_examples.append({\n            \"textInput\": row['note'],  # Description\n            \"output\": row['category']  # Category:subcategory\n        })\n    \n    print(f\"Created {len(training_examples)} training examples\")\n    \n    # Optionally show a few examples\n    print(\"\\nSample training examples:\")\n    for example in training_examples[:3]:\n        print(f\"Input: '{example['textInput'][:50]}...'\")\n        print(f\"Output: '{example['output']}'\")\n        print(\"---\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T05:55:59.981540Z","iopub.execute_input":"2025-04-09T05:55:59.981930Z","iopub.status.idle":"2025-04-09T05:56:00.021807Z","shell.execute_reply.started":"2025-04-09T05:55:59.981898Z","shell.execute_reply":"2025-04-09T05:56:00.020643Z"}},"outputs":[{"name":"stdout","text":"\nCreating training examples...\nCreated 520 training examples\n\nSample training examples:\nInput: 'GOOGLE *YouTube 4,650-253-0000...'\nOutput: 'TV, Streaming'\n---\nInput: '12 Pack Self Watering Spikes for Plants, Vacation ...'\nOutput: 'TRANSFER'\n---\nInput: '200 Feet Self watering Wick Cord for Vacation Self...'\nOutput: 'Home, garden'\n---\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"# 5. Start the fine-tuning process\nif len(training_examples) > 0:\n    print(\"\\nStarting fine-tuning process...\")\n    \n    # Prepare training data in the required format\n    training_data = {\"examples\": training_examples}\n    \n    # Start the fine-tuning job\n    try:\n        tuning_op = client.tunings.tune(\n            base_model=\"models/gemini-1.5-flash-001-tuning\",\n            training_dataset=training_data,\n            config=types.CreateTuningJobConfig(\n                tuned_model_display_name=\"Personal Transaction Classifier\",\n                batch_size=16,\n                epoch_count=3,\n            ),\n        )\n        \n        print(f\"Fine-tuning initiated. Model ID: {tuning_op.name}\")\n        print(f\"Status: {tuning_op.state}\")\n        \n        # Save the model ID for later use\n        with open(\"tuned_model_id.txt\", \"w\") as f:\n            f.write(tuning_op.name)\n        \n    except Exception as e:\n        print(f\"Error starting fine-tuning job: {e}\")\nelse:\n    print(\"No training examples created. Please fix the issues above.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T05:56:22.865199Z","iopub.execute_input":"2025-04-09T05:56:22.865571Z","iopub.status.idle":"2025-04-09T05:56:22.876057Z","shell.execute_reply.started":"2025-04-09T05:56:22.865539Z","shell.execute_reply":"2025-04-09T05:56:22.874751Z"}},"outputs":[{"name":"stdout","text":"\nStarting fine-tuning process...\nError starting fine-tuning job: 22 validation errors for _CreateTuningJobParameters\ntraining_dataset.examples.35.textInput\n  Input should be a valid string [type=string_type, input_value=nan, input_type=float]\n    For further information visit https://errors.pydantic.dev/2.11/v/string_type\ntraining_dataset.examples.36.textInput\n  Input should be a valid string [type=string_type, input_value=nan, input_type=float]\n    For further information visit https://errors.pydantic.dev/2.11/v/string_type\ntraining_dataset.examples.38.textInput\n  Input should be a valid string [type=string_type, input_value=nan, input_type=float]\n    For further information visit https://errors.pydantic.dev/2.11/v/string_type\ntraining_dataset.examples.99.textInput\n  Input should be a valid string [type=string_type, input_value=nan, input_type=float]\n    For further information visit https://errors.pydantic.dev/2.11/v/string_type\ntraining_dataset.examples.109.textInput\n  Input should be a valid string [type=string_type, input_value=nan, input_type=float]\n    For further information visit https://errors.pydantic.dev/2.11/v/string_type\ntraining_dataset.examples.111.textInput\n  Input should be a valid string [type=string_type, input_value=nan, input_type=float]\n    For further information visit https://errors.pydantic.dev/2.11/v/string_type\ntraining_dataset.examples.112.textInput\n  Input should be a valid string [type=string_type, input_value=nan, input_type=float]\n    For further information visit https://errors.pydantic.dev/2.11/v/string_type\ntraining_dataset.examples.183.textInput\n  Input should be a valid string [type=string_type, input_value=nan, input_type=float]\n    For further information visit https://errors.pydantic.dev/2.11/v/string_type\ntraining_dataset.examples.185.textInput\n  Input should be a valid string [type=string_type, input_value=nan, input_type=float]\n    For further information visit https://errors.pydantic.dev/2.11/v/string_type\ntraining_dataset.examples.186.textInput\n  Input should be a valid string [type=string_type, input_value=nan, input_type=float]\n    For further information visit https://errors.pydantic.dev/2.11/v/string_type\ntraining_dataset.examples.255.textInput\n  Input should be a valid string [type=string_type, input_value=nan, input_type=float]\n    For further information visit https://errors.pydantic.dev/2.11/v/string_type\ntraining_dataset.examples.256.textInput\n  Input should be a valid string [type=string_type, input_value=nan, input_type=float]\n    For further information visit https://errors.pydantic.dev/2.11/v/string_type\ntraining_dataset.examples.258.textInput\n  Input should be a valid string [type=string_type, input_value=nan, input_type=float]\n    For further information visit https://errors.pydantic.dev/2.11/v/string_type\ntraining_dataset.examples.259.textInput\n  Input should be a valid string [type=string_type, input_value=nan, input_type=float]\n    For further information visit https://errors.pydantic.dev/2.11/v/string_type\ntraining_dataset.examples.261.textInput\n  Input should be a valid string [type=string_type, input_value=nan, input_type=float]\n    For further information visit https://errors.pydantic.dev/2.11/v/string_type\ntraining_dataset.examples.372.textInput\n  Input should be a valid string [type=string_type, input_value=nan, input_type=float]\n    For further information visit https://errors.pydantic.dev/2.11/v/string_type\ntraining_dataset.examples.374.textInput\n  Input should be a valid string [type=string_type, input_value=nan, input_type=float]\n    For further information visit https://errors.pydantic.dev/2.11/v/string_type\ntraining_dataset.examples.375.textInput\n  Input should be a valid string [type=string_type, input_value=nan, input_type=float]\n    For further information visit https://errors.pydantic.dev/2.11/v/string_type\ntraining_dataset.examples.449.textInput\n  Input should be a valid string [type=string_type, input_value=nan, input_type=float]\n    For further information visit https://errors.pydantic.dev/2.11/v/string_type\ntraining_dataset.examples.450.textInput\n  Input should be a valid string [type=string_type, input_value=nan, input_type=float]\n    For further information visit https://errors.pydantic.dev/2.11/v/string_type\ntraining_dataset.examples.452.textInput\n  Input should be a valid string [type=string_type, input_value=nan, input_type=float]\n    For further information visit https://errors.pydantic.dev/2.11/v/string_type\ntraining_dataset.examples.463.textInput\n  Input should be a valid string [type=string_type, input_value=nan, input_type=float]\n    For further information visit https://errors.pydantic.dev/2.11/v/string_type\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-23-4dbd052d670e>:10: ExperimentalWarning: The SDK's tuning implementation is experimental, and may change in future versions.\n  tuning_op = client.tunings.tune(\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"from google import genai\nfrom google.genai import types\nfrom kaggle_secrets import UserSecretsClient\n\n# Get API key\nGOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\nclient = genai.Client(api_key=GOOGLE_API_KEY)\n\n# Training data from step 1\ntraining_data = {\"examples\": training_examples}\n\n# Start fine-tuning\ntuning_op = client.tunings.tune(\n    base_model=\"models/gemini-1.5-flash-001-tuning\",\n    training_dataset=training_data,\n    config=types.CreateTuningJobConfig(\n        tuned_model_display_name=\"Personal Transaction Classifier\",\n        batch_size=16,\n        epoch_count=3,  # Slightly increased for better learning\n    ),\n)\n\n# Print status and save the model name for later use\nprint(f\"Fine-tuning initiated. Model ID: {tuning_op.name}\")\nprint(f\"Status: {tuning_op.state}\")\n\n# Save the model ID to a file for later reference\nwith open(\"tuned_model_id.txt\", \"w\") as f:\n    f.write(tuning_op.name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T05:55:12.677586Z","iopub.status.idle":"2025-04-09T05:55:12.678099Z","shell.execute_reply":"2025-04-09T05:55:12.677871Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def categorize_transaction_with_tuned_model(description):\n    \"\"\"Use fine-tuned model to categorize a transaction\"\"\"\n    \n    # Load the model ID from the file\n    with open(\"tuned_model_id.txt\", \"r\") as f:\n        model_id = f.read().strip()\n    \n    # Call the fine-tuned model directly with just the description\n    response = client.models.generate_content(\n        model=model_id,  # Your fine-tuned model ID\n        contents=[{\"text\": description}]  # Just the raw description, no prompt needed\n    )\n    \n    # Parse the response - the model outputs direct category:subcategory format\n    result_text = response.text.strip()\n    \n    # Handle the category:subcategory format\n    # The model should output in the exact format from your training data\n    return result_text\n\ndef process_transactions_with_tuned_model(descriptions, batch_size=10, show_progress=True):\n    \"\"\"Process transactions using the fine-tuned model\"\"\"\n    all_results = []\n    \n    # Setup progress bar\n    if show_progress:\n        pbar = tqdm(total=len(descriptions), desc=\"Categorizing transactions\")\n    \n    # Process in batches\n    for i in range(0, len(descriptions), batch_size):\n        batch = descriptions[i:i+batch_size]\n        batch_results = []\n        \n        print(f\"Processing batch {i//batch_size + 1} ({i} to {min(i+batch_size, len(descriptions))})...\")\n        \n        # Process each transaction in the current batch\n        for description in batch:\n            try:\n                result = categorize_transaction_with_tuned_model(description)\n                batch_results.append(result)\n                \n            except Exception as e:\n                print(f\"Error processing '{description}': {e}\")\n                batch_results.append(\"Unknown\")\n            \n            # Update progress bar\n            if show_progress:\n                pbar.update(1)\n        \n        # Add batch results to overall results\n        all_results.extend(batch_results)\n        \n        # Show sample result\n        if len(batch) > 0:\n            print(f\"Sample result - '{batch[0]}': Category='{batch_results[0]}'\")\n            print(\"-\" * 50)\n    \n    # Close progress bar\n    if show_progress:\n        pbar.close()\n    \n    return all_results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T05:55:12.679351Z","iopub.status.idle":"2025-04-09T05:55:12.679833Z","shell.execute_reply":"2025-04-09T05:55:12.679632Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate_tuned_model_performance(test_df, actual_column='category', description_column='note'):\n    \"\"\"Evaluate the performance of the fine-tuned model on test data\"\"\"\n    \n    # Get predictions for all descriptions\n    descriptions = test_df[description_column].tolist()\n    predicted_categories = process_transactions_with_tuned_model(descriptions)\n    \n    # Calculate accuracy\n    test_df['predicted'] = predicted_categories\n    accuracy = (test_df[actual_column] == test_df['predicted']).mean()\n    \n    # Look at errors\n    errors = test_df[test_df[actual_column] != test_df['predicted']]\n    \n    # Focus on Transfer errors\n    false_transfers = errors[errors['predicted'] == 'Transfer']\n    transfer_as_others = errors[(errors[actual_column] == 'Transfer') & (errors['predicted'] != 'Transfer')]\n    \n    print(f\"Overall accuracy: {accuracy:.2%}\")\n    print(f\"Number of errors: {len(errors)}\")\n    print(f\"False transfers (wrongly classified as Transfer): {len(false_transfers)}\")\n    print(f\"Transfers classified as something else: {len(transfer_as_others)}\")\n    \n    # Sample of each error type\n    print(\"\\nSample false transfers:\")\n    print(false_transfers[[description_column, actual_column, 'predicted']].head())\n    \n    print(\"\\nSample transfers classified as something else:\")\n    print(transfer_as_others[[description_column, actual_column, 'predicted']].head())\n    \n    return accuracy, errors","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T05:55:12.680794Z","iopub.status.idle":"2025-04-09T05:55:12.681243Z","shell.execute_reply":"2025-04-09T05:55:12.681073Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. First prepare your training data (run once)\n# (code from Step 1)\n\n# 2. Submit the fine-tuning job (run once)\n# (code from Step 2)\n\n# 3. Process transactions with your fine-tuned model (for ongoing use)\ndef main():\n    file_path = \"/kaggle/input/finance-test/transactions_20250104_test.csv\"\n    df = pd.read_csv(file_path)\n    \n    # Process transactions using the fine-tuned model\n    results = process_transactions_with_tuned_model(df['description'].tolist())\n    \n    # Convert results to category/subcategory format for your CSV\n    categories = []\n    subcategories = []\n    \n    for result in results:\n        # Handle different possible formats from your training data\n        if ':' in result:\n            # If model returns \"Category:Subcategory\"\n            cat, subcat = result.split(':', 1)\n            categories.append(cat)\n            subcategories.append(subcat)\n        else:\n            # If model returns just the category\n            categories.append(result)\n            subcategories.append(\"\")\n    \n    # Add results to dataframe\n    df['category'] = categories\n    df['subcategory'] = subcategories\n    \n    # Save results\n    df.to_csv(\"fine_tuned_categorized_transactions.csv\", index=False)\n    print(\"Results saved to fine_tuned_categorized_transactions.csv\")\n    \n    # Display sample\n    print(\"\\nSample of results:\")\n    print(df[['description', 'category', 'subcategory']].head(20))\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T05:55:12.682854Z","iopub.status.idle":"2025-04-09T05:55:12.683486Z","shell.execute_reply":"2025-04-09T05:55:12.683155Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"def categorise_transaction(description):\n    few_shot_prompt = f\"\"\"\n    You are an AI assistant specializing in financial transaction categorization.\n    Your task is to analyze the provided transaction description and assign it to the most appropriate category and subcategory from the predefined lists.\n\n    **Allowed Categories:**\n    [   - Food & Beverages\n       - Shopping\n       - Housing\n       - Transportation\n       - Vehicle\n       - Life & Entertainment\n       - Communication, PC\n       - Financial expenses\n       - Investments\n       - Income\n    ]\n\n    **Allowed Subcategories (grouped by category for context):**\n    [\"Food & Beverages\": [\"Bar, cafe, drink, snacks\", \"Groceries\", \"Restaurant, fast-food\"],\n       \"Shopping\": [\"Clothes & Footwear\", \"Drug-store, chemist\", \"Electronics, accessories\",\n                    \"Gifts, joy\", \"Health and beauty\", \"Home, garden\", \"Jewels, accessories\",\n                    \"Kids\", \"Leisure time\", \"Pets, animals\", \"Stationery, tools\"],\n       \"Housing\": [\"Energy, utilities\", \"Maintenance, repairs\", \"Mortgage\", \"Property insurance\",\n                   \"Rent\", \"Services\"],\n       \"Transportation\": [\"Business trips\", \"Long distance\", \"Public transport\", \"Taxi\"],\n       \"Vehicle\": [\"Fuel\", \"Leasing\", \"Parking\", \"Rentals\", \"Vehicle insurance\", \"Vehicle maintenance\"],\n       \"Life & Entertainment\": [\"Active sport, fitness\", \"Alcohol, tobacco\", \"Books, audio, subscriptions\",\n                                \"Charity, gifts\", \"Culture, sport events\", \"Education, development\",\n                                \"Health care, doctor\", \"Hobbies\", \"Holiday, trips, hotels\",\n                                \"Life events\", \"Lottery, gambling\", \"TV, Streaming\", \"Wellness, beauty\"],\n       \"Communication, PC\": [\"Internet\", \"Postal services\", \"Software, apps, games\", \"Telephony, mobile phone\"],\n       \"Financial expenses\": [\"Advisory\", \"Charges, Fees\", \"Child Support\", \"Fines\", \"Insurances\",\n                              \"Loans, interests\", \"Taxes\"],\n       \"Investments\": [\"Collections\", \"Financial investments\", \"Realty\", \"Savings\", \"Vehicles, chattels\"],\n       \"Income\": [\"Checks, coupons\", \"Child Support\", \"Dues & grants\", \"Gifts\", \"Interests, dividends\",\n                  \"Lending, renting\", \"Lottery, gambling\", \"Refunds (tax, purchase)\",\n                  \"Rental income\", \"Sale\", \"Wage, invoices\"]\n       \"Transfer\":[\"Transfer\"]\n    ]\n\n    **Instructions:**\n    [...] Respond ONLY with a JSON object [...]\n\n    **Examples:**\n\n    * **Transaction Description:** \"BP Fuel Richmond VIC\"\n        **Output:** {{\"category\": \"Transportation\", \"subcategory\": \"Fuel\"}}\n\n    * **Transaction Description:** \"Salary Deposit ACME Corp\"\n        **Output:** {{\"category\": \"Income\", \"subcategory\": \"Salary\"}}\n\n    ---\n    **Now, categorize the following transaction:**\n\n    **Description:**\n    \"{description}\"\n\n    Output\n    \"\"\"\n    return few_shot_prompt\n","metadata":{"execution":{"iopub.status.busy":"2025-04-09T03:46:19.060763Z","iopub.execute_input":"2025-04-09T03:46:19.061261Z","iopub.status.idle":"2025-04-09T03:46:19.066629Z","shell.execute_reply.started":"2025-04-09T03:46:19.061199Z","shell.execute_reply":"2025-04-09T03:46:19.065605Z"}}},{"cell_type":"code","source":"file_path = \"/kaggle/input/finance-test/transactions_20250104_test.csv\"\nimport pandas as pd\nimport json\ndf = pd.read_csv(file_path)\n\n# Import tqdm for progress bar\nfrom tqdm.notebook import tqdm\nimport time\n\ndef process_transactions_in_batches(descriptions, batch_size=5, show_progress=True):\n    all_categories = []\n    all_subcategories = []\n    \n    # Setup progress bar\n    if show_progress:\n        pbar = tqdm(total=len(descriptions), desc=\"Categorizing transactions\")\n    \n    # Process in batches\n    for i in range(0, len(descriptions), batch_size):\n        batch = descriptions[i:i+batch_size]\n        batch_categories = []\n        batch_subcategories = []\n        \n        print(f\"Processing batch {i//batch_size + 1} ({i} to {min(i+batch_size, len(descriptions))})...\")\n        \n        # Process each transaction in the current batch\n        for description in batch:\n            try:\n                # Get the formatted prompt using the function\n                prompt_text = categorise_transaction(description)\n                \n                # Start timing the API call\n                start_time = time.time()\n                \n                response = client.models.generate_content(\n                    model='gemini-2.0-flash',\n                    config=types.GenerateContentConfig(\n                        temperature=0.1,\n                        top_p=1,\n                        max_output_tokens=250,\n                    ),\n                    contents=[{\"text\": prompt_text}]  # Send the already formatted prompt\n                )\n                \n                # Calculate API call duration\n                api_time = time.time() - start_time\n                \n                # Extract category and subcategory from the JSON response\n                response_text = response.text\n                # Find JSON object using braces\n                json_start = response_text.find('{')\n                json_end = response_text.rfind('}') + 1\n                \n                if json_start >= 0 and json_end > json_start:\n                    json_str = response_text[json_start:json_end]\n                    result = json.loads(json_str)\n                    category = result.get('category', 'Unknown')\n                    subcategory = result.get('subcategory', 'Unknown')\n                else:\n                    category = 'Error'\n                    subcategory = 'Could not parse response'\n                    print(f\"Parse error for '{description}'. Response: {response_text[:100]}...\")\n                    \n            except json.JSONDecodeError as je:\n                print(f\"JSON error for '{description}': {je}\")\n                print(f\"Response text: {response_text if 'response_text' in locals() else 'No response text'}\")\n                category = 'Error'\n                subcategory = 'JSON parse error'\n            except Exception as e:\n                print(f\"Error processing '{description}': {e}\")\n                print(f\"Response was: {response.text if 'response' in locals() else 'No response'}\")\n                category = 'Error'\n                subcategory = str(e)\n            \n            batch_categories.append(category)\n            batch_subcategories.append(subcategory)\n            \n            # Update progress bar\n            if show_progress:\n                pbar.update(1)\n        \n        # Add batch results to overall results\n        all_categories.extend(batch_categories)\n        all_subcategories.extend(batch_subcategories)\n        \n        # Show detailed sample result\n        if len(batch) > 0:\n            print(f\"Sample result - '{batch[0]}': Category='{batch_categories[0]}', Subcategory='{batch_subcategories[0]}'\")\n            print(f\"API response time: {api_time:.2f} seconds\")\n            print(\"-\" * 50)\n    \n    # Close progress bar\n    if show_progress:\n        pbar.close()\n    \n    return all_categories, all_subcategories\n\n# Function to save results to CSV\ndef save_results_to_csv(df, filename=\"categorized_transactions.csv\"):\n    df.to_csv(filename, index=False)\n    print(f\"Results saved to {filename}\")\n    return filename","metadata":{"execution":{"iopub.status.busy":"2025-04-09T05:55:12.684545Z","iopub.status.idle":"2025-04-09T05:55:12.684990Z","shell.execute_reply":"2025-04-09T05:55:12.684805Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# For processing the full dataset (uncomment when ready)\nprint(\"\\nProcessing full dataset...\")\nfull_categories, full_subcategories = process_transactions_in_batches(\n    df['description'].tolist(), \n    batch_size=10,  # Increased batch size for faster processing\n    show_progress=True\n)\n\n# Add results to the main dataframe\ndf['category'] = full_categories\ndf['subcategory'] = full_subcategories\n\n# Save full results\nsave_results_to_csv(df, \"full_categorized_transactions.csv\")\n\n# Display a sample of results\nprint(\"\\nSample of Final Results:\")\nprint(df[['description', 'category', 'subcategory']].head(20))\n","metadata":{"execution":{"iopub.status.busy":"2025-04-09T03:46:19.629058Z","iopub.execute_input":"2025-04-09T03:46:19.629507Z","iopub.status.idle":"2025-04-09T03:46:55.076255Z","shell.execute_reply.started":"2025-04-09T03:46:19.629465Z","shell.execute_reply":"2025-04-09T03:46:55.075134Z"}}}]}