{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11325548,"sourceType":"datasetVersion","datasetId":7084116},{"sourceId":11335753,"sourceType":"datasetVersion","datasetId":7091033}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip uninstall -qy jupyterlab  # Remove unused packages from Kaggle's base image that conflict\n!pip install -U -q \"google-genai==1.7.0\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T12:48:15.339879Z","iopub.execute_input":"2025-04-09T12:48:15.340294Z","iopub.status.idle":"2025-04-09T12:48:20.817276Z","shell.execute_reply.started":"2025-04-09T12:48:15.340268Z","shell.execute_reply":"2025-04-09T12:48:20.815877Z"}},"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Skipping jupyterlab as it is not installed.\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"from google import genai\nfrom google.genai import types\nfrom kaggle_secrets import UserSecretsClient\nimport pandas as pd\nimport json\nfrom tqdm.notebook import tqdm\nimport time\nfrom IPython.display import HTML, Markdown, display","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T12:48:20.818934Z","iopub.execute_input":"2025-04-09T12:48:20.819251Z","iopub.status.idle":"2025-04-09T12:48:20.824741Z","shell.execute_reply.started":"2025-04-09T12:48:20.819225Z","shell.execute_reply":"2025-04-09T12:48:20.823612Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\n\nGOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T12:48:20.827093Z","iopub.execute_input":"2025-04-09T12:48:20.827439Z","iopub.status.idle":"2025-04-09T12:48:21.008910Z","shell.execute_reply.started":"2025-04-09T12:48:20.827401Z","shell.execute_reply":"2025-04-09T12:48:21.008066Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"import sqlite3\n\n# Connect to your database\ndb_conn = sqlite3.connect('/kaggle/working/transaction_categories.db')\ncursor = db_conn.cursor()\n\n# Create the tables\ncursor.execute('''\nCREATE TABLE IF NOT EXISTS categories (\n    category_id INTEGER PRIMARY KEY AUTOINCREMENT,\n    name VARCHAR(100) NOT NULL UNIQUE,\n    description TEXT,\n    display_order INT DEFAULT 100,\n    is_active BOOLEAN DEFAULT 1,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n)\n''')\n\ncursor.execute('''\nCREATE TABLE IF NOT EXISTS subcategories (\n    subcategory_id INTEGER PRIMARY KEY AUTOINCREMENT,\n    category_id INTEGER NOT NULL,\n    name VARCHAR(100) NOT NULL,\n    description TEXT,\n    display_order INT DEFAULT 100,\n    is_active BOOLEAN DEFAULT 1,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    FOREIGN KEY (category_id) REFERENCES categories(category_id)\n)\n''')\n\ncursor.execute('''\nCREATE TABLE IF NOT EXISTS keyword_rules (\n    rule_id INTEGER PRIMARY KEY AUTOINCREMENT,\n    category_id INTEGER NOT NULL,\n    subcategory_id INTEGER,\n    keyword VARCHAR(255) NOT NULL,\n    match_type VARCHAR(20) DEFAULT 'contains',\n    priority INT DEFAULT 100,\n    is_active BOOLEAN DEFAULT 1,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    created_by VARCHAR(100),\n    FOREIGN KEY (category_id) REFERENCES categories(category_id),\n    FOREIGN KEY (subcategory_id) REFERENCES subcategories(subcategory_id)\n)\n''')\n\ncursor.execute('''\nCREATE TABLE IF NOT EXISTS transaction_categories (\n    transaction_id VARCHAR(100) PRIMARY KEY,\n    category_id INTEGER,\n    subcategory_id INTEGER,\n    confidence_score DECIMAL(5,4),\n    categorization_method VARCHAR(50),\n    is_manually_reviewed BOOLEAN DEFAULT 0,\n    categorized_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    FOREIGN KEY (category_id) REFERENCES categories(category_id),\n    FOREIGN KEY (subcategory_id) REFERENCES subcategories(subcategory_id)\n)\n''')\n\n# Insert main categories\ncategories = [\n    ('Food & Beverages', 'Expenses related to food and drinks', 10),\n    ('Shopping', 'Retail purchases and shopping expenses', 20),\n    ('Housing', 'Home-related expenses including rent and utilities', 30),\n    ('Transportation', 'Public and private transportation costs', 40),\n    ('Vehicle', 'Car and vehicle related expenses', 50),\n    ('Life & Entertainment', 'Leisure activities and entertainment', 60),\n    ('Communication, PC', 'Internet, phone and computer expenses', 70),\n    ('Financial expenses', 'Banking fees, loans, and financial costs', 80),\n    ('Investments', 'Investment-related transactions', 90),\n    ('Income', 'All sources of incoming money', 100),\n    ('Transfer', 'Money transfers between accounts', 110)\n]\n\ncursor.executemany('INSERT OR IGNORE INTO categories (name, description, display_order) VALUES (?, ?, ?)', categories)\n\n# Insert subcategories for Food & Beverages\nfood_subcategories = [\n    (1, 'Bar, cafe, drink, snacks', 10),\n    (1, 'Groceries', 20),\n    (1, 'Restaurant, fast-food', 30)\n]\ncursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', food_subcategories)\n\n# Insert subcategories for Shopping\nshopping_subcategories = [\n    (2, 'Clothes & Footwear', 10),\n    (2, 'Drug-store, chemist', 20),\n    (2, 'Electronics, accessories', 30),\n    (2, 'Gifts, joy', 40),\n    (2, 'Health and beauty', 50),\n    (2, 'Home, garden', 60),\n    (2, 'Jewels, accessories', 70),\n    (2, 'Kids', 80),\n    (2, 'Leisure time', 90),\n    (2, 'Pets, animals', 100),\n    (2, 'Stationery, tools', 110)\n]\ncursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', shopping_subcategories)\n\nhousing_subcategories=[\n    (3, 'Energy, utilities', 10),\n    (3, 'Maintenance, repairs', 20),\n    (3, 'Mortgage', 30),\n    (3, 'Property insurance', 40),\n    (3, 'Rent', 50),\n    (3, 'Services', 60)\n]\ncursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', housing_subcategories)\n\n\ntransportation_subcategories=[\n    (4, 'Business trips', 10),\n    (4, 'Long distance', 20),\n    (4, 'Public transport', 30),\n    (4, 'Taxi', 40)\n]\ncursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', transportation_subcategories)\n\nvehicle_subcategories=[\n    (5, 'Fuel', 10),\n    (5, 'Leasing', 20),\n    (5, 'Parking', 30),\n    (5, 'Rentals', 40),\n    (5, 'Vehicle insurance', 50),\n    (5, 'Vehicle maintenance', 60)\n]\ncursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', vehicle_subcategories)\n\nlife_subcategories=[\n    (6, 'Active sport, fitness', 10),\n    (6, 'Alcohol, tobacco', 20),\n    (6, 'Books, audio, subscriptions', 30),\n    (6, 'Charity, gifts', 40),\n    (6, 'Culture, sport events', 50),\n    (6, 'Education, development', 60),\n    (6, 'Health care, doctor', 70),\n    (6, 'Hobbies', 80),\n    (6, 'Holiday, trips, hotels', 90),\n    (6, 'Life events', 100),\n    (6, 'Lottery, gambling', 110),\n    (6, 'TV, Streaming', 120),\n    (6, 'Wellness, beauty', 130)\n]\ncursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', life_subcategories)\n\ncommunication_subcategories=[\n    (7, 'Internet', 10),\n    (7, 'Postal services', 20),\n    (7, 'Software, apps, games', 30),\n    (7, 'Telephony, mobile phone', 40)\n]\ncursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', communication_subcategories)\n\nfinancial_subcategories=[\n    (8, 'Advisory', 10),\n    (8, 'Charges, Fees', 20),\n    (8, 'Child Support', 30),\n    (8, 'Fines', 40),\n    (8, 'Insurances', 50),\n    (8, 'Loans, interests', 60),\n    (8, 'Taxes', 70)\n]\ncursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', financial_subcategories)\n\ninvestments_subcategories=[\n    (9, 'Collections', 10),\n    (9, 'Financial investments', 20),\n    (9, 'Realty', 30),\n    (9, 'Savings', 40),\n    (9, 'Vehicles, chattels', 50)\n]\ncursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', investments_subcategories)\n\nincome_subcategories=[\n    (10, 'Checks, coupons', 10),\n    (10, 'Child Support', 20),\n    (10, 'Dues & grants', 30),\n    (10, 'Gifts', 40),\n    (10, 'Interests, dividends', 50),\n    (10, 'Lending, renting', 60),\n    (10, 'Lottery, gambling', 70),\n    (10, 'Refunds (tax, purchase)', 80),\n    (10, 'Rental income', 90),\n    (10, 'Sale', 100),\n    (10, 'Wage, invoices', 110)\n]\ncursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', income_subcategories)\n\ntransfer_subcatgories=[\n    (11, 'Tran', 10),   \n]\ncursor.executemany('INSERT OR IGNORE INTO subcategories (category_id, name, display_order) VALUES (?, ?, ?)', transfer_subcatgories)\n\n# Commit the changes\ndb_conn.commit()\nprint(\"Database schema created successfully!\")\n\n# Close the connection when done\ndb_conn.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T12:48:21.010497Z","iopub.execute_input":"2025-04-09T12:48:21.010864Z","iopub.status.idle":"2025-04-09T12:48:21.042256Z","shell.execute_reply.started":"2025-04-09T12:48:21.010829Z","shell.execute_reply":"2025-04-09T12:48:21.041294Z"}},"outputs":[{"name":"stdout","text":"Database schema created successfully!\n","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"I want to add keyword matching rule table","metadata":{}},{"cell_type":"code","source":"import sqlite3\n\n# Connect to your database\ndb_conn = sqlite3.connect('/kaggle/working/transaction_categories.db')\ncursor = db_conn.cursor()\n\n# Define your rules as Python dictionaries, merging those with the same category/subcategory\nrules = [\n    {\"category\": \"Food & Beverages\", \"subcategory\": \"Bar, cafe, drink, snacks\", \"keywords\": [\n        \"Haddons\", \"Axil\", \"NEW ZEALAND NATURAL\", \"WOODFROG\", \"GONG CHA\", \"miss gourmet\", \n        \"Pigeon Trade Pty Ltd\", \"MS BEAN SOLUTIONS\", \"BEER DELUXE\", \"Tmix\", \"PIGEON TRADE\"\n    ]},\n    {\"category\": \"Shopping\", \"subcategory\": \"Drug-store, chemist\", \"keywords\": [\n        \"chemist warehouse\"\n    ]},\n    {\"category\": \"Shopping\", \"subcategory\": \"Electronics, accessories\", \"keywords\": [\n        \"Nintendo\", \"Playstation\"\n    ]},\n    {\"category\": \"Vehicle\", \"subcategory\": \"Fuel\", \"keywords\": [\n        \"CovaU Pty Ltd - XXX7618\"\n    ]},\n    {\"category\": \"Food & Beverages\", \"subcategory\": \"Groceries\", \"keywords\": [\n        \"woolworth\", \"safeway\", \"coles\", \"renaissance\", \"laguna\", \"tang\", \"fu yao\", \"daiso\", \n        \"hu hui\", \"sao sang\", \"lx group\", \"saccas fruit world\", \"qv asian grocery shop\", \n        \"costco wholesale\", \"aldi\", \"whitehorse fruit life\", \"glenferrie gourmet\", \n        \"box h tanhung asian\", \"bin\", \"JIN FU ASIAN GROCERY\", \"AK & MS\", \"Mario Super\", \n        \"FISH PIER\", \"AMAZING VEGIES PTY\", \"FISH MARKET\"\n    ]},\n    {\"category\": \"Life & Entertainment\", \"subcategory\": \"Hobbies\", \"keywords\": [\n        \"IMAX\", \"Lido\", \"Hoyts\", \"Ls Sporting Shooters\"\n    ]},\n    {\"category\": \"Income\", \"subcategory\": \"Interests, dividends\", \"keywords\": [\n        \"Interest\", \"PX7DT644XRQJXQ6B\"\n    ]},\n    {\"category\": \"Housing\", \"subcategory\": \"Maintenance, repairs\", \"keywords\": [\n        \"IKEA\", \"BUNNINGS\", \"W/D ANYTHIN\"\n    ]},\n    {\"category\": \"Communication, PC\", \"subcategory\": \"Telephony, mobile phone\", \"keywords\": [\n        \"Aldimobile\", \"TELSTRA PRE-PAID\"\n    ]},\n    {\"category\": \"Transportation\", \"subcategory\": \"Public transport\", \"keywords\": [\n        \"Myki\"\n    ]},\n    {\"category\": \"Income\", \"subcategory\": \"Refunds (tax, purchase)\", \"keywords\": [\n        \"Refund\", \"Cashback\"\n    ]},\n    {\"category\": \"Food & Beverages\", \"subcategory\": \"Restaurant, fast-food\", \"keywords\": [\n        \"dosa cafe\", \"rainbox chicken\", \"daily deli\", \"sushi hub\", \"sushi club\", \n        \"lanzhou beef noodle\", \"gensuke\", \"zen japanese\", \"mml group\", \"bluebag fresh\", \n        \"sapa\", \"compassmelbmuseumcafe\", \"sichuan food street\", \"tokyo table\", \"mcdonald\", \n        \"samurai japanese\", \"yueto\", \"sq *grain\", \"osha thai\", \"collins restaurants\", \n        \"thailander\", \"heavenly thai\", \"wang wins pty ltd\", \"dooboo\", \"the resistance bar & c\", \n        \"m & l trading pty lt\", \"guzman y gomez\", \"tandooridencinbrvl\", \"tim ho wan\", \n        \"mml restaurant group\", \"bakers delight\", \"happy house\", \"pacific seafood bbq\", \n        \"kfc\", \"SUSHI JIRO\", \"GHAO PTY\", \"PHOENX KTCHN\", \"HAWKER CHAN\", \"BREAD CITY\", \n        \"GHAO\", \"UNAY PTY LTD\", \"SJ EASTLAND VIC\", \"old street\", \"Breadtop\", \n        \"Christian Pipolo\", \"Stc Group Pty Ltd\"\n    ]},\n    {\"category\": \"Income\", \"subcategory\": \"Wage, invoices\", \"keywords\": [\n        \"seek\"\n    ]},\n    # Add the TRANSFER rule as requested\n    {\"category\": \"Transfer\", \"subcategory\": \"Tran\", \"keywords\": [\n        \"TRANSFER\"\n    ]}\n]\n\n# Add code to insert each rule, ensuring case-insensitive matching\ndef add_rules_to_database(rules):\n    # Configure SQLite for case-insensitive comparison for LIKE operator\n    cursor.execute(\"PRAGMA case_sensitive_like = FALSE;\")\n    \n    total_keywords = 0\n    \n    for rule in rules:\n        # Get category ID\n        cursor.execute(\"SELECT category_id FROM categories WHERE name = ?\", (rule[\"category\"],))\n        category_result = cursor.fetchone()\n        if not category_result:\n            # Create category if it doesn't exist\n            cursor.execute(\"INSERT INTO categories (name) VALUES (?)\", (rule[\"category\"],))\n            category_id = cursor.lastrowid\n        else:\n            category_id = category_result[0]\n        \n        # Get subcategory ID if applicable\n        subcategory_id = None\n        if rule[\"subcategory\"]:\n            cursor.execute(\"SELECT subcategory_id FROM subcategories WHERE name = ? AND category_id = ?\", \n                          (rule[\"subcategory\"], category_id))\n            subcategory_result = cursor.fetchone()\n            if subcategory_result:\n                subcategory_id = subcategory_result[0]\n            else:\n                # Create subcategory if it doesn't exist\n                cursor.execute(\n                    \"INSERT INTO subcategories (category_id, name) VALUES (?, ?)\",\n                    (category_id, rule[\"subcategory\"])\n                )\n                subcategory_id = cursor.lastrowid\n        \n        # Add each keyword as a separate rule\n        for i, keyword in enumerate(rule[\"keywords\"]):\n            try:\n                cursor.execute(\n                    \"\"\"INSERT INTO keyword_rules \n                       (category_id, subcategory_id, keyword, match_type, priority)\n                       VALUES (?, ?, ?, ?, ?)\"\"\",\n                    (category_id, subcategory_id, keyword, 'contains', (i+1)*10)\n                )\n                total_keywords += 1\n            except sqlite3.IntegrityError:\n                print(f\"Skipping duplicate keyword: {keyword}\")\n    \n    # Commit the changes\n    db_conn.commit()\n    print(f\"Added {total_keywords} keyword rules to the database\")\n    print(f\"Created rules for {len(rules)} category/subcategory combinations\")\n\n# Run the function to add all rules\nadd_rules_to_database(rules)\n\n# Print a summary of the rules in the database\ndef print_rule_summary():\n    cursor.execute(\"\"\"\n        SELECT \n            c.name as category, \n            s.name as subcategory, \n            COUNT(kr.rule_id) as keyword_count\n        FROM \n            keyword_rules kr\n            JOIN categories c ON kr.category_id = c.category_id\n            LEFT JOIN subcategories s ON kr.subcategory_id = s.subcategory_id\n        GROUP BY \n            c.name, s.name\n        ORDER BY \n            c.name, s.name\n    \"\"\")\n    \n    results = cursor.fetchall()\n    print(\"\\n=== Rule Summary ===\")\n    print(\"Category | Subcategory | Keywords\")\n    print(\"---------+-------------+---------\")\n    for row in results:\n        category, subcategory, count = row\n        subcategory = subcategory if subcategory else \"N/A\"\n        print(f\"{category} | {subcategory} | {count}\")\n    print(\"=====================\")\n\nprint_rule_summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T12:48:21.043446Z","iopub.execute_input":"2025-04-09T12:48:21.043856Z","iopub.status.idle":"2025-04-09T12:48:21.075100Z","shell.execute_reply.started":"2025-04-09T12:48:21.043819Z","shell.execute_reply":"2025-04-09T12:48:21.074128Z"}},"outputs":[{"name":"stdout","text":"Added 102 keyword rules to the database\nCreated rules for 14 category/subcategory combinations\n\n=== Rule Summary ===\nCategory | Subcategory | Keywords\n---------+-------------+---------\nCommunication, PC | Telephony, mobile phone | 6\nFood & Beverages | Bar, cafe, drink, snacks | 33\nFood & Beverages | Groceries | 75\nFood & Beverages | Restaurant, fast-food | 138\nHousing | Maintenance, repairs | 9\nIncome | Interests, dividends | 6\nIncome | Refunds (tax, purchase) | 6\nIncome | Wage, invoices | 3\nLife & Entertainment | Hobbies | 12\nShopping | Drug-store, chemist | 3\nShopping | Electronics, accessories | 6\nTransfer | Tran | 3\nTransportation | Public transport | 3\nVehicle | Fuel | 3\n=====================\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"import sqlite3\nimport pandas as pd\nimport numpy as np\nfrom google import genai\nfrom google.genai import types\nimport time\nfrom tqdm.notebook import tqdm\n\nclass TransactionCategorizer:\n    def __init__(self, db_path, model_id, api_key):\n        \"\"\"Initialize the categorizer with database and model connections\"\"\"\n        self.db_conn = sqlite3.connect(db_path)\n        self.client = genai.Client(api_key=api_key)\n        self.model_id = model_id\n        self.confidence_threshold = 0.7  # Configure this based on your needs\n        self.search_threshold = 0.5     # Configure this based on your needs\n        \n    def categorize_transaction(self, description):\n        \"\"\"Multi-stage categorization system for a single transaction\"\"\"\n        # Stage 1: Try keyword matching first\n        category, subcategory, keyword_confidence = self.apply_keyword_rules(description)\n        \n        if keyword_confidence > 0:  # If we found a keyword match\n            return {\n                \"category\": category,\n                \"subcategory\": subcategory,\n                \"confidence\": keyword_confidence,\n                \"method\": \"keyword_matching\",\n                \"needs_review\": False\n            }\n        \n        # Stage 2: Compare fine-tuned model vs few-shot prompting\n        ft_category, ft_subcategory, ft_confidence = self.predict_with_finetuned(description)\n        fs_category, fs_subcategory, fs_confidence = self.predict_with_fewshot(description)\n        \n        # Choose the method with higher confidence\n        if ft_confidence >= fs_confidence:\n            category, subcategory, confidence = ft_category, ft_subcategory, ft_confidence\n            method = \"fine_tuned_model\"\n        else:\n            category, subcategory, confidence = fs_category, fs_subcategory, fs_confidence\n            method = \"few_shot_prompting\"\n        \n        # If confidence is good enough, return the result\n        if confidence >= self.confidence_threshold:\n            return {\n                \"category\": category,\n                \"subcategory\": subcategory,\n                \"confidence\": confidence,\n                \"method\": method,\n                \"needs_review\": False\n            }\n        \n        # Stage 3: Try search-based approach if confidence is too low\n        if confidence < self.search_threshold:\n            search_category, search_subcategory, search_confidence = self.search_based_categorization(description)\n            \n            # If search gives better results, use those\n            if search_confidence > confidence:\n                category, subcategory, confidence = search_category, search_subcategory, search_confidence\n                method = \"search_based\"\n        \n        # Stage 4: Flag for review if still low confidence\n        needs_review = confidence < self.confidence_threshold\n        \n        return {\n            \"category\": category, \n            \"subcategory\": subcategory,\n            \"confidence\": confidence,\n            \"method\": method,\n            \"needs_review\": needs_review\n        }\n    \n    def apply_keyword_rules(self, description):\n        \"\"\"Apply keyword matching rules from database\"\"\"\n        cursor = self.db_conn.cursor()\n        \n        # Search for matching keywords in order of priority\n        cursor.execute(\"\"\"\n            SELECT \n                c.name as category, \n                s.name as subcategory,\n                kr.priority as priority\n            FROM \n                keyword_rules kr\n                JOIN categories c ON kr.category_id = c.category_id\n                LEFT JOIN subcategories s ON kr.subcategory_id = s.subcategory_id\n            WHERE \n                LOWER(?) LIKE '%' || LOWER(kr.keyword) || '%'\n            ORDER BY \n                kr.priority ASC\n            LIMIT 1\n        \"\"\", (description,))\n        \n        result = cursor.fetchone()\n        \n        if result:\n            category, subcategory, priority = result\n            # Keyword matches are highly confident\n            confidence = 1.0\n            return category, subcategory or \"\", confidence\n        else:\n            return \"\", \"\", 0.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T12:48:21.076363Z","iopub.execute_input":"2025-04-09T12:48:21.076738Z","iopub.status.idle":"2025-04-09T12:48:21.086878Z","shell.execute_reply.started":"2025-04-09T12:48:21.076702Z","shell.execute_reply":"2025-04-09T12:48:21.085909Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"    def predict_with_finetuned(self, description, retry_count=2):\n        \"\"\"Get prediction using fine-tuned model with confidence estimation\"\"\"\n        for attempt in range(retry_count + 1):\n            try:\n                # Call the fine-tuned model\n                response = self.client.models.generate_content(\n                    model=self.model_id,\n                    contents=[{\"text\": description}],\n                    config=types.GenerateContentConfig(\n                        temperature=0.1,\n                        max_output_tokens=50,\n                        top_p=1,\n                    )\n                )\n                \n                if hasattr(response, 'text') and response.text:\n                    result = response.text.strip()\n                    \n                    # Parse the response - assuming format \"Category: Subcategory\"\n                    if \":\" in result:\n                        category, subcategory = result.split(\":\", 1)\n                        category = category.strip()\n                        subcategory = subcategory.strip()\n                    else:\n                        category = result\n                        subcategory = \"\"\n                    \n                    # Estimate confidence - use model's score if available\n                    confidence = 0.85  # Default confidence\n                    if hasattr(response, 'candidates') and response.candidates:\n                        for candidate in response.candidates:\n                            if hasattr(candidate, 'score'):\n                                confidence = candidate.score\n                    \n                    return category, subcategory, confidence\n                else:\n                    if attempt < retry_count:\n                        time.sleep(2)\n                    else:\n                        return \"\", \"\", 0.0\n            except Exception as e:\n                if attempt < retry_count:\n                    time.sleep(2)\n                else:\n                    print(f\"Fine-tuned model error: {str(e)[:100]}...\")\n                    return \"\", \"\", 0.0\n        \n        return \"\", \"\", 0.0\n    \n    def predict_with_fewshot(self, description):\n        \"\"\"Use few-shot prompting with the base model\"\"\"\n        # Create a prompt with examples from your database\n        examples = self.get_examples_from_db(5)  # Get 5 examples\n        \n        prompt = \"\"\"Categorize this financial transaction into the most appropriate category and subcategory.\nUse only categories and subcategories from the examples below.\n\nExamples:\n\"\"\"\n        \n        for example in examples:\n            prompt += f\"Description: {example['description']}\\nCategory: {example['category']}\\nSubcategory: {example['subcategory']}\\n\\n\"\n        \n        prompt += f\"Now categorize this transaction:\\nDescription: {description}\\n\"\n        \n        try:\n            response = self.client.models.generate_content(\n                model=\"gemini-1.5-flash\",  # Use base model, not fine-tuned\n                contents=[prompt],\n                config=types.GenerateContentConfig(\n                    temperature=0.2,\n                    max_output_tokens=100,\n                )\n            )\n            \n            if hasattr(response, 'text') and response.text:\n                result_text = response.text.strip()\n                \n                # Parse the response\n                category = \"\"\n                subcategory = \"\"\n                \n                for line in result_text.split('\\n'):\n                    if line.lower().startswith(\"category:\"):\n                        category = line.split(\":\", 1)[1].strip()\n                    elif line.lower().startswith(\"subcategory:\"):\n                        subcategory = line.split(\":\", 1)[1].strip()\n                \n                # Estimate confidence based on certainty indicators\n                confidence = 0.75  # Base confidence\n                uncertainty_phrases = [\"uncertain\", \"not sure\", \"possibly\", \"might be\"]\n                if any(phrase in result_text.lower() for phrase in uncertainty_phrases):\n                    confidence -= 0.2\n                \n                return category, subcategory, confidence\n            else:\n                return \"\", \"\", 0.0\n        except Exception as e:\n            print(f\"Few-shot prediction error: {str(e)[:100]}...\")\n            return \"\", \"\", 0.0\n    \n    def get_examples_from_db(self, count=5):\n        \"\"\"Get categorized examples from database for few-shot learning\"\"\"\n        cursor = self.db_conn.cursor()\n        cursor.execute(\"\"\"\n            SELECT tc.transaction_id, tc.category_id, tc.subcategory_id, \n                   c.name as category, s.name as subcategory\n            FROM transaction_categories tc\n            JOIN categories c ON tc.category_id = c.category_id\n            LEFT JOIN subcategories s ON tc.subcategory_id = s.subcategory_id\n            WHERE tc.is_manually_reviewed = 1\n            ORDER BY RANDOM()\n            LIMIT ?\n        \"\"\", (count,))\n        \n        examples = []\n        for row in cursor.fetchall():\n            transaction_id, _, _, category, subcategory = row\n            # You'd need to join with your transactions table to get the description\n            examples.append({\n                \"description\": f\"Transaction #{transaction_id}\",  # Replace with actual description\n                \"category\": category,\n                \"subcategory\": subcategory or \"\"\n            })\n        \n        # If not enough examples in DB, use hardcoded ones\n        if len(examples) < count:\n            default_examples = [\n                {\"description\": \"WOOLWORTHS\", \"category\": \"Food & Beverages\", \"subcategory\": \"Groceries\"},\n                {\"description\": \"MCDONALD\", \"category\": \"Food & Beverages\", \"subcategory\": \"Restaurant, fast-food\"},\n                {\"description\": \"IKEA\", \"category\": \"Housing\", \"subcategory\": \"Maintenance, repairs\"},\n                {\"description\": \"TELSTRA\", \"category\": \"Communication, PC\", \"subcategory\": \"Telephony, mobile phone\"},\n                {\"description\": \"REFUND\", \"category\": \"Income\", \"subcategory\": \"Refunds (tax, purchase)\"}\n            ]\n            examples.extend(default_examples[:count - len(examples)])\n        \n        return examples","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T12:48:21.088184Z","iopub.execute_input":"2025-04-09T12:48:21.088580Z","iopub.status.idle":"2025-04-09T12:48:21.110853Z","shell.execute_reply.started":"2025-04-09T12:48:21.088541Z","shell.execute_reply":"2025-04-09T12:48:21.109860Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"def search_based_categorization(self, description):\n        \"\"\"Use embeddings to find similar transactions\"\"\"\n        try:\n            # Get embedding for the description\n            response = self.client.models.embed_content(\n                model=\"models/text-embedding-004\",\n                contents=description,\n                config=types.EmbedContentConfig(\n                    task_type=\"classification\",\n                ),\n            )\n            \n            if hasattr(response, 'embeddings') and response.embeddings:\n                embedding = response.embeddings[0].values\n                \n                # Here you would search for similar descriptions in your database\n                # using vector similarity search\n                # For now, we'll simulate this with a prompt to Gemini\n                \n                search_prompt = f\"\"\"Based on transaction text: \"{description}\"\n                \nI need to classify this transaction into one of these categories and subcategories:\n- Food & Beverages: [Bar/cafe, Groceries, Restaurant]\n- Shopping: [Clothes, Drug-store, Electronics, etc]\n- Housing: [Utilities, Repairs, Mortgage, Rent]\n- Transportation: [Business trips, Public transport, Taxi]\n- Vehicle: [Fuel, Maintenance, Insurance]\n- Income: [Refunds, Wages, Dividends]\n- Life & Entertainment: [Sports, Health, Hobbies, Travel]\n\nFirst explain your reasoning, then provide your final answer in this exact format:\nCategory: [category]\nSubcategory: [subcategory]\nConfidence: [number between 0 and 1]\"\"\"\n\n                search_response = self.client.models.generate_content(\n                    model=\"gemini-1.5-flash\",\n                    contents=[search_prompt],\n                    config=types.GenerateContentConfig(\n                        temperature=0.1,\n                        max_output_tokens=500,\n                    )\n                )\n                \n                if hasattr(search_response, 'text') and search_response.text:\n                    result_text = search_response.text.strip()\n                    \n                    # Parse the response\n                    category = \"\"\n                    subcategory = \"\"\n                    confidence = 0.4  # Default confidence for search-based\n                    \n                    for line in result_text.split('\\n'):\n                        if line.lower().startswith(\"category:\"):\n                            category = line.split(\":\", 1)[1].strip()\n                        elif line.lower().startswith(\"subcategory:\"):\n                            subcategory = line.split(\":\", 1)[1].strip()\n                        elif line.lower().startswith(\"confidence:\"):\n                            try:\n                                conf_value = float(line.split(\":\", 1)[1].strip())\n                                if 0 <= conf_value <= 1:\n                                    confidence = conf_value\n                            except ValueError:\n                                pass\n                    \n                    return category, subcategory, confidence\n            \n            return \"\", \"\", 0.0\n        except Exception as e:\n            print(f\"Search-based categorization error: {str(e)[:100]}...\")\n            return \"\", \"\", 0.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T12:48:21.112912Z","iopub.execute_input":"2025-04-09T12:48:21.113256Z","iopub.status.idle":"2025-04-09T12:48:21.135285Z","shell.execute_reply.started":"2025-04-09T12:48:21.113229Z","shell.execute_reply":"2025-04-09T12:48:21.134184Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"def process_transactions(file_path, db_path, api_key, model_id, batch_size=5):\n    \"\"\"Process a batch of transactions using the multi-stage system\"\"\"\n    try:\n        # Load transaction data\n        df = pd.read_csv(file_path)\n        print(f\"Loaded {len(df)} transactions\")\n        \n        # Initialize the categorizer\n        categorizer = TransactionCategorizer(db_path, model_id, api_key)\n        \n        # Prepare results storage\n        results = []\n        \n        # Setup progress bar\n        pbar = tqdm(total=len(df), desc=\"Categorizing transactions\")\n        \n        # Process in batches\n        for i in range(0, len(df), batch_size):\n            batch = df.iloc[i:i+batch_size]\n            batch_results = []\n            \n            for _, row in batch.iterrows():\n                description = row['description']\n                if pd.isna(description) or not description:\n                    result = {\n                        \"category\": \"\",\n                        \"subcategory\": \"\",\n                        \"confidence\": 0.0,\n                        \"method\": \"none\",\n                        \"needs_review\": True\n                    }\n                else:\n                    # Apply multi-stage categorization\n                    result = categorizer.categorize_transaction(description)\n                \n                batch_results.append(result)\n                pbar.update(1)\n            \n            results.extend(batch_results)\n            \n            # Show a sample from this batch for monitoring\n            if batch_results:\n                print(f\"\\nSample: '{batch.iloc[0]['description']}'\")\n                print(f\"Result: {batch_results[0]}\")\n                print(\"-\" * 50)\n        \n        # Close progress bar\n        pbar.close()\n        \n        # Add results to dataframe\n        df['category'] = [r['category'] for r in results]\n        df['subcategory'] = [r['subcategory'] for r in results]\n        df['confidence'] = [r['confidence'] for r in results]\n        df['method'] = [r['method'] for r in results]\n        df['needs_review'] = [r['needs_review'] for r in results]\n        \n        # Save results\n        output_file = \"transactions_categorized.csv\"\n        df.to_csv(output_file, index=False)\n        \n        # Show summary\n        print(\"\\nCategorization method summary:\")\n        method_counts = df['method'].value_counts()\n        for method, count in method_counts.items():\n            print(f\"  - {method}: {count} transactions ({count/len(df):.1%})\")\n        \n        review_count = df['needs_review'].sum()\n        print(f\"\\nTransactions needing review: {review_count} ({review_count/len(df):.1%})\")\n        \n        return df\n    \n    except Exception as e:\n        print(f\"Error in processing: {str(e)}\")\n        return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T12:48:21.136472Z","iopub.execute_input":"2025-04-09T12:48:21.136756Z","iopub.status.idle":"2025-04-09T12:48:21.160357Z","shell.execute_reply.started":"2025-04-09T12:48:21.136731Z","shell.execute_reply":"2025-04-09T12:48:21.159332Z"}},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":"client = genai.Client(api_key=GOOGLE_API_KEY)\n\n# You can optionally set your model configuration here too\nmodel_config = types.GenerateContentConfig(\n    temperature=0.1,\n    top_p=1,\n    max_output_tokens=250,\n)","metadata":{"execution":{"iopub.status.busy":"2025-04-09T11:28:17.220602Z","iopub.status.idle":"2025-04-09T11:28:17.221022Z","shell.execute_reply":"2025-04-09T11:28:17.220841Z"}}},{"cell_type":"markdown","source":"from google.api_core import retry\n\n\nis_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n\ngenai.models.Models.generate_content = retry.Retry(\n    predicate=is_retriable)(genai.models.Models.generate_content)","metadata":{"execution":{"iopub.status.busy":"2025-04-09T11:28:17.221804Z","iopub.status.idle":"2025-04-09T11:28:17.222223Z","shell.execute_reply":"2025-04-09T11:28:17.222039Z"}}},{"cell_type":"markdown","source":"# 3. Load your pre-categorized transactions\nprint(\"Loading training data...\")\ntraining_file_path = \"/kaggle/input/my-categorized-transactions/my_categorized_transactions.csv\"\n\ntry:\n    df_training = pd.read_csv(training_file_path)\n    print(f\"Successfully loaded {len(df_training)} training examples\")\n    \n    # Print sample to verify columns\n    print(\"\\nSample of training data:\")\n    print(df_training.head(2))\n    print(\"\\nColumns in training data:\", df_training.columns.tolist())\n    \nexcept FileNotFoundError:\n    print(f\"ERROR: Could not find file at {training_file_path}\")\n    print(\"Please ensure your CSV is uploaded to the correct location\")","metadata":{"execution":{"iopub.status.busy":"2025-04-09T11:28:17.223112Z","iopub.status.idle":"2025-04-09T11:28:17.223565Z","shell.execute_reply":"2025-04-09T11:28:17.223379Z"}}},{"cell_type":"markdown","source":"# 4. Prepare training examples (FIXED VERSION)\nprint(\"\\nCreating training examples...\")\ntraining_examples = []\n\n# Check if the required columns exist\nif 'note' not in df_training.columns or 'category' not in df_training.columns:\n    print(\"ERROR: Training data must have 'note' and 'category' columns\")\n    print(f\"Available columns: {df_training.columns.tolist()}\")\nelse:\n    # Filter out rows with NaN values in the 'note' column\n    df_valid = df_training.dropna(subset=['note'])\n    \n    print(f\"Original data: {len(df_training)} rows\")\n    print(f\"After removing NaN values: {len(df_valid)} rows\")\n    print(f\"Removed {len(df_training) - len(df_valid)} rows with missing descriptions\")\n    \n    for _, row in df_valid.iterrows():\n        # Create training example with correct fields\n        training_examples.append({\n            \"textInput\": str(row['note']),  # Ensure it's a string\n            \"output\": str(row['category'])  # Ensure it's a string\n        })\n    \n    print(f\"Created {len(training_examples)} training examples\")\n    \n    # Optionally show a few examples\n    print(\"\\nSample training examples:\")\n    for example in training_examples[:3]:\n        print(f\"Input: '{example['textInput'][:50]}...'\")\n        print(f\"Output: '{example['output']}'\")\n        print(\"---\")\n\n\n","metadata":{"execution":{"iopub.status.busy":"2025-04-09T11:28:17.224483Z","iopub.status.idle":"2025-04-09T11:28:17.224908Z","shell.execute_reply":"2025-04-09T11:28:17.224724Z"}}},{"cell_type":"markdown","source":"# 5. Start the fine-tuning process (IMPROVED VERSION)\nfrom collections.abc import Iterable\n\nif len(training_examples) > 0:\n    print(\"\\nPreparing for fine-tuning process...\")\n    \n    # Prepare training data in the required format\n    training_data = {\"examples\": training_examples}\n    \n    # Check for existing model to reuse\n    model_id = None\n    \n    try:\n        # Try to read previous model ID from file\n        try:\n            with open(\"tuned_model_id.txt\", \"r\") as f:\n                saved_model_id = f.read().strip()\n                if saved_model_id:\n                    print(f\"Found previously saved model ID: {saved_model_id}\")\n                    model_id = saved_model_id\n        except FileNotFoundError:\n            print(\"No previously saved model ID found.\")\n        \n        # If no saved ID, check for existing models\n        if not model_id:\n            queued_model = None\n            print(\"Checking for existing tuned models...\")\n            \n            # List models in reverse order (newest first)\n            for m in reversed(client.tunings.list()):\n                # Look for models with your specific format tunedModels/personal-transaction-classifier-*\n                if m.name.startswith('tunedModels/personal-transaction-classifier-'):\n                    # If there is a completed model, use it\n                    if m.state.name == 'JOB_STATE_SUCCEEDED':\n                        model_id = m.name\n                        print(f'Found existing completed model to reuse: {model_id}')\n                        break\n                    elif m.state.name == 'JOB_STATE_RUNNING' and not queued_model:\n                        # If there's a model still running, remember it\n                        queued_model = m.name\n                        print(f'Found model still in progress: {queued_model}')\n            \n            # Use queued model if found and no completed model\n            if not model_id and queued_model:\n                model_id = queued_model\n                print(f'Using in-progress model: {model_id}')\n        \n        # Create new model if needed\n        if not model_id:\n            print(\"Starting new fine-tuning job...\")\n            tuning_op = client.tunings.tune(\n                base_model=\"models/gemini-1.5-flash-001-tuning\",\n                training_dataset=training_data,\n                config=types.CreateTuningJobConfig(\n                    tuned_model_display_name=\"personal-transaction-classifier\",  # Lowercase to match your existing model\n                    batch_size=16,\n                    epoch_count=3,\n                ),\n            )\n            \n            model_id = tuning_op.name\n            print(f\"Fine-tuning initiated. Model ID: {model_id}\")\n            print(f\"Current status: {tuning_op.state}\")\n            \n            # Poll for status updates (optional)\n            print(\"Initial training status:\")\n            print(f\"  - State: {tuning_op.state}\")\n            print(f\"  - Create time: {tuning_op.create_time}\")\n            if hasattr(tuning_op, 'progress') and tuning_op.progress:\n                print(f\"  - Progress: {tuning_op.progress}%\")\n        \n        # Save the model ID for later use\n        with open(\"tuned_model_id.txt\", \"w\") as f:\n            f.write(model_id)\n        \n        print(f\"\\nUsing model: {model_id}\")\n        print(\"This ID has been saved and will be used for predictions\")\n        \n    except Exception as e:\n        print(f\"Error in fine-tuning process: {e}\")\nelse:\n    print(\"No valid training examples created. Please fix the issues above.\")","metadata":{"execution":{"iopub.status.busy":"2025-04-09T11:28:17.226130Z","iopub.status.idle":"2025-04-09T11:28:17.226570Z","shell.execute_reply":"2025-04-09T11:28:17.226386Z"}}},{"cell_type":"markdown","source":"# Step 6: Focused Subcategory Generation\n# This version assumes your fine-tuned model generates subcategories\n\nimport time\nfrom tqdm.notebook import tqdm\n\ndef get_subcategory(description, model_id, retry_count=2):\n    \"\"\"Get subcategory using fine-tuned model with better error handling\"\"\"\n    for attempt in range(retry_count + 1):\n        try:\n            # Call the fine-tuned model directly with just the description\n            response = client.models.generate_content(\n                model=model_id,  # Your fine-tuned model ID\n                contents=[{\"text\": description}],  # Just the raw description\n                config=types.GenerateContentConfig(\n                    temperature=0.1,\n                    max_output_tokens=50,\n                    top_p=1,\n                )\n            )\n            \n            # Check if we got a valid response\n            if hasattr(response, 'text') and response.text:\n                subcategory = response.text.strip()\n                print(f\"Got subcategory: '{subcategory}' for '{description}'\")\n                return subcategory\n            else:\n                if attempt < retry_count:\n                    print(f\"Empty response for '{description}'. Retrying...\")\n                    time.sleep(2)\n                else:\n                    print(f\"No valid subcategory after {retry_count+1} attempts\")\n                    return \"\"\n        except Exception as e:\n            if attempt < retry_count:\n                print(f\"Attempt {attempt+1} failed: {str(e)[:100]}... Retrying...\")\n                time.sleep(2)\n            else:\n                print(f\"Failed to get subcategory: {str(e)[:100]}...\")\n                return \"\"\n    \n    return \"\"  # Default if all attempts fail\n\ndef process_transactions(descriptions, batch_size=5):\n    \"\"\"Process transactions to get subcategories\"\"\"\n    all_subcategories = []\n    \n    # Get the model ID\n    try:\n        with open(\"tuned_model_id.txt\", \"r\") as f:\n            model_id = f.read().strip()\n            print(f\"Using model ID: {model_id}\")\n    except FileNotFoundError:\n        print(\"Model ID file not found. Please ensure tuned_model_id.txt exists.\")\n        return [\"\"] * len(descriptions)\n    \n    # Setup progress bar\n    pbar = tqdm(total=len(descriptions), desc=\"Getting subcategories\")\n    \n    # Process in batches\n    for i in range(0, len(descriptions), batch_size):\n        batch = descriptions[i:i+batch_size]\n        batch_results = []\n        \n        print(f\"\\nProcessing batch {i//batch_size + 1} ({i} to {min(i+batch_size, len(descriptions))})...\")\n        \n        # Process each transaction in the batch\n        for description in batch:\n            if not description or pd.isna(description):\n                batch_results.append(\"\")\n            else:\n                subcategory = get_subcategory(description, model_id)\n                batch_results.append(subcategory)\n            \n            # Update progress bar\n            pbar.update(1)\n        \n        # Add results to overall list\n        all_subcategories.extend(batch_results)\n        \n        # Show sample from this batch\n        if batch:\n            print(f\"Sample: '{batch[0]}' â†’ Subcategory: '{batch_results[0]}'\")\n            print(\"-\" * 50)\n    \n    # Close progress bar\n    pbar.close()\n    \n    return all_subcategories\n\ndef main():\n    \"\"\"Main function to process transactions and save results\"\"\"\n    print(\"Loading transactions data...\")\n    file_path = \"/kaggle/input/finance-test/transactions_20250104_test.csv\"\n    \n    try:\n        df = pd.read_csv(file_path)\n        print(f\"Loaded {len(df)} transactions\")\n        \n        # Process to get subcategories\n        subcategories = process_transactions(df['description'].tolist(), batch_size=5)\n        \n        # Add results to dataframe\n        df['subcategory'] = subcategories\n        \n        # Save results\n        output_file = \"transactions_with_subcategories.csv\"\n        df.to_csv(output_file, index=False)\n        print(f\"\\nResults saved to {output_file}\")\n        \n        # Display sample\n        print(\"\\nSample of results:\")\n        print(df[['description', 'subcategory']].head(10))\n        \n        # Count non-empty subcategories\n        non_empty = sum(1 for s in subcategories if s)\n        print(f\"\\nGot subcategories for {non_empty} of {len(subcategories)} transactions ({non_empty/len(subcategories):.1%})\")\n        \n        return df\n    \n    except Exception as e:\n        print(f\"Error in main process: {e}\")\n        return None\n\n# Run the main function\nif __name__ == \"__main__\":\n    result_df = main()","metadata":{"execution":{"iopub.status.busy":"2025-04-09T11:28:17.227583Z","iopub.status.idle":"2025-04-09T11:28:17.228037Z","shell.execute_reply":"2025-04-09T11:28:17.227841Z"}}}]}