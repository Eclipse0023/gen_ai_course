{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11325548,"sourceType":"datasetVersion","datasetId":7084116},{"sourceId":11335753,"sourceType":"datasetVersion","datasetId":7091033}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip uninstall -qy jupyterlab  # Remove unused packages from Kaggle's base image that conflict\n!pip install -U -q \"google-genai==1.7.0\"\n!pip install ipython-sql\n%load_ext sql","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T10:51:59.848358Z","iopub.execute_input":"2025-04-09T10:51:59.848761Z","iopub.status.idle":"2025-04-09T10:52:10.126175Z","shell.execute_reply.started":"2025-04-09T10:51:59.848732Z","shell.execute_reply":"2025-04-09T10:52:10.125169Z"}},"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Skipping jupyterlab as it is not installed.\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: ipython-sql in /usr/local/lib/python3.10/dist-packages (0.5.0)\nRequirement already satisfied: prettytable in /usr/local/lib/python3.10/dist-packages (from ipython-sql) (3.12.0)\nRequirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from ipython-sql) (7.34.0)\nRequirement already satisfied: sqlalchemy>=2.0 in /usr/local/lib/python3.10/dist-packages (from ipython-sql) (2.0.36)\nRequirement already satisfied: sqlparse in /usr/local/lib/python3.10/dist-packages (from ipython-sql) (0.5.3)\nRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from ipython-sql) (1.17.0)\nRequirement already satisfied: ipython-genutils in /usr/local/lib/python3.10/dist-packages (from ipython-sql) (0.2.0)\nRequirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=2.0->ipython-sql) (4.12.2)\nRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=2.0->ipython-sql) (3.1.1)\nRequirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-sql) (75.1.0)\nRequirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-sql) (0.19.2)\nRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-sql) (4.4.2)\nRequirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-sql) (0.7.5)\nRequirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-sql) (5.7.1)\nRequirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-sql) (3.0.48)\nRequirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-sql) (2.19.1)\nRequirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-sql) (0.2.0)\nRequirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-sql) (0.1.7)\nRequirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-sql) (4.9.0)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prettytable->ipython-sql) (0.2.13)\nRequirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->ipython-sql) (0.8.4)\nRequirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->ipython-sql) (0.7.0)\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"from google import genai\nfrom google.genai import types\nfrom kaggle_secrets import UserSecretsClient\nimport pandas as pd\nimport json\nfrom tqdm.notebook import tqdm\nimport time\nfrom IPython.display import HTML, Markdown, display","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T10:52:14.135918Z","iopub.execute_input":"2025-04-09T10:52:14.136320Z","iopub.status.idle":"2025-04-09T10:52:14.141093Z","shell.execute_reply.started":"2025-04-09T10:52:14.136287Z","shell.execute_reply":"2025-04-09T10:52:14.139984Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\n\nGOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T10:52:16.192810Z","iopub.execute_input":"2025-04-09T10:52:16.193186Z","iopub.status.idle":"2025-04-09T10:52:16.321895Z","shell.execute_reply.started":"2025-04-09T10:52:16.193150Z","shell.execute_reply":"2025-04-09T10:52:16.320860Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"%load_ext sql\n%sql sqlite:///sample.db","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"-- Create the 'categories' table\nCREATE TABLE IF NOT EXISTS categories (\n    category_id INTEGER PRIMARY KEY AUTOINCREMENT,\n    name VARCHAR(100) NOT NULL UNIQUE,\n    description TEXT,\n    display_order INT DEFAULT 100,\n    is_active BOOLEAN DEFAULT 1,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n\n-- Create the 'subcategories' table\nCREATE TABLE IF NOT EXISTS subcategories (\n    subcategory_id INTEGER PRIMARY KEY AUTOINCREMENT,\n    category_id INTEGER NOT NULL,\n    name VARCHAR(100) NOT NULL,\n    description TEXT,\n    display_order INT DEFAULT 100,\n    is_active BOOLEAN DEFAULT 1,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    FOREIGN KEY (category_id) REFERENCES categories(category_id)\n);\n\n-- Create the 'keyword_rules' table\nCREATE TABLE IF NOT EXISTS keyword_rules (\n    rule_id INTEGER PRIMARY KEY AUTOINCREMENT,\n    category_id INTEGER NOT NULL,\n    subcategory_id INTEGER,\n    keyword VARCHAR(255) NOT NULL,\n    match_type VARCHAR(20) DEFAULT 'contains',\n    priority INT DEFAULT 100,\n    is_active BOOLEAN DEFAULT 1,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    created_by VARCHAR(100),\n    FOREIGN KEY (category_id) REFERENCES categories(category_id),\n    FOREIGN KEY (subcategory_id) REFERENCES subcategories(subcategory_id)\n);\n\n-- Create the 'transaction_categories' table\nCREATE TABLE IF NOT EXISTS transaction_categories (\n    transaction_id VARCHAR(100) PRIMARY KEY,\n    category_id INTEGER,\n    subcategory_id INTEGER,\n    confidence_score DECIMAL(5,4),\n    categorization_method VARCHAR(50),\n    is_manually_reviewed BOOLEAN DEFAULT 0,\n    categorized_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    FOREIGN KEY (category_id) REFERENCES categories(category_id),\n    FOREIGN KEY (subcategory_id) REFERENCES subcategories(subcategory_id)\n);\n\n-- Insert data into the 'categories' table\nINSERT INTO categories (name, description) VALUES\n    ('Groceries', 'Food and household items from supermarkets'),\n    ('Dining', 'Restaurants, cafes, and food delivery'),\n    ('Shopping', 'Retail purchases and online shopping'),\n    ('Transport', 'Public transit, taxis, and ride sharing'),\n    ('Utilities', 'Bills for electricity, water, internet, etc.'),\n    ('Entertainment', 'Movies, streaming services, and recreational activities');\n\n-- Insert data into the 'subcategories' table\nINSERT INTO subcategories (category_id, name, description) VALUES\n    (1, 'Supermarket', 'Major grocery stores'),\n    (1, 'Specialty Food', 'Bakeries, butchers, etc.'),\n    (2, 'Restaurants', 'Sit-down dining establishments'),\n    (2, 'Fast Food', 'Quick service restaurants'),\n    (2, 'Food Delivery', 'Meal delivery services'),\n    (3, 'Clothing', 'Apparel and accessories');\n\n-- Insert data into the 'keyword_rules' table\nINSERT INTO keyword_rules (category_id, subcategory_id, keyword, match_type, priority) VALUES\n    (1, 1, 'WALMART', 'contains', 10),\n    (1, 1, 'KROGER', 'contains', 20),\n    (1, 1, 'SAFEWAY', 'contains', 30),\n    (1, 2, 'BAKERY', 'contains', 40),\n    (2, 3, 'RESTAURANT', 'contains', 10),\n    (2, 4, 'MCDONALD', 'contains', 20),\n    (2, 5, 'UBER EATS', 'contains', 30),\n    (2, 5, 'DOORDASH', 'contains', 40),\n    (3, 6, 'AMAZON', 'contains', 10),\n    (4, NULL, 'UBER', 'contains', 10),\n    (4, NULL, 'LYFT', 'contains', 20),\n    (5, NULL, 'ELECTRIC', 'contains', 10),\n    (5, NULL, 'WATER', 'contains', 20),\n    (6, NULL, 'NETFLIX', 'contains', 10),\n    (6, NULL, 'SPOTIFY', 'contains', 20);\n\n-- Insert data into the 'transaction_categories' table\nINSERT INTO transaction_categories (transaction_id, category_id, subcategory_id, confidence_score, categorization_method) VALUES\n    ('T12345', 1, 1, 0.95, 'keyword_rule'),\n    ('T23456', 2, 5, 0.85, 'keyword_rule'),\n    ('T34567', 6, NULL, 0.95, 'keyword_rule'),\n    ('T45678', 4, NULL, 0.90, 'keyword_rule'),\n    ('T56789', 3, 6, 0.95, 'keyword_rule');","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"client = genai.Client(api_key=GOOGLE_API_KEY)\n\n# You can optionally set your model configuration here too\nmodel_config = types.GenerateContentConfig(\n    temperature=0.1,\n    top_p=1,\n    max_output_tokens=250,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T05:58:55.926182Z","iopub.execute_input":"2025-04-09T05:58:55.926592Z","iopub.status.idle":"2025-04-09T05:58:56.059723Z","shell.execute_reply.started":"2025-04-09T05:58:55.926555Z","shell.execute_reply":"2025-04-09T05:58:56.058532Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from google.api_core import retry\n\n\nis_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n\ngenai.models.Models.generate_content = retry.Retry(\n    predicate=is_retriable)(genai.models.Models.generate_content)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T05:58:56.060755Z","iopub.execute_input":"2025-04-09T05:58:56.061066Z","iopub.status.idle":"2025-04-09T05:58:56.066410Z","shell.execute_reply.started":"2025-04-09T05:58:56.061039Z","shell.execute_reply":"2025-04-09T05:58:56.065354Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 3. Load your pre-categorized transactions\nprint(\"Loading training data...\")\ntraining_file_path = \"/kaggle/input/my-categorized-transactions/my_categorized_transactions.csv\"\n\ntry:\n    df_training = pd.read_csv(training_file_path)\n    print(f\"Successfully loaded {len(df_training)} training examples\")\n    \n    # Print sample to verify columns\n    print(\"\\nSample of training data:\")\n    print(df_training.head(2))\n    print(\"\\nColumns in training data:\", df_training.columns.tolist())\n    \nexcept FileNotFoundError:\n    print(f\"ERROR: Could not find file at {training_file_path}\")\n    print(\"Please ensure your CSV is uploaded to the correct location\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T05:58:56.067475Z","iopub.execute_input":"2025-04-09T05:58:56.067840Z","iopub.status.idle":"2025-04-09T05:58:56.107928Z","shell.execute_reply.started":"2025-04-09T05:58:56.067794Z","shell.execute_reply":"2025-04-09T05:58:56.106645Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 4. Prepare training examples (FIXED VERSION)\nprint(\"\\nCreating training examples...\")\ntraining_examples = []\n\n# Check if the required columns exist\nif 'note' not in df_training.columns or 'category' not in df_training.columns:\n    print(\"ERROR: Training data must have 'note' and 'category' columns\")\n    print(f\"Available columns: {df_training.columns.tolist()}\")\nelse:\n    # Filter out rows with NaN values in the 'note' column\n    df_valid = df_training.dropna(subset=['note'])\n    \n    print(f\"Original data: {len(df_training)} rows\")\n    print(f\"After removing NaN values: {len(df_valid)} rows\")\n    print(f\"Removed {len(df_training) - len(df_valid)} rows with missing descriptions\")\n    \n    for _, row in df_valid.iterrows():\n        # Create training example with correct fields\n        training_examples.append({\n            \"textInput\": str(row['note']),  # Ensure it's a string\n            \"output\": str(row['category'])  # Ensure it's a string\n        })\n    \n    print(f\"Created {len(training_examples)} training examples\")\n    \n    # Optionally show a few examples\n    print(\"\\nSample training examples:\")\n    for example in training_examples[:3]:\n        print(f\"Input: '{example['textInput'][:50]}...'\")\n        print(f\"Output: '{example['output']}'\")\n        print(\"---\")\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T05:58:56.109026Z","iopub.execute_input":"2025-04-09T05:58:56.109332Z","iopub.status.idle":"2025-04-09T05:58:56.160684Z","shell.execute_reply.started":"2025-04-09T05:58:56.109305Z","shell.execute_reply":"2025-04-09T05:58:56.159634Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 5. Start the fine-tuning process (IMPROVED VERSION)\nfrom collections.abc import Iterable\n\nif len(training_examples) > 0:\n    print(\"\\nPreparing for fine-tuning process...\")\n    \n    # Prepare training data in the required format\n    training_data = {\"examples\": training_examples}\n    \n    # Check for existing model to reuse\n    model_id = None\n    \n    try:\n        # Try to read previous model ID from file\n        try:\n            with open(\"tuned_model_id.txt\", \"r\") as f:\n                saved_model_id = f.read().strip()\n                if saved_model_id:\n                    print(f\"Found previously saved model ID: {saved_model_id}\")\n                    model_id = saved_model_id\n        except FileNotFoundError:\n            print(\"No previously saved model ID found.\")\n        \n        # If no saved ID, check for existing models\n        if not model_id:\n            queued_model = None\n            print(\"Checking for existing tuned models...\")\n            \n            # List models in reverse order (newest first)\n            for m in reversed(client.tunings.list()):\n                # Look for models with your specific format tunedModels/personal-transaction-classifier-*\n                if m.name.startswith('tunedModels/personal-transaction-classifier-'):\n                    # If there is a completed model, use it\n                    if m.state.name == 'JOB_STATE_SUCCEEDED':\n                        model_id = m.name\n                        print(f'Found existing completed model to reuse: {model_id}')\n                        break\n                    elif m.state.name == 'JOB_STATE_RUNNING' and not queued_model:\n                        # If there's a model still running, remember it\n                        queued_model = m.name\n                        print(f'Found model still in progress: {queued_model}')\n            \n            # Use queued model if found and no completed model\n            if not model_id and queued_model:\n                model_id = queued_model\n                print(f'Using in-progress model: {model_id}')\n        \n        # Create new model if needed\n        if not model_id:\n            print(\"Starting new fine-tuning job...\")\n            tuning_op = client.tunings.tune(\n                base_model=\"models/gemini-1.5-flash-001-tuning\",\n                training_dataset=training_data,\n                config=types.CreateTuningJobConfig(\n                    tuned_model_display_name=\"personal-transaction-classifier\",  # Lowercase to match your existing model\n                    batch_size=16,\n                    epoch_count=3,\n                ),\n            )\n            \n            model_id = tuning_op.name\n            print(f\"Fine-tuning initiated. Model ID: {model_id}\")\n            print(f\"Current status: {tuning_op.state}\")\n            \n            # Poll for status updates (optional)\n            print(\"Initial training status:\")\n            print(f\"  - State: {tuning_op.state}\")\n            print(f\"  - Create time: {tuning_op.create_time}\")\n            if hasattr(tuning_op, 'progress') and tuning_op.progress:\n                print(f\"  - Progress: {tuning_op.progress}%\")\n        \n        # Save the model ID for later use\n        with open(\"tuned_model_id.txt\", \"w\") as f:\n            f.write(model_id)\n        \n        print(f\"\\nUsing model: {model_id}\")\n        print(\"This ID has been saved and will be used for predictions\")\n        \n    except Exception as e:\n        print(f\"Error in fine-tuning process: {e}\")\nelse:\n    print(\"No valid training examples created. Please fix the issues above.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T06:09:45.954358Z","iopub.execute_input":"2025-04-09T06:09:45.954789Z","iopub.status.idle":"2025-04-09T06:09:45.967533Z","shell.execute_reply.started":"2025-04-09T06:09:45.954760Z","shell.execute_reply":"2025-04-09T06:09:45.966526Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 6: Focused Subcategory Generation\n# This version assumes your fine-tuned model generates subcategories\n\nimport time\nfrom tqdm.notebook import tqdm\n\ndef get_subcategory(description, model_id, retry_count=2):\n    \"\"\"Get subcategory using fine-tuned model with better error handling\"\"\"\n    for attempt in range(retry_count + 1):\n        try:\n            # Call the fine-tuned model directly with just the description\n            response = client.models.generate_content(\n                model=model_id,  # Your fine-tuned model ID\n                contents=[{\"text\": description}],  # Just the raw description\n                config=types.GenerateContentConfig(\n                    temperature=0.1,\n                    max_output_tokens=50,\n                    top_p=1,\n                )\n            )\n            \n            # Check if we got a valid response\n            if hasattr(response, 'text') and response.text:\n                subcategory = response.text.strip()\n                print(f\"Got subcategory: '{subcategory}' for '{description}'\")\n                return subcategory\n            else:\n                if attempt < retry_count:\n                    print(f\"Empty response for '{description}'. Retrying...\")\n                    time.sleep(2)\n                else:\n                    print(f\"No valid subcategory after {retry_count+1} attempts\")\n                    return \"\"\n        except Exception as e:\n            if attempt < retry_count:\n                print(f\"Attempt {attempt+1} failed: {str(e)[:100]}... Retrying...\")\n                time.sleep(2)\n            else:\n                print(f\"Failed to get subcategory: {str(e)[:100]}...\")\n                return \"\"\n    \n    return \"\"  # Default if all attempts fail\n\ndef process_transactions(descriptions, batch_size=5):\n    \"\"\"Process transactions to get subcategories\"\"\"\n    all_subcategories = []\n    \n    # Get the model ID\n    try:\n        with open(\"tuned_model_id.txt\", \"r\") as f:\n            model_id = f.read().strip()\n            print(f\"Using model ID: {model_id}\")\n    except FileNotFoundError:\n        print(\"Model ID file not found. Please ensure tuned_model_id.txt exists.\")\n        return [\"\"] * len(descriptions)\n    \n    # Setup progress bar\n    pbar = tqdm(total=len(descriptions), desc=\"Getting subcategories\")\n    \n    # Process in batches\n    for i in range(0, len(descriptions), batch_size):\n        batch = descriptions[i:i+batch_size]\n        batch_results = []\n        \n        print(f\"\\nProcessing batch {i//batch_size + 1} ({i} to {min(i+batch_size, len(descriptions))})...\")\n        \n        # Process each transaction in the batch\n        for description in batch:\n            if not description or pd.isna(description):\n                batch_results.append(\"\")\n            else:\n                subcategory = get_subcategory(description, model_id)\n                batch_results.append(subcategory)\n            \n            # Update progress bar\n            pbar.update(1)\n        \n        # Add results to overall list\n        all_subcategories.extend(batch_results)\n        \n        # Show sample from this batch\n        if batch:\n            print(f\"Sample: '{batch[0]}' → Subcategory: '{batch_results[0]}'\")\n            print(\"-\" * 50)\n    \n    # Close progress bar\n    pbar.close()\n    \n    return all_subcategories\n\ndef main():\n    \"\"\"Main function to process transactions and save results\"\"\"\n    print(\"Loading transactions data...\")\n    file_path = \"/kaggle/input/finance-test/transactions_20250104_test.csv\"\n    \n    try:\n        df = pd.read_csv(file_path)\n        print(f\"Loaded {len(df)} transactions\")\n        \n        # Process to get subcategories\n        subcategories = process_transactions(df['description'].tolist(), batch_size=5)\n        \n        # Add results to dataframe\n        df['subcategory'] = subcategories\n        \n        # Save results\n        output_file = \"transactions_with_subcategories.csv\"\n        df.to_csv(output_file, index=False)\n        print(f\"\\nResults saved to {output_file}\")\n        \n        # Display sample\n        print(\"\\nSample of results:\")\n        print(df[['description', 'subcategory']].head(10))\n        \n        # Count non-empty subcategories\n        non_empty = sum(1 for s in subcategories if s)\n        print(f\"\\nGot subcategories for {non_empty} of {len(subcategories)} transactions ({non_empty/len(subcategories):.1%})\")\n        \n        return df\n    \n    except Exception as e:\n        print(f\"Error in main process: {e}\")\n        return None\n\n# Run the main function\nif __name__ == \"__main__\":\n    result_df = main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T06:45:17.809079Z","iopub.execute_input":"2025-04-09T06:45:17.809499Z","iopub.status.idle":"2025-04-09T06:49:13.284024Z","shell.execute_reply.started":"2025-04-09T06:45:17.809467Z","shell.execute_reply":"2025-04-09T06:49:13.282457Z"}},"outputs":[],"execution_count":null}]}